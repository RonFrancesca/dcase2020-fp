{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zB7J94k-CTYz"
      },
      "source": [
        "## Feature extraction\n",
        "This notebook aims to properly extract the features from the development and evaluation dataset, needed to properly train the neural network implemented and proposed as system for the DCASE Challenge 2020, Task3. \n",
        "The script extracts, from a file audio, the log mel spectrogram using a 64 mel band filter, together with acoustic intensity vector (in the case of Ambisonic format) or generalized cross-correlation (in the case of Microphone Array format). These features will be given as input to the convolutional recurrent neural network to make predictions regarding sound event detection and sound event localization.\n",
        "\n",
        "With the aim of additionally increase the SELD score and to reduce the overfitting of the system, the training dataset size will be increased using data augmentation based on channel rotations and reflection on the xy plane in the FOA domain. In particular, we implemented the 16 patterns technique proposed for the first time by Mazzon et.al in [1].  The suggested data manipulations correspond to rotations of 0, -90◦, +90◦, and +180◦ related to the azimuth angle, leading to 8 rotations about the z axis, and 2 reflections with respect to the xy plane (considering the opposite elevation angle), for a total of 15 new patterns plus the original one. The user can decide how many augmented files would like to create from the the original one, increasing the dataset size. \n",
        "\n",
        "The user is invitated to follow the instructions, to change file and folder paths and configuring the system as he sees fit. \n",
        "\n",
        "[1]: Mazzon, L., et al. \"Sound event localization and detection using FOA domain spatial augmentation.\" Proc. of the 4th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE). 2019.\n",
        "\n",
        "Please, follow the instructions and change the folder paths with the ones related to your machine or drive. \n",
        "The paths needed to be changed are marked with a TODO command. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z7u2CLlRCr_U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39d7ae60-d051-4741-eaea-7405188b5340"
      },
      "source": [
        "#all the data are save in gogole colab so the first instruction would be mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5cxe_5lJOPAm",
        "colab": {}
      },
      "source": [
        "#importing all the modules needed fo running the script \n",
        "import os \n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import scipy.io.wavfile as wav"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPa3ZWZb3J1J",
        "colab_type": "text"
      },
      "source": [
        "## Parameters definition\n",
        "\n",
        "The next cell defines all the paramaters, which need to be properly configured  to run the algorithm.  \n",
        "\n",
        "Please, change the folder paths with the ones related to you machine and/or drive. The paths to be changed are marked with a TODO comand. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kjFYsjtLClNK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "33ef9fc3-80e5-4b1b-a15c-a71868dc6d65"
      },
      "source": [
        "# DATASET LOADING PARAMETERS\n",
        "case = 1\n",
        "# 'dev' - development or 'eval' - evaluation dataset, or both. \n",
        "process_str = 'dev, eval'\n",
        "\n",
        "# 'dev' - development or 'eval' - evaluation dataset\n",
        "mode='eval'\n",
        "# 'foa' - ambisonic or 'mic' - microphone signals                                               \n",
        "dataset='foa'    \n",
        "\n",
        "#if the dataset used is foa, than we re using data augmentation, otherwise no data augmnetatin will be used for the mic.   \n",
        "dataset_aug = False if dataset == 'mic' else True\n",
        "\n",
        "#BASE PATH\n",
        "#base dir path. \n",
        "#TODO: Change it with with proper path in your machine\n",
        "base_dir = '/content/drive/My Drive/Dataset-FP/'\n",
        "#defining cross-validation split\n",
        "if mode == 'dev':\n",
        "  train_splits = [3, 4, 5, 6]\n",
        "elif mode == 'eval':\n",
        "  train_splits = [2, 3, 4, 5, 6]\n",
        "\n",
        "#how many new files we want to create from the original one.\n",
        "data_augmentation_nb = 2\n",
        "\n",
        "# INPUT PATH\n",
        "# Base folder containing the foa/mic and metadata folders. \n",
        "#TODO: Change it with proper path in your machine \n",
        "dataset_dir = base_dir + 'dataset-eval'\n",
        "# Directory where extracted features and labels    \n",
        "feat_label_dir= os.path.join(dataset_dir, 'feat_label-AR/')       \n",
        "\n",
        "#pattern rotation to consider for data augmentation. \n",
        "# From the 16 patterns listed in the paper above mentioned, we decided to consider all the patterns but the orignal one. \n",
        "rotation_pattern = 15\n",
        "                                                                                                                                                                                      \n",
        "#FEATURE PARAMS\n",
        "fs=24000\n",
        "hop_len_s=0.02\n",
        "label_hop_len_s=0.1\n",
        "max_audio_len_s=60\n",
        "nb_mel_bins=64            \n",
        "\n",
        "unique_classes = {\n",
        "            'alarm': 0,\n",
        "            'baby': 1,\n",
        "            'crash': 2,\n",
        "            'dog': 3,\n",
        "            'engine': 4,\n",
        "            'female_scream': 5,\n",
        "            'female_speech': 6,\n",
        "            'fire': 7,\n",
        "            'footsteps': 8,\n",
        "            'knock': 9,\n",
        "            'male_scream': 10,\n",
        "            'male_speech': 11,\n",
        "            'phone': 12,\n",
        "            'piano': 13\n",
        "        }\n",
        "\n",
        "\n",
        "    # ########### User defined parameters ##############\n",
        "    # different user parameters so to set dev or eval mode and foa or mic dataset, or quick test \n",
        "if case == 1:\n",
        "  print(\"USING DEFAULT PARAMETERS\\n\")\n",
        "\n",
        "elif case == 2:\n",
        "  mode = 'dev'\n",
        "  dataset = 'mic'\n",
        "\n",
        "elif case == 3:\n",
        "   mode = 'eval'\n",
        "   dataset = 'mic'\n",
        "\n",
        "elif case == 4:\n",
        "    mode = 'dev'\n",
        "    dataset = 'foa'\n",
        "\n",
        "elif case == 5:\n",
        "    mode = 'eval'\n",
        "    dataset = 'foa'\n",
        "\n",
        "elif case == 999:\n",
        "      print(\"QUICK TEST MODE\\n\")\n",
        "      quick_test = True\n",
        "      epochs_per_fit = 1\n",
        "\n",
        "else:\n",
        "    print('ERROR: unknown argument {}'.format(case))\n",
        "    exit()\n",
        "       "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USING DEFAULT PARAMETERS\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HsywcYlYOU33",
        "colab": {}
      },
      "source": [
        "#function to create a new folder in case it does not exist yet. \n",
        "def create_folder(folder_name):\n",
        "    \"\"\"\n",
        "    The function creates a new folder in case it does not exist yet.\n",
        "    Parameters\n",
        "    -------------\n",
        "      :folder_name: name of the folder to be created (folder path)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(folder_name):\n",
        "        print('{} folder does not exist, creating it.'.format(folder_name))\n",
        "        os.makedirs(folder_name)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lk07_GLVOVcE"
      },
      "source": [
        "## Data augmentation definition \n",
        "\n",
        "The next cell defines and implements 15 channel rotations and reflection in FOA domain. \n",
        "Once a rotation pattern has been randomly selected, the same augmentation pattern will be applied for the input feature extraction of a data and for its corresponding label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jlK4gp2tP0UG",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import scipy.io.wavfile as wav\n",
        "\n",
        "\n",
        "def apply_data_augmentation(audio):\n",
        "    \"\"\"\n",
        "    The function applies one of the 15 patterns in order to augment the data. The 15 patterns are based on 90, -90 and 180 degree channel rotations\n",
        "    Parameters\n",
        "    -------------\n",
        "      :audio: audio to be augmented\n",
        "    Return\n",
        "    -------------\n",
        "      :audio_aug: augmented audio\n",
        "      :pattern: pattern applied for the data augmentation of this particular audio\n",
        "    \"\"\"\n",
        "    \n",
        "    #selecting the a random pattern between the 15 implemented\n",
        "    pattern = random.randrange(rotation_pattern) # original case not considered for augmentation\n",
        "    print(\"Data augmentation pattern {}\".format(pattern))\n",
        "        \n",
        "    audio_aug = np.zeros(audio.shape)\n",
        "\n",
        "    w = audio[:, 0]\n",
        "    y = audio[:, 1]\n",
        "    z = audio[:, 2]\n",
        "    x = audio[:, 3]\n",
        "    \n",
        "    # w channel never change \n",
        "    audio_aug[:, 0] = w\n",
        "\n",
        "    # the 15 pattern rotations and reflection\n",
        "    if (pattern == 0):\n",
        "        #print('Φ - pi/2')\n",
        "        audio_aug[:, 1] = -x\n",
        "        audio_aug[:, 3] = y\n",
        "        audio_aug[:, 2] = z  \n",
        "    elif (pattern == 1):\n",
        "        #print('Φ - pi/2, -θ')\n",
        "        audio_aug[:, 1] = -x\n",
        "        audio_aug[:, 3] = y\n",
        "        audio_aug[:, 2] = -z\n",
        "    \n",
        "    elif (pattern == 2):\n",
        "        #print('Φ, -θ')\n",
        "        audio_aug[:, 1] = y\n",
        "        audio_aug[:, 3] = x\n",
        "        audio_aug[:, 2] = -z\n",
        "    \n",
        "    elif (pattern == 3):\n",
        "        #print('Φ + pi/2')\n",
        "        audio_aug[:, 1] = x\n",
        "        audio_aug[:, 3] = -y\n",
        "        audio_aug[:, 2] = z    \n",
        "    elif (pattern == 4):    \n",
        "        #print('Φ + pi/2, -θ')\n",
        "        audio_aug[:, 1] = x\n",
        "        audio_aug[:, 3] = -y\n",
        "        audio_aug[:, 2] = -z\n",
        "        \n",
        "    elif (pattern == 5):\n",
        "        #print('-Φ - pi/2')\n",
        "        audio_aug[:, 1] = x\n",
        "        audio_aug[:, 3] = -y\n",
        "        audio_aug[:, 2] = z\n",
        "    elif (pattern == 6):\n",
        "        #print('-Φ - pi/2, -θ')\n",
        "        audio_aug[:, 1] = -x\n",
        "        audio_aug[:, 3] = -y\n",
        "        audio_aug[:, 2] = -z\n",
        "    \n",
        "    elif (pattern == 7):\n",
        "        #print('-Φ')\n",
        "        audio_aug[:, 1] = -y\n",
        "        audio_aug[:, 3] = x\n",
        "        audio_aug[:, 2] = z  \n",
        "    elif (pattern == 8):\n",
        "        # - azimuth - pi/2 - elevation\n",
        "        #print('-Φ, -θ')\n",
        "        audio_aug[:, 1] = -y\n",
        "        audio_aug[:, 3] = x\n",
        "        audio_aug[:, 2] = -z\n",
        "    \n",
        "    elif (pattern == 9):\n",
        "        #print('-Φ + pi/2')\n",
        "        audio_aug[:, 1] = x\n",
        "        audio_aug[:, 3] = y\n",
        "        audio_aug[:, 2] = z  \n",
        "    elif (pattern == 10):\n",
        "        #print('-Φ + pi/2, -θ')\n",
        "        audio_aug[:, 1] = x\n",
        "        audio_aug[:, 3] = y\n",
        "        audio_aug[:, 2] = -z\n",
        "    \n",
        "    elif (pattern == 11):\n",
        "        #print('-Φ + pi')\n",
        "        audio_aug[:, 1] = y\n",
        "        audio_aug[:, 3] = -x\n",
        "        audio_aug[:, 2] = z      \n",
        "    elif (pattern == 12):\n",
        "        # azimuth + pi/2 - elevation\n",
        "        #print('-Φ + pi, -θ')\n",
        "        audio_aug[:, 1] = y\n",
        "        audio_aug[:, 3] = -x\n",
        "        audio_aug[:, 2] = -z\n",
        "        \n",
        "    elif (pattern == 13):\n",
        "        #print('Φ + pi')\n",
        "        audio_aug[:, 1] = -x\n",
        "        audio_aug[:, 3] = -y\n",
        "        audio_aug[:, 2] = z    \n",
        "    elif (pattern == 14):\n",
        "        #print('Φ + pi, -θ')\n",
        "        audio_aug[:, 1] = -x\n",
        "        audio_aug[:, 3] = -y\n",
        "        audio_aug[:, 2] = -z  \n",
        "    \n",
        "    else:\n",
        "        print(\"Wrong pattern selection\")\n",
        "        \n",
        "    return audio_aug, pattern\n",
        "\n",
        "\n",
        "def label_rotation(label, pattern):\n",
        "\n",
        "  \"\"\"\n",
        "    The function use the pattern received as input to augment the label of the corresponding audio.\n",
        "    The channel rotation, implemented frame-by-frame, is implemnted on the label frame received as input. \n",
        "    Parameters\n",
        "    -------------\n",
        "      :label: frame of the label to be augmented\n",
        "      :pattern: pattern applied\n",
        "    Return\n",
        "    -------------\n",
        "      :label_aug: augmented frame of the label\n",
        "    \"\"\"\n",
        "\n",
        "  label_aug = np.zeros(len(label))\n",
        "    # w channel never change \n",
        "  label_aug[0] = label[0]\n",
        "\n",
        "  if (pattern == 0):\n",
        "        #print('Φ - pi/2')\n",
        "        label_aug[1] = label[2]\n",
        "        label_aug[2] = -label[1]\n",
        "        label_aug[3] = label[3]\n",
        "  elif (pattern == 1):\n",
        "        #print('Φ - pi/2, -θ')\n",
        "        label_aug[1] = label[2]\n",
        "        label_aug[2] = -label[1]\n",
        "        label_aug[3] = -label[3]\n",
        "    \n",
        "  elif (pattern == 2):\n",
        "        #print('Φ, -θ')\n",
        "        label_aug[1] = label[1]\n",
        "        label_aug[2] = label[2]\n",
        "        label_aug[3] = -label[3]\n",
        "    \n",
        "  elif (pattern == 3):\n",
        "        #print('Φ + pi/2')\n",
        "        label_aug[1] = -label[2]\n",
        "        label_aug[2] = label[1]\n",
        "        label_aug[3] = label[3]\n",
        "  elif (pattern == 4):    \n",
        "        #print('Φ + pi/2, -θ')\n",
        "        label_aug[1] = -label[2]\n",
        "        label_aug[2] = label[1]\n",
        "        label_aug[3] = -label[3]\n",
        "        \n",
        "  elif (pattern == 5):\n",
        "        #print('-Φ - pi/2')\n",
        "        label_aug[1] = -label[2]\n",
        "        label_aug[2] = label[1]\n",
        "        label_aug[3] = label[3]\n",
        "  elif (pattern == 6):\n",
        "        #print('-Φ - pi/2, -θ')\n",
        "        label_aug[1] = -label[2]\n",
        "        label_aug[2] = -label[1]\n",
        "        label_aug[3] = -label[3]\n",
        "    \n",
        "  elif (pattern == 7):\n",
        "        #print('-Φ')\n",
        "        label_aug[1] = label[1]\n",
        "        label_aug[2] = -label[2]\n",
        "        label_aug[3] = label[3]\n",
        "  elif (pattern == 8):\n",
        "        # - azimuth - pi/2 - elevation\n",
        "        #print('-Φ, -θ')\n",
        "        label_aug[1] = label[1]\n",
        "        label_aug[2] = -label[2]\n",
        "        label_aug[3] = -label[3]\n",
        "    \n",
        "  elif (pattern == 9):\n",
        "        #print('-Φ + pi/2')\n",
        "        label_aug[1] = label[2]\n",
        "        label_aug[2] = label[1]\n",
        "        label_aug[3] = label[3]\n",
        "  elif (pattern == 10):\n",
        "        #print('-Φ + pi/2, -θ')\n",
        "        label_aug[1] = label[2]\n",
        "        label_aug[2] = label[1]\n",
        "        label_aug[3] = -label[3]\n",
        "    \n",
        "  elif (pattern == 11):\n",
        "        #print('-Φ + pi')\n",
        "        label_aug[1] = -label[1]\n",
        "        label_aug[2] = label[2]\n",
        "        label_aug[3] = label[3]     \n",
        "  elif (pattern == 12):\n",
        "        # azimuth + pi/2 - elevation\n",
        "        #print('-Φ + pi, -θ')\n",
        "        label_aug[1] = -label[1]\n",
        "        label_aug[2] = label[2]\n",
        "        label_aug[3] = -label[3]\n",
        "        \n",
        "  elif (pattern == 13):\n",
        "        #print('Φ + pi')\n",
        "        label_aug[1] = -label[2]\n",
        "        label_aug[2] = -label[1]\n",
        "        label_aug[3] = label[3]\n",
        "  elif (pattern == 14):\n",
        "        #print('Φ + pi, -θ')\n",
        "        label_aug[1] = -label[2]\n",
        "        label_aug[2] = -label[1]\n",
        "        label_aug[3] = -label[3]\n",
        "    \n",
        "  else:\n",
        "        print(\"Wrong pattern selection\")\n",
        "        \n",
        "  return label_aug\n",
        "\n",
        "def label_augmentation(label_dir, pattern):\n",
        "\n",
        "    #print(\"Label augmentation pattern {}\".format(pattern))\n",
        "    \n",
        "    \"\"\"\n",
        "    The function applies the received pattern in order to augment the label of the corresponding audio.\n",
        "    The function receives a label dictinary as input and augment the label frame by frame. \n",
        "    Parameters\n",
        "    -------------\n",
        "      :label: label dictionary containing all the frames that need to be augmented. \n",
        "      :pattern: pattern to apply\n",
        "    Return\n",
        "    -------------\n",
        "      :label_aug: augmented label dictinary\n",
        "    \"\"\"\n",
        "\n",
        "    label_aug_dict = {}\n",
        "    for frame_cnt in label_dir.keys():\n",
        "        if frame_cnt not in label_aug_dict:\n",
        "            label_aug_dict[frame_cnt] = []\n",
        "            for tmp_val in label_dir[frame_cnt]:\n",
        "                aug_tmp_value = label_rotation(tmp_val, pattern)\n",
        "                label_aug_dict[frame_cnt].append(aug_tmp_value)\n",
        "    return label_aug_dict"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm9iZYHnMrRl",
        "colab_type": "text"
      },
      "source": [
        "# Feature class \n",
        "\n",
        "The next cell defines and implements the class used for features extraction and the relative functions such as extraction of spectrogram, log-mel band spectrgram, acoustic intensity vector and generilized cross-correlactrion according to the domain. \n",
        "\n",
        "Next cell also implement the functions used for the features extraction process. In particular, the feature extraction, the pre-process of the features (normalization of dataset) and the extraction of corrispective labels. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1vfLkOnyCczy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1f478178-daf5-480e-e419-7fa64a311ee6"
      },
      "source": [
        "# Contains routines for labels creation, features extraction and normalization\n",
        "from sklearn import preprocessing\n",
        "from sklearn.externals import joblib\n",
        "import math\n",
        "\n",
        "\n",
        "def nCr(n, r):\n",
        "    return math.factorial(n) // math.factorial(r) // math.factorial(n-r)\n",
        "\n",
        "\n",
        "class FeatureClass:\n",
        "    def __init__(self, is_eval=False):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        --------------\n",
        "        :param params: parameters dictionary\n",
        "        :param is_eval: if True, does not load dataset labels.\n",
        "        \"\"\"\n",
        "\n",
        "        # Input directories\n",
        "        self._feat_label_dir = feat_label_dir\n",
        "        self._dataset_dir = dataset_dir\n",
        "        self._dataset_combination = '{}_{}'.format(dataset, 'eval' if is_eval else 'dev')\n",
        "        self._aud_dir = os.path.join(self._dataset_dir, self._dataset_combination)\n",
        "        self._desc_dir = None if is_eval else os.path.join(self._dataset_dir, 'metadata_dev')\n",
        "\n",
        "        # Output directories\n",
        "        self._label_dir = None\n",
        "        self._feat_dir = None\n",
        "        self._feat_dir_norm = None\n",
        "\n",
        "        # Local parameters\n",
        "        self._is_eval = is_eval\n",
        "\n",
        "        self._fs = fs\n",
        "        self._hop_len_s = hop_len_s\n",
        "        self._hop_len = int(self._fs * self._hop_len_s)\n",
        "\n",
        "        self._label_hop_len_s = label_hop_len_s\n",
        "        self._label_hop_len = int(self._fs * self._label_hop_len_s)\n",
        "        self._label_frame_res = self._fs / float(self._label_hop_len)\n",
        "        self._nb_label_frames_1s = int(self._label_frame_res)\n",
        "\n",
        "        self._win_len = 2 * self._hop_len\n",
        "        self._nfft = self._next_greater_power_of_2(self._win_len)\n",
        "        self._nb_mel_bins = nb_mel_bins\n",
        "        self._mel_wts = librosa.filters.mel(sr=self._fs, n_fft=self._nfft, n_mels=self._nb_mel_bins).T\n",
        "\n",
        "        self._dataset = dataset\n",
        "        self._eps = 1e-8\n",
        "        self._nb_channels = 4\n",
        "\n",
        "        # Sound event classes dictionary\n",
        "        self._unique_classes = unique_classes\n",
        "        self._audio_max_len_samples = max_audio_len_s * self._fs \n",
        "\n",
        "        self._max_feat_frames = int(np.ceil(self._audio_max_len_samples / float(self._hop_len)))\n",
        "        self._max_label_frames = int(np.ceil(self._audio_max_len_samples / float(self._label_hop_len)))\n",
        "\n",
        "    def _load_audio(self, audio_path):\n",
        "\n",
        "        fs, audio = wav.read(audio_path)\n",
        "        audio = audio[:, :self._nb_channels] / 32768.0 + self._eps\n",
        "        if audio.shape[0] < self._audio_max_len_samples:\n",
        "            zero_pad = np.random.rand(self._audio_max_len_samples - audio.shape[0], audio.shape[1])*self._eps\n",
        "            audio = np.vstack((audio, zero_pad))\n",
        "        elif audio.shape[0] > self._audio_max_len_samples:\n",
        "            audio = audio[:self._audio_max_len_samples, :]\n",
        "        return audio, fs\n",
        "\n",
        "    # INPUT FEATURES\n",
        "    @staticmethod\n",
        "    def _next_greater_power_of_2(x):\n",
        "        return 2 ** (x - 1).bit_length()\n",
        "\n",
        "    def _spectrogram(self, audio_input):\n",
        "      \"\"\"\n",
        "        The function generates the spectrogram of a audio received as input\n",
        "        Parameters\n",
        "        -------------\n",
        "          :audio_input: audio received as input\n",
        "        Return\n",
        "        -------------\n",
        "          :spectra: spectrogram of the audio received as input\n",
        "      \"\"\"\n",
        "      _nb_ch = audio_input.shape[1]\n",
        "      nb_bins = self._nfft // 2\n",
        "      spectra = np.zeros((self._max_feat_frames, nb_bins + 1, _nb_ch), dtype=complex)\n",
        "      for ch_cnt in range(_nb_ch):\n",
        "          stft_ch = librosa.core.stft(np.asfortranarray(audio_input[:, ch_cnt]), n_fft=self._nfft, hop_length=self._hop_len,\n",
        "                                        win_length=self._win_len, window='hann')\n",
        "          spectra[:, :, ch_cnt] = stft_ch[:, :self._max_feat_frames].T\n",
        "      return spectra\n",
        "\n",
        "    def _get_mel_spectrogram(self, linear_spectra_list):\n",
        "        \"\"\"\n",
        "        The function generates the list of log mel spectrogram of an audio, together with the list of augmented audio generated\n",
        "        Parameters\n",
        "        -------------\n",
        "          :audio_input: list of audio received as input\n",
        "        Return\n",
        "        -------------\n",
        "          :spectra: spectrogram of the audio received as input\n",
        "        \"\"\"\n",
        "        \n",
        "        mel_feat_list = []\n",
        "        for spec in range(len(linear_spectra_list)):\n",
        "            linear_spectra = linear_spectra_list[spec]\n",
        "            mel_feat = np.zeros((linear_spectra.shape[0], self._nb_mel_bins, linear_spectra.shape[-1]))\n",
        "            for ch_cnt in range(linear_spectra.shape[-1]):\n",
        "                mag_spectra = np.abs(linear_spectra[:, :, ch_cnt])**2\n",
        "                mel_spectra = np.dot(mag_spectra, self._mel_wts)\n",
        "                log_mel_spectra = librosa.power_to_db(mel_spectra)\n",
        "                mel_feat[:, :, ch_cnt] = log_mel_spectra\n",
        "            mel_feat = mel_feat.reshape((linear_spectra.shape[0], self._nb_mel_bins * linear_spectra.shape[-1]))\n",
        "            mel_feat_list.append(mel_feat)\n",
        "\n",
        "        return mel_feat_list\n",
        "\n",
        "    def _get_foa_intensity_vectors(self, linear_spectra_list):\n",
        "        \"\"\"\n",
        "        Function to generate the list of acoustic intensity vector of the list of log mel spectrogram received as input\n",
        "        Parameters\n",
        "        -------------\n",
        "          :linear_spectra_list: list of log mel spectrogram received as input\n",
        "        Return\n",
        "        -------------\n",
        "          :foa_iv_list: list of acoustic intensity vector \n",
        "        \"\"\"\n",
        "        foa_iv_list = []\n",
        "        for spec in range(len(linear_spectra_list)):\n",
        "            linear_spectra = linear_spectra_list[spec]\n",
        "            IVx = np.real(np.conj(linear_spectra[:, :, 0]) * linear_spectra[:, :, 1])\n",
        "            IVy = np.real(np.conj(linear_spectra[:, :, 0]) * linear_spectra[:, :, 2])\n",
        "            IVz = np.real(np.conj(linear_spectra[:, :, 0]) * linear_spectra[:, :, 3])\n",
        "\n",
        "            normal = np.sqrt(IVx ** 2 + IVy ** 2 + IVz ** 2) + self._eps\n",
        "            IVx = np.dot(IVx / normal, self._mel_wts)\n",
        "            IVy = np.dot(IVy / normal, self._mel_wts)\n",
        "            IVz = np.dot(IVz / normal, self._mel_wts)\n",
        "\n",
        "            # we are doing the following instead of simply concatenating to keep the processing similar to mel_spec and gcc\n",
        "            foa_iv = np.dstack((IVx, IVy, IVz))\n",
        "            foa_iv = foa_iv.reshape((linear_spectra.shape[0], self._nb_mel_bins * 3))\n",
        "            if np.isnan(foa_iv).any():\n",
        "                print('Feature extraction is generating nan outputs')\n",
        "                exit()\n",
        "            foa_iv_list.append(foa_iv)\n",
        "\n",
        "        return foa_iv_list\n",
        "\n",
        "\n",
        "\n",
        "    def _get_gcc(self, linear_spectra_list):\n",
        "        \"\"\"\n",
        "        The fucntion generates the list of generalized cross-correlation vector of the list of log mel spectrogram received as input\n",
        "        Parameters\n",
        "        -------------\n",
        "          :linear_spectra_list: list of log mel spectrogram received as input\n",
        "        Return\n",
        "        -------------\n",
        "          :gcc_feat_list: list of generalized cross-validation vector  \n",
        "        \"\"\"\n",
        "        gcc_feat_list = []\n",
        "        for linear_spectra in linear_spectra_list:\n",
        "            gcc_channels = nCr(linear_spectra.shape[-1], 2)\n",
        "            gcc_feat = np.zeros((linear_spectra.shape[0], self._nb_mel_bins, gcc_channels))\n",
        "            cnt = 0\n",
        "            for m in range(linear_spectra.shape[-1]):\n",
        "                for n in range(m+1, linear_spectra.shape[-1]):\n",
        "                    R = np.conj(linear_spectra[:, :, m]) * linear_spectra[:, :, n]\n",
        "                    cc = np.fft.irfft(np.exp(1.j*np.angle(R)))\n",
        "                    cc = np.concatenate((cc[:, -self._nb_mel_bins//2:], cc[:, :self._nb_mel_bins//2]), axis=-1)\n",
        "                    gcc_feat[:, :, cnt] = cc\n",
        "                    cnt += 1\n",
        "            gcc_feat = gcc_feat.reshape((linear_spectra.shape[0], self._nb_mel_bins*gcc_channels))\n",
        "            gcc_feat_list.append(gcc_feat)\n",
        "\n",
        "        return gcc_feat_list\n",
        "\n",
        "    def _get_spectrogram_for_file(self, audio_filename):\n",
        "        \"\"\"\n",
        "        The function generates the list of spectogram for file. The list contains the original spectrogram of the audio and the augmented ones\n",
        "        Parameters\n",
        "        -------------\n",
        "          :audio_filename: filename of the audio file\n",
        "        Return\n",
        "        -------------\n",
        "          :list_spec: list of spectrogram (original + augmented)\n",
        "        \"\"\"\n",
        "        list_spec = []\n",
        "        \n",
        "        if dataset == 'foa':\n",
        "          pattern = np.zeros(data_augmentation_nb, dtype=int)\n",
        "\n",
        "        audio_in, fs = self._load_audio(os.path.join(self._aud_dir, audio_filename))\n",
        "        audio_spec = self._spectrogram(audio_in)\n",
        "\n",
        "        list_spec.append(audio_spec)\n",
        "\n",
        "        if int(audio_filename[4]) in train_splits and dataset == 'foa' and not self._is_eval:\n",
        "            for i in range(data_augmentation_nb):\n",
        "                audio_aug, pattern[i] = apply_data_augmentation(audio_in)\n",
        "                audio_spec_aug = self._spectrogram(audio_aug)\n",
        "                list_spec.append(audio_spec_aug)\n",
        "\n",
        "        if dataset == 'foa':\n",
        "          return list_spec, pattern\n",
        "        else:\n",
        "          return list_spec\n",
        "\n",
        "\n",
        "    # OUTPUT LABELS\n",
        "    def get_labels_for_file(self, _desc_file):\n",
        "        \"\"\"\n",
        "        The function reads description file and returns classification based SED labels and regression based DOA labels\n",
        "        Parameters:\n",
        "        ----------------\n",
        "          :param _desc_file: metadata description file\n",
        "        Returns:\n",
        "        ----------------------------\n",
        "        :return: label_mat: labels of the format [sed_label, doa_label],\n",
        "              where sed_label is of dimension [nb_frames, nb_classes] which is 1 for active sound event else zero\n",
        "              where doa_labels is of dimension [nb_frames, 3*nb_classes], nb_classes each for x, y, z axis,\n",
        "        \"\"\"\n",
        "        se_label = np.zeros((self._max_label_frames, len(self._unique_classes)))\n",
        "        x_label = np.zeros((self._max_label_frames, len(self._unique_classes)))\n",
        "        y_label = np.zeros((self._max_label_frames, len(self._unique_classes)))\n",
        "        z_label = np.zeros((self._max_label_frames, len(self._unique_classes)))\n",
        "\n",
        "        for frame_ind, active_event_list in _desc_file.items():\n",
        "            if frame_ind < self._max_label_frames:\n",
        "                for active_event in active_event_list:\n",
        "                    se_label[frame_ind, active_event[0]] = 1\n",
        "                    x_label[frame_ind, active_event[0]] = active_event[1]\n",
        "                    y_label[frame_ind, active_event[0]] = active_event[2]\n",
        "                    z_label[frame_ind, active_event[0]] = active_event[3]\n",
        "\n",
        "        label_mat = np.concatenate((se_label, x_label, y_label, z_label), axis=1)\n",
        "        return label_mat\n",
        "\n",
        "    # ------------------------------- EXTRACT FEATURE AND PREPROCESS IT -------------------------------\n",
        "    def extract_all_feature(self):\n",
        "        \"\"\"\n",
        "          The function extracts all the features from audio data file\n",
        "        \"\"\"\n",
        "        # setting up folders\n",
        "        self._feat_dir = self.get_unnormalized_feat_dir()\n",
        "        create_folder(self._feat_dir)\n",
        "\n",
        "        # extraction starts\n",
        "        print('Extracting spectrogram:')\n",
        "        print('\\t\\taud_dir {}\\n\\t\\tdesc_dir {}\\n\\t\\tfeat_dir {}'.format(\n",
        "            self._aud_dir, self._desc_dir, self._feat_dir))\n",
        "\n",
        "        for file_cnt, file_name in enumerate(os.listdir(self._aud_dir)):\n",
        "            #save the pattern implemented for each file\n",
        "            if file_name != '.DS_Store':\n",
        "                wav_filename = '{}.wav'.format(file_name.split('.')[0])\n",
        "                if dataset == 'foa':\n",
        "                  # implementation of data augmnetation based on channel rotations\n",
        "                  spect, pattern = self._get_spectrogram_for_file(wav_filename)\n",
        "                else:\n",
        "                  spect = self._get_spectrogram_for_file(wav_filename)\n",
        "\n",
        "                #extract mel\n",
        "                mel_spect = self._get_mel_spectrogram(spect)\n",
        "\n",
        "                feat_list = []\n",
        "                if self._dataset is 'foa':\n",
        "                    # extract intensity vectors\n",
        "                    foa_iv = self._get_foa_intensity_vectors(spect)\n",
        "                    for foa_index in range(len(foa_iv)):\n",
        "\n",
        "                        #plot figures\n",
        "                        #print(\"Plotting\")\n",
        "                        #plot.figure(figsize=(22, 5))\n",
        "\n",
        "                        #plot.subplot(211, xlabel='Time', ylabel='Frequency', title='mel-spect %s' % (foa_index)), \\\n",
        "                        #plot.imshow(mel_spect[foa_index].T)\n",
        "\n",
        "\n",
        "                        #plot.subplot(212, xlabel='Time', ylabel='Frequency', title='intensity vector'), \\\n",
        "                        #plot.imshow(foa_iv[foa_index].T)\n",
        "                        #plot.savefig(fname=image_dir + '%s_plot_%s.png' % (file_name.split('.')[0], foa_index), format='png')\n",
        "                        #plot.close()\n",
        "\n",
        "                        feat = np.concatenate((mel_spect[foa_index], foa_iv[foa_index]), axis=-1)\n",
        "                        feat_list.append(feat)\n",
        "\n",
        "                elif self._dataset is 'mic':\n",
        "                    # extract gcc\n",
        "                    gcc = self._get_gcc(spect)\n",
        "                    for gcc_index in range(len(gcc)):\n",
        "\n",
        "                        # plot figures\n",
        "                        # print(\"Plotting\")\n",
        "                        #plot.figure(figsize=(22, 5))\n",
        "\n",
        "                        #plot.subplot(211, xlabel='Time', ylabel='Frequency', title='mel-spect %s' % (gcc_index))\n",
        "                        #plot.imshow(mel_spect[gcc_index].T)\n",
        "\n",
        "                        #plot.subplot(212, xlabel='Time', ylabel='Frequency', title='generilized cross corrrelation')\n",
        "                        #plot.imshow(gcc[gcc_index].T)\n",
        "                        #plot.savefig(fname=image_dir + '%s_plot_%s.png' % (file_name.split('.')[0], gcc_index),\n",
        "                         #            format='png')\n",
        "\n",
        "                        feat = np.concatenate((mel_spect[gcc_index], gcc[gcc_index]), axis=-1)\n",
        "                        feat_list.append(feat)\n",
        "                else:\n",
        "                    print('ERROR: Unknown dataset format {}'.format(self._dataset))\n",
        "                    exit()\n",
        "\n",
        "                if len(feat_list) != 0:\n",
        "                    for element in range(len(feat_list)):\n",
        "                        print('{}: {}, {}, {}'.format(file_cnt, file_name, feat_list[element].shape, element))\n",
        "                        if element == 0:\n",
        "                          #no need to change the label name \n",
        "                          np.save(os.path.join(self._feat_dir, '{}-{}.npy'.format(wav_filename.split('.')[0], element)), feat_list[element]) \n",
        "                        else:\n",
        "                          #augmented data\n",
        "                          np.save(os.path.join(self._feat_dir, '{}-{}-{}.npy'.format(wav_filename.split('.')[0], element, pattern[element-1])), feat_list[element])\n",
        "                          #creation of augmented data labels\n",
        "                          label_folder = self._desc_dir\n",
        "                          label_path = os.path.join(label_folder, wav_filename.replace('.wav', '.csv'))\n",
        "                          label_dir_polar = self.load_output_format_file(label_path)\n",
        "                          label_dir = self.convert_output_format_polar_to_cartesian(label_dir_polar)\n",
        "                          #make the same rotation for the label and save with the same name on the metadata folder \n",
        "                          label_aug_dict = label_augmentation(label_dir, pattern[element-1])\n",
        "                          label_aug_dict_pol = self.convert_output_format_cartesian_to_polar(label_aug_dict)\n",
        "                          output_file_path = os.path.join(label_folder, '{}-{}-{}.csv'.format(wav_filename.split('.')[0], element, pattern[element-1]))\n",
        "                          self.write_polar_file(output_file_path, label_aug_dict_pol)\n",
        "                          \n",
        "                      \n",
        "\n",
        "\n",
        "    def preprocess_features(self):\n",
        "        \"\"\"\n",
        "          The function pre-processes and normalizes all the features already extracted using StandardScalar preprocessing\n",
        "        \"\"\"\n",
        "        # Setting up folders and filenames\n",
        "        self._feat_dir = self.get_unnormalized_feat_dir()\n",
        "        self._feat_dir_norm = self.get_normalized_feat_dir()\n",
        "        create_folder(self._feat_dir_norm)\n",
        "        normalized_features_wts_file = self.get_normalized_wts_file()\n",
        "        spec_scaler = None\n",
        "\n",
        "        # pre-processing starts\n",
        "        if self._is_eval:\n",
        "            spec_scaler = joblib.load(normalized_features_wts_file)\n",
        "            print('Normalized_features_wts_file: {}. Loaded.'.format(normalized_features_wts_file))\n",
        "\n",
        "        else:\n",
        "            #normalization using partial fit only on training dataset\n",
        "            print('Estimating weights for normalizing feature files:')\n",
        "            print('\\t\\tfeat_dir: {}'.format(self._feat_dir))\n",
        "\n",
        "            spec_scaler = preprocessing.StandardScaler()\n",
        "            for file_cnt, file_name in enumerate(os.listdir(self._feat_dir)):\n",
        "                print('{}: {}'.format(file_cnt, file_name))\n",
        "                feat_file = np.load(os.path.join(self._feat_dir, file_name))\n",
        "                spec_scaler.partial_fit(feat_file)\n",
        "                del feat_file\n",
        "            joblib.dump(\n",
        "                spec_scaler,\n",
        "                normalized_features_wts_file\n",
        "            )\n",
        "            print('Normalized_features_wts_file: {}. Saved.'.format(normalized_features_wts_file))\n",
        "\n",
        "        #transformign all the dataset\n",
        "        print('Normalizing feature files:')\n",
        "        print('\\t\\tfeat_dir_norm {}'.format(self._feat_dir_norm))\n",
        "        for file_cnt, file_name in enumerate(os.listdir(self._feat_dir)):\n",
        "            print('{}: {}'.format(file_cnt, file_name))\n",
        "            feat_file = np.load(os.path.join(self._feat_dir, file_name))\n",
        "            feat_file = spec_scaler.transform(feat_file)\n",
        "            np.save(\n",
        "                os.path.join(self._feat_dir_norm, file_name),\n",
        "                feat_file\n",
        "            )\n",
        "            del feat_file\n",
        "\n",
        "        print('normalized files written to {}'.format(self._feat_dir_norm))\n",
        "\n",
        "    # ------------------------------- EXTRACT LABELS AND PREPROCESS IT -------------------------------\n",
        "    def extract_all_labels(self):\n",
        "        \"\"\"\n",
        "         The function properly extracts all the labels\n",
        "        \"\"\"\n",
        "        self._label_dir = self.get_label_dir()\n",
        "\n",
        "        print('Extracting labels:')\n",
        "        print('\\t\\taud_dir {}\\n\\t\\tdesc_dir {}\\n\\t\\tlabel_dir {}'.format(\n",
        "            self._aud_dir, self._desc_dir, self._label_dir))\n",
        "        create_folder(self._label_dir)\n",
        "\n",
        "        for file_cnt, file_name in enumerate(os.listdir(self._desc_dir)):\n",
        "            wav_filename = '{}.wav'.format(file_name.split('.')[0])\n",
        "            desc_file_polar = self.load_output_format_file(os.path.join(self._desc_dir, file_name))\n",
        "            desc_file = self.convert_output_format_polar_to_cartesian(desc_file_polar)\n",
        "            label_mat = self.get_labels_for_file(desc_file)\n",
        "            print('{}: {}, {}'.format(file_cnt, file_name, label_mat.shape))\n",
        "            np.save(os.path.join(self._label_dir, '{}.npy'.format(wav_filename.split('.')[0])), label_mat)\n",
        "\n",
        "    # -------------------------------  DCASE OUTPUT  FORMAT FUNCTIONS -------------------------------\n",
        "    def load_output_format_file(self, _output_format_file):\n",
        "        \"\"\"\n",
        "         The function loads DCASE output format csv file and returns it in dictionary format\n",
        "        Parameters:\n",
        "        -------------------\n",
        "          :param _output_format_file: DCASE output format CSV\n",
        "        Returns:\n",
        "        -------------\n",
        "          :return: _output_dict: dictionary\n",
        "        \"\"\"\n",
        "        _output_dict = {}\n",
        "        _fid = open(_output_format_file, 'r')\n",
        "        for _line in _fid:\n",
        "            _words = _line.strip().split(',')\n",
        "            _frame_ind = int(_words[0])\n",
        "            if _frame_ind not in _output_dict:\n",
        "                _output_dict[_frame_ind] = []\n",
        "            if len(_words) == 5: #read polar coordinates format, we ignore the track count \n",
        "                _output_dict[_frame_ind].append([int(_words[1]), float(_words[3]), float(_words[4])])\n",
        "            elif len(_words) == 6: # read Cartesian coordinates format, we ignore the track count\n",
        "                _output_dict[_frame_ind].append([int(_words[1]), float(_words[3]), float(_words[4]), float(_words[5])])\n",
        "        _fid.close()\n",
        "        return _output_dict\n",
        "\n",
        "    def write_output_format_file(self, _output_format_file, _output_format_dict):\n",
        "        \"\"\"\n",
        "        The function writes DCASE output format csv file, given output format dictionary\n",
        "        Parameters:\n",
        "        -----------------------\n",
        "        :param _output_format_file: file name to write\n",
        "        :param _output_format_dict: output dictionary\n",
        "        \"\"\"\n",
        "        _fid = open(_output_format_file, 'w')\n",
        "        # _fid.write('{},{},{},{}\\n'.format('frame number with 20ms hop (int)', 'class index (int)', 'azimuth angle (int)', 'elevation angle (int)'))\n",
        "        for _frame_ind in _output_format_dict.keys():\n",
        "            for _value in _output_format_dict[_frame_ind]:\n",
        "                # Write Cartesian format output. Since baseline does not estimate track count we use a fixed value.\n",
        "                _fid.write('{},{},{},{},{},{}\\n'.format(int(_frame_ind), int(_value[0]), 0, float(_value[1]), float(_value[2]), float(_value[3])))\n",
        "        _fid.close()\n",
        "\n",
        "    def write_polar_file(self, output_file, output_format_dict):\n",
        "        \"\"\"\n",
        "        The function writes DCASE output format csv file in polar coordinates, given output format dictionary\n",
        "        Parameters:\n",
        "        -----------------------\n",
        "        :param _output_format_file: file name to write\n",
        "        :param _output_format_dict: output dictionary\n",
        "        \"\"\"\n",
        "        _fid = open(output_file, 'w')\n",
        "        # _fid.write('{},{},{},{}\\n'.format('frame number with 20ms hop (int)', 'class index (int)', 'azimuth angle (int)', 'elevation angle (int)'))\n",
        "        for _frame_ind in output_format_dict.keys():\n",
        "            for _value in output_format_dict[_frame_ind]:\n",
        "              # Write polar coordinates format output. Since baseline does not estimate track count we use a fixed value.\n",
        "                _fid.write('{},{},{},{},{}\\n'.format(int(_frame_ind), int(_value[0]), 0, int(_value[1]), int(_value[2])))\n",
        "    \n",
        "        _fid.close()\n",
        "\n",
        "    def segment_labels(self, _pred_dict, _max_frames):\n",
        "        '''\n",
        "        The function Collects class-wise sound event location information in segments of length 1s from reference dataset\n",
        "        Paremeters:\n",
        "        -----------------\n",
        "        :param _pred_dict: Dictionary containing frame-wise sound event time and location information. Output of SELD method\n",
        "        :param _max_frames: Total number of frames in the recording\n",
        "        Return:\n",
        "        -----------------\n",
        "        :return: Dictionary containing class-wise sound event location information in each segment of audio\n",
        "                dictionary_name[segment-index][class-index] = list(frame-cnt-within-segment, azimuth, elevation)\n",
        "        '''\n",
        "        nb_blocks = int(np.ceil(_max_frames/float(self._nb_label_frames_1s)))\n",
        "        output_dict = {x: {} for x in range(nb_blocks)}\n",
        "        for frame_cnt in range(0, _max_frames, self._nb_label_frames_1s):\n",
        "\n",
        "            # Collect class-wise information for each block\n",
        "            # [class][frame] = <list of doa values>\n",
        "            # Data structure supports multi-instance occurence of same class\n",
        "            block_cnt = frame_cnt // self._nb_label_frames_1s\n",
        "            loc_dict = {}\n",
        "            for audio_frame in range(frame_cnt, frame_cnt+self._nb_label_frames_1s):\n",
        "                if audio_frame not in _pred_dict:\n",
        "                    continue\n",
        "                for value in _pred_dict[audio_frame]:\n",
        "                    if value[0] not in loc_dict:\n",
        "                        loc_dict[value[0]] = {}\n",
        "\n",
        "                    block_frame = audio_frame - frame_cnt\n",
        "                    if block_frame not in loc_dict[value[0]]:\n",
        "                        loc_dict[value[0]][block_frame] = []\n",
        "                    loc_dict[value[0]][block_frame].append(value[1:])\n",
        "\n",
        "            # Update the block wise details collected above in a global structure\n",
        "            for class_cnt in loc_dict:\n",
        "                if class_cnt not in output_dict[block_cnt]:\n",
        "                    output_dict[block_cnt][class_cnt] = []\n",
        "\n",
        "                keys = [k for k in loc_dict[class_cnt]]\n",
        "                values = [loc_dict[class_cnt][k] for k in loc_dict[class_cnt]]\n",
        "\n",
        "                output_dict[block_cnt][class_cnt].append([keys, values])\n",
        "\n",
        "        return output_dict\n",
        "\n",
        "    def regression_label_format_to_output_format(self, _sed_labels, _doa_labels):\n",
        "        \"\"\"\n",
        "        The function converts the sed (classification) and doa labels predicted in regression format to dcase output format.\n",
        "        Paremeters:\n",
        "        -----------------\n",
        "        :param _sed_labels: SED labels matrix [nb_frames, nb_classes]\n",
        "        :param _doa_labels: DOA labels matrix [nb_frames, 2*nb_classes] or [nb_frames, 3*nb_classes]\n",
        "        Return:\n",
        "        ------------------  \n",
        "        :return: _output_dict: returns a dict containing dcase output format\n",
        "        \"\"\"\n",
        "        _nb_classes = len(self._unique_classes)\n",
        "        _is_polar = _doa_labels.shape[-1] == 2*_nb_classes\n",
        "        _azi_labels, _ele_labels = None, None\n",
        "        _x, _y, _z = None, None, None\n",
        "        if _is_polar:\n",
        "            _azi_labels = _doa_labels[:, :_nb_classes]\n",
        "            _ele_labels = _doa_labels[:, _nb_classes:]\n",
        "        else:\n",
        "            _x = _doa_labels[:, :_nb_classes]\n",
        "            _y = _doa_labels[:, _nb_classes:2*_nb_classes]\n",
        "            _z = _doa_labels[:, 2*_nb_classes:]\n",
        "\n",
        "        _output_dict = {}\n",
        "        for _frame_ind in range(_sed_labels.shape[0]):\n",
        "            _tmp_ind = np.where(_sed_labels[_frame_ind, :])\n",
        "            if len(_tmp_ind[0]):\n",
        "                _output_dict[_frame_ind] = []\n",
        "                for _tmp_class in _tmp_ind[0]:\n",
        "                    if _is_polar:\n",
        "                        _output_dict[_frame_ind].append([_tmp_class, _azi_labels[_frame_ind, _tmp_class], _ele_labels[_frame_ind, _tmp_class]])\n",
        "                    else:\n",
        "                        _output_dict[_frame_ind].append([_tmp_class, _x[_frame_ind, _tmp_class], _y[_frame_ind, _tmp_class], _z[_frame_ind, _tmp_class]])\n",
        "        return _output_dict\n",
        "\n",
        "    def convert_output_format_polar_to_cartesian(self, in_dict):\n",
        "        \"\"\"\n",
        "        The function converts the output format from polar coordinates to cartesian coordinates\n",
        "        Paremeters:\n",
        "        -----------------\n",
        "        :in_dict: dictionary output format in polar coordinates to be converted in cartesian coordinates\n",
        "        Return:\n",
        "        ------------------  \n",
        "        :return: out_dict: returns a dict containing in_dict data converted from polar to cartesian coordinates\n",
        "        \"\"\"\n",
        "        out_dict = {}\n",
        "        for frame_cnt in in_dict.keys():\n",
        "            if frame_cnt not in out_dict:\n",
        "                out_dict[frame_cnt] = []\n",
        "                for tmp_val in in_dict[frame_cnt]:\n",
        "\n",
        "                    ele_rad = tmp_val[2]*np.pi/180.\n",
        "                    azi_rad = tmp_val[1]*np.pi/180\n",
        "\n",
        "                    tmp_label = np.cos(ele_rad)\n",
        "                    x = np.cos(azi_rad) * tmp_label\n",
        "                    y = np.sin(azi_rad) * tmp_label\n",
        "                    z = np.sin(ele_rad)\n",
        "                    out_dict[frame_cnt].append([tmp_val[0], x, y, z])\n",
        "        return out_dict\n",
        "\n",
        "    def convert_output_format_cartesian_to_polar(self, in_dict):\n",
        "        \"\"\"\n",
        "        The function converts the output format from cartesian coordinates to poolar coordinates\n",
        "        Paremeters:\n",
        "        -----------------\n",
        "        :in_dict: dictionary output format in cartesian coordinates to be converted in polar coordinates\n",
        "        Return:\n",
        "        ------------------  \n",
        "        :return: out_dict: returns a dict containing in_dict data converted from cartesian to polar coordinates\n",
        "        \"\"\"\n",
        "        out_dict = {}\n",
        "        for frame_cnt in in_dict.keys():\n",
        "            if frame_cnt not in out_dict:\n",
        "                out_dict[frame_cnt] = []\n",
        "                for tmp_val in in_dict[frame_cnt]:\n",
        "                    x, y, z = tmp_val[1], tmp_val[2], tmp_val[3]\n",
        "\n",
        "                    # in degrees\n",
        "                    azimuth = np.arctan2(y, x) * 180 / np.pi\n",
        "                    elevation = np.arctan2(z, np.sqrt(x**2 + y**2)) * 180 / np.pi\n",
        "                    r = np.sqrt(x**2 + y**2 + z**2)\n",
        "                    out_dict[frame_cnt].append([tmp_val[0], azimuth, elevation])\n",
        "        return out_dict\n",
        "\n",
        "    # ------------------------------- Misc public functions -------------------------------\n",
        "    def get_classes(self):\n",
        "        \"\"\"\n",
        "        The function returns the unique classes dictionary\n",
        "        \"\"\"\n",
        "        return self._unique_classes\n",
        "\n",
        "    def get_normalized_feat_dir(self):\n",
        "        \"\"\"\n",
        "        The function returns the normalized feature folder path\n",
        "        \"\"\"\n",
        "        return os.path.join(\n",
        "            self._feat_label_dir,\n",
        "            '{}_norm'.format(self._dataset_combination)\n",
        "        )\n",
        "\n",
        "    def get_unnormalized_feat_dir(self):\n",
        "        \"\"\"\n",
        "        The function returns the unnormalized feature folder path\n",
        "        \"\"\"\n",
        "        return os.path.join(\n",
        "            self._feat_label_dir,\n",
        "            '{}'.format(self._dataset_combination)\n",
        "        )\n",
        "\n",
        "    def get_label_dir(self):\n",
        "        \"\"\"\n",
        "        The function returns the label feature folder path (if it is not the evaluation dataset)\n",
        "        \"\"\"\n",
        "        if self._is_eval:\n",
        "            return None\n",
        "        else:\n",
        "            return os.path.join(\n",
        "                self._feat_label_dir, '{}_label'.format(self._dataset_combination)\n",
        "            )\n",
        "\n",
        "    def get_normalized_wts_file(self):\n",
        "        \"\"\"\n",
        "        The function returns the weighthed pre-process file \n",
        "        \"\"\"\n",
        "        return os.path.join(\n",
        "            self._feat_label_dir,\n",
        "            '{}_wts'.format(self._dataset)\n",
        "        )\n",
        "\n",
        "    def get_nb_channels(self):\n",
        "        \"\"\"\n",
        "        The function returns the number of channels considered\n",
        "        \"\"\"\n",
        "        return self._nb_channels\n",
        "\n",
        "    def get_nb_classes(self):\n",
        "        \"\"\"\n",
        "        The function returns the number of classes considered\n",
        "        \"\"\"\n",
        "        return len(self._unique_classes)\n",
        "\n",
        "    def nb_frames_1s(self):\n",
        "        \"\"\"\n",
        "        The function returns the number of frame contained in 1s\n",
        "        \"\"\"\n",
        "        return self._nb_label_frames_1s\n",
        "\n",
        "    def get_hop_len_sec(self):\n",
        "        \"\"\"\n",
        "        The function returns the hop lenght in seconds\n",
        "        \"\"\"\n",
        "        return self._hop_len_s\n",
        "\n",
        "    def get_nb_frames(self):\n",
        "        \"\"\"\n",
        "        The function returns the maximum number of frames considered\n",
        "        \"\"\"\n",
        "        return self._max_label_frames\n",
        "\n",
        "    def get_nb_mel_bins(self):\n",
        "        \"\"\"\n",
        "        The function returns the number of mel band considered\n",
        "        \"\"\"\n",
        "        return self._nb_mel_bins\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mneKRotR8ALG",
        "colab_type": "text"
      },
      "source": [
        "## Running the algorithm \n",
        "\n",
        "The next cell runs the features extraction algorithm related to the development dataset. It first extracts all the features, secondly pre-processes the features (normalization process) and, lastly, it extracts the corresponding labels. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3sXqVHxCSSN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "999b8ec1-aa9f-4dbc-c0e8-c6036f558997"
      },
      "source": [
        "# Extracts the features, labels, and normalizes the development and evaluation split features.\n",
        "if 'dev' in process_str:\n",
        "    # -------------- Extract features and labels for development set -----------------------------\n",
        "    dev_feat_cls = FeatureClass(is_eval=False)\n",
        "\n",
        "    # Extract features and normalize them\n",
        "    dev_feat_cls.extract_all_feature()\n",
        "    dev_feat_cls.preprocess_features()\n",
        "\n",
        "    # # Extract labels in regression mode\n",
        "    dev_feat_cls.extract_all_labels()\n",
        "\n",
        "    print(\"Development dataset extraction finished\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Development dataset extraction finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivf-uZlGRMJi",
        "colab_type": "text"
      },
      "source": [
        "The next cell runs the features extraction algorithm related to the evaluation dataset. It first extracts all the features and then pre-processes the data. \n",
        "Labels are not extracted in this case, because there is not ground truth for the predictions to be made. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcQImVZeTZoW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce1de851-b164-4f5e-e53a-890ae0d3359f"
      },
      "source": [
        "if 'eval' in process_str:\n",
        "    # -----------------------------Extract ONLY features for evaluation set-----------------------------\n",
        "    eval_feat_cls = FeatureClass(is_eval=True)\n",
        "\n",
        "    # Extract features and normalize them\n",
        "    eval_feat_cls.extract_all_feature()\n",
        "    eval_feat_cls.preprocess_features()\n",
        "    print(\"Evaluation dataset extraction finished\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation dataset extraction finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}