{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFr_5CNN_1x48_mse_dropCNNRNN_AR_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJRz4TmfGB0B",
        "colab_type": "text"
      },
      "source": [
        "##SELDTnet for DCASE Challenge 2020\n",
        "\n",
        "We already have all the data pre-processed, this notebook aim to train the SELDnet train taking as features the input data coming from the pro-processed data, inlcuding the augmented ones and predict on new data used as testing. \n",
        "Version: Keras baseline system, channels first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_Or5LtqG3QS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing the right version of tensorflow\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I3-jaV3G3xp",
        "colab_type": "code",
        "outputId": "755cba0a-fb4a-4e7f-f776-5afc9aad2ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#making sure the tensorflow version is the correct one \n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pScgmzzNG5US",
        "colab_type": "code",
        "outputId": "6c660e63-3aa7-4ed0-b78e-f45e3c0734cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6Vp5_CNGdW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing all the libraries we will need \n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plot\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBKn5fMyHd10",
        "colab_type": "text"
      },
      "source": [
        "## Parameters definition\n",
        "\n",
        "Here will be defined all the paramaters needed to define anc configure the networks we will use for the SELD task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "275oeMl8XAP2",
        "colab_type": "code",
        "outputId": "ca1235f6-44fb-419b-a16e-e441f9768651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Parameters used in the feature extraction, neural network model, and training the SELDnet can be changed here.\n",
        "# different cases to select different dataset type\n",
        "case = 1\n",
        "\n",
        "# DATASET LOADING PARAMETERS\n",
        "# 'dev' - development or 'eval' - evaluation dataset\n",
        "mode='eval'\n",
        "# 'foa' - ambisonic or 'mic' - microphone signals                                               \n",
        "dataset='foa' \n",
        "\n",
        "# quick test a nd data augmentation flag\n",
        "quick_test=False     \n",
        "\n",
        "#dataset name could be 'daa or 'dar' or 'base'\n",
        "dataset_name = 'dataset-eval' \n",
        "dataset_aug = False if dataset == 'mic' else True\n",
        "\n",
        "#network could be keras or TF\n",
        "network = 'TFr-5CNN-1x48-mse-dropCNNRNN-AR2' \n",
        "#BASE PATH\n",
        "network_name = network + '-{}'.format(dataset)\n",
        "base_dir = '/content/drive/My Drive/Dataset-FP/'\n",
        "\n",
        "# INPUT PATH\n",
        "# Base folder containing the foa/mic and metadata folders\n",
        "dataset_dir = \"\".join([base_dir, dataset_name])\n",
        "# Directory where extracted features and labels    \n",
        "feat_label_dir= os.path.join(dataset_dir, 'feat_label-AR/')  \n",
        "\n",
        "# OUTPUT PATH\n",
        "output_dir = \"\".join([base_dir, dataset_name, '-output/', network_name])\n",
        "# Dumps the trained models and training curves in this folder\n",
        "#model_dir=os.path.join(output_dir, 'models/')\n",
        "# If true, dumps the results recording-wise in 'dcase_dir' path.          \n",
        "dcase_output=True\n",
        "# Dumps the recording-wise network output in this folder\n",
        "#dcase_dir = os.path.join(output_dir, 'results/')   \n",
        "# Train for maximum epochs                                 \n",
        "nb_epochs=50                                \n",
        "                                                                                                                                                                                  \n",
        "\n",
        "#FEATURE PARAMS\n",
        "fs=24000\n",
        "hop_len_s=0.02\n",
        "label_hop_len_s=0.1\n",
        "max_audio_len_s=60\n",
        "nb_mel_bins=64\n",
        "\n",
        "# DNN MODEL PARAMETERS\n",
        "# Feature sequence length\n",
        "label_sequence_length=60\n",
        "# Batch size                                  \n",
        "batch_size=128\n",
        "# Dropout rate, constant for all layers                                           \n",
        "dropout_rate_cnn=0.2\n",
        "dropout_rate_rnn = 0.2\n",
        "dropout_rate = 0\n",
        "# Number of CNN nodes, constant for each layer                                            \n",
        "nb_cnn2d_filt=64                                           \n",
        "\n",
        "# CNN frequency pooling, length of list = number of CNN layers, list value = pooling per layer\n",
        "f_pool_size=[2, 2, 2, 2, 2]                                      \n",
        "\n",
        "# RNN contents, length of list = number of layers, list value = number of nodes\n",
        "rnn_size=[128, 128] \n",
        "# FNN contents, length of list = number of layers, list value = number of nodes                                      \n",
        "fnn_size=[128]\n",
        "# [sed, doa] weight for scaling the DNN outputs                                           \n",
        "loss_weights=[1., 1000.] \n",
        "# Number of epochs per fit                                               \n",
        "epochs_per_fit=5\n",
        "# supports: mse, masked_mse. mse- original seld approach; masked_mse - dcase 2020 approach                                           \n",
        "doa_objective='mse'     \n",
        "\n",
        "        \n",
        "#METRIC PARAMETERS\n",
        "lad_doa_thresh=20\n",
        "sed_threshold=0.5\n",
        "nb_classes = 14\n",
        "\n",
        "feature_label_resolution = int(label_hop_len_s // hop_len_s)\n",
        "feature_sequence_length = label_sequence_length * feature_label_resolution\n",
        "# CNN time pooling\n",
        "t_pool_size = [feature_label_resolution, 1, 1, 1, 1]\n",
        "# Stop training if patience is reached       \n",
        "patience = 4                \n",
        "\n",
        "unique_classes = {\n",
        "            'alarm': 0,\n",
        "            'baby': 1,\n",
        "            'crash': 2,\n",
        "            'dog': 3,\n",
        "            'engine': 4,\n",
        "            'female_scream': 5,\n",
        "            'female_speech': 6,\n",
        "            'fire': 7,\n",
        "            'footsteps': 8,\n",
        "            'knock': 9,\n",
        "            'male_scream': 10,\n",
        "            'male_speech': 11,\n",
        "            'phone': 12,\n",
        "            'piano': 13\n",
        "        }\n",
        "\n",
        "\n",
        "    # ########### User defined parameters ##############\n",
        "    # different user parameters so to set dev or eval mode and foa or mic dataset, or quick test \n",
        "if case == 1:\n",
        "  print(\"USING DEFAULT PARAMETERS\\n\")\n",
        "\n",
        "elif case == 2:\n",
        "  mode = 'dev'\n",
        "  dataset = 'mic'\n",
        "\n",
        "elif case == 3:\n",
        "   mode = 'eval'\n",
        "   dataset = 'mic'\n",
        "\n",
        "elif case == 4:\n",
        "    mode = 'dev'\n",
        "    dataset = 'foa'\n",
        "\n",
        "elif case == 5:\n",
        "    mode = 'eval'\n",
        "    dataset = 'foa'\n",
        "\n",
        "elif case == 999:\n",
        "      print(\"QUICK TEST MODE\\n\")\n",
        "      quick_test = True\n",
        "      epochs_per_fit = 1\n",
        "\n",
        "else:\n",
        "    print('ERROR: unknown argument {}'.format(case))\n",
        "    exit()\n",
        "       "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USING DEFAULT PARAMETERS\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb2Xa7_TOcX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#utils function\n",
        "#creation of folder\n",
        "def create_folder(folder_name):\n",
        "    if not os.path.exists(folder_name):\n",
        "        print('{} folder does not exist, creating it.'.format(folder_name))\n",
        "        os.makedirs(folder_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diMFE0CcN7qT",
        "colab_type": "text"
      },
      "source": [
        "## Feature class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFzgazc7N5RT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Contains routines for labels creation, features extraction and normalization\n",
        "import librosa\n",
        "import math\n",
        "import random\n",
        "\n",
        "\n",
        "def nCr(n, r):\n",
        "    return math.factorial(n) // math.factorial(r) // math.factorial(n-r)\n",
        "\n",
        "\n",
        "class FeatureClass:\n",
        "    def __init__(self, is_eval=False):\n",
        "        \"\"\"\n",
        "        :param is_eval: if True, does not load dataset labels.\n",
        "        \"\"\"\n",
        "\n",
        "        # Input directories\n",
        "        self._feat_label_dir = feat_label_dir\n",
        "        self._dataset_dir = dataset_dir\n",
        "        self._dataset_combination = '{}_{}'.format(dataset, 'eval' if is_eval else 'dev')\n",
        "        self._aud_dir = os.path.join(self._dataset_dir, self._dataset_combination)\n",
        "\n",
        "        self._desc_dir = None if is_eval else os.path.join(self._dataset_dir, 'metadata_dev')\n",
        "\n",
        "        # Output directories\n",
        "        self._label_dir = None\n",
        "        self._feat_dir = None\n",
        "        self._feat_dir_norm = None\n",
        "\n",
        "        # Local parameters\n",
        "        self._is_eval = is_eval\n",
        "\n",
        "        self._fs = fs\n",
        "        self._hop_len_s = hop_len_s\n",
        "        self._hop_len = int(self._fs * self._hop_len_s)\n",
        "\n",
        "        self._label_hop_len_s = label_hop_len_s\n",
        "        self._label_hop_len = int(self._fs * self._label_hop_len_s)\n",
        "        self._label_frame_res = self._fs / float(self._label_hop_len)\n",
        "        self._nb_label_frames_1s = int(self._label_frame_res)\n",
        "\n",
        "        self._win_len = 2 * self._hop_len\n",
        "        self._nfft = self._next_greater_power_of_2(self._win_len)\n",
        "        self._nb_mel_bins = nb_mel_bins\n",
        "        self._mel_wts = librosa.filters.mel(sr=self._fs, n_fft=self._nfft, n_mels=self._nb_mel_bins).T\n",
        "\n",
        "        self._dataset = dataset\n",
        "        self._eps = 1e-8\n",
        "        self._nb_channels = 4\n",
        "\n",
        "        # Sound event classes dictionary\n",
        "        self._unique_classes = unique_classes\n",
        "        self._audio_max_len_samples = max_audio_len_s * self._fs  # TODO: Fix the audio synthesis code to always generate 60s of\n",
        "        # audio. Currently it generates audio till the last active sound event, which is not always 60s long. This is a\n",
        "        # quick fix to overcome that. We need this because, for processing and training we need the length of features\n",
        "        # to be fixed.\n",
        "\n",
        "        self._max_feat_frames = int(np.ceil(self._audio_max_len_samples / float(self._hop_len)))\n",
        "        self._max_label_frames = int(np.ceil(self._audio_max_len_samples / float(self._label_hop_len)))\n",
        "\n",
        "    @staticmethod\n",
        "    def _next_greater_power_of_2(x):\n",
        "        return 2 ** (x - 1).bit_length()\n",
        "\n",
        "    # -------------------------------  DCASE OUTPUT  FORMAT FUNCTIONS -------------------------------\n",
        "    \n",
        "\n",
        "    def write_output_format_file(self, _output_format_file, _output_format_dict):\n",
        "        \"\"\"\n",
        "        Writes DCASE output format csv file, given output format dictionary\n",
        "\n",
        "        :param _output_format_file:\n",
        "        :param _output_format_dict:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        _fid = open(_output_format_file, 'w')\n",
        "        for _frame_ind in _output_format_dict.keys():\n",
        "            for _value in _output_format_dict[_frame_ind]:\n",
        "                # Write Cartesian format output. Since baseline does not estimate track count we use a fixed value.\n",
        "                _fid.write('{},{},{},{},{},{}\\n'.format(int(_frame_ind), int(_value[0]), 0, float(_value[1]), float(_value[2]), float(_value[3])))\n",
        "        _fid.close()\n",
        "\n",
        "    def segment_labels(self, _pred_dict, _max_frames):\n",
        "        '''\n",
        "            Collects class-wise sound event location information in segments of length 1s from reference dataset\n",
        "        :param _pred_dict: Dictionary containing frame-wise sound event time and location information. Output of SELD method\n",
        "        :param _max_frames: Total number of frames in the recording\n",
        "        :return: Dictionary containing class-wise sound event location information in each segment of audio\n",
        "                dictionary_name[segment-index][class-index] = list(frame-cnt-within-segment, azimuth, elevation)\n",
        "        '''\n",
        "        nb_blocks = int(np.ceil(_max_frames/float(self._nb_label_frames_1s)))\n",
        "        output_dict = {x: {} for x in range(nb_blocks)}\n",
        "        for frame_cnt in range(0, _max_frames, self._nb_label_frames_1s):\n",
        "\n",
        "            # Collect class-wise information for each block\n",
        "            # [class][frame] = <list of doa values>\n",
        "            # Data structure supports multi-instance occurence of same class\n",
        "            block_cnt = frame_cnt // self._nb_label_frames_1s\n",
        "            loc_dict = {}\n",
        "            for audio_frame in range(frame_cnt, frame_cnt+self._nb_label_frames_1s):\n",
        "                if audio_frame not in _pred_dict:\n",
        "                    continue\n",
        "                for value in _pred_dict[audio_frame]:\n",
        "                    if value[0] not in loc_dict:\n",
        "                        loc_dict[value[0]] = {}\n",
        "\n",
        "                    block_frame = audio_frame - frame_cnt\n",
        "                    if block_frame not in loc_dict[value[0]]:\n",
        "                        loc_dict[value[0]][block_frame] = []\n",
        "                    loc_dict[value[0]][block_frame].append(value[1:])\n",
        "\n",
        "            # Update the block wise details collected above in a global structure\n",
        "            for class_cnt in loc_dict:\n",
        "                if class_cnt not in output_dict[block_cnt]:\n",
        "                    output_dict[block_cnt][class_cnt] = []\n",
        "\n",
        "                keys = [k for k in loc_dict[class_cnt]]\n",
        "                values = [loc_dict[class_cnt][k] for k in loc_dict[class_cnt]]\n",
        "\n",
        "                output_dict[block_cnt][class_cnt].append([keys, values])\n",
        "\n",
        "        return output_dict\n",
        "\n",
        "    def regression_label_format_to_output_format(self, _sed_labels, _doa_labels):\n",
        "        \"\"\"\n",
        "        Converts the sed (classification) and doa labels predicted in regression format to dcase output format.\n",
        "\n",
        "        :param _sed_labels: SED labels matrix [nb_frames, nb_classes]\n",
        "        :param _doa_labels: DOA labels matrix [nb_frames, 2*nb_classes] or [nb_frames, 3*nb_classes]\n",
        "        :return: _output_dict: returns a dict containing dcase output format\n",
        "        \"\"\"\n",
        "\n",
        "        _nb_classes = len(self._unique_classes)\n",
        "        _is_polar = _doa_labels.shape[-1] == 2*_nb_classes\n",
        "        _azi_labels, _ele_labels = None, None\n",
        "        _x, _y, _z = None, None, None\n",
        "        if _is_polar:\n",
        "            _azi_labels = _doa_labels[:, :_nb_classes]\n",
        "            _ele_labels = _doa_labels[:, _nb_classes:]\n",
        "        else:\n",
        "            _x = _doa_labels[:, :_nb_classes]\n",
        "            _y = _doa_labels[:, _nb_classes:2*_nb_classes]\n",
        "            _z = _doa_labels[:, 2*_nb_classes:]\n",
        "\n",
        "        _output_dict = {}\n",
        "        for _frame_ind in range(_sed_labels.shape[0]):\n",
        "            _tmp_ind = np.where(_sed_labels[_frame_ind, :])\n",
        "            if len(_tmp_ind[0]):\n",
        "                _output_dict[_frame_ind] = []\n",
        "                for _tmp_class in _tmp_ind[0]:\n",
        "                    if _is_polar:\n",
        "                        _output_dict[_frame_ind].append([_tmp_class, _azi_labels[_frame_ind, _tmp_class], _ele_labels[_frame_ind, _tmp_class]])\n",
        "                    else:\n",
        "                        _output_dict[_frame_ind].append([_tmp_class, _x[_frame_ind, _tmp_class], _y[_frame_ind, _tmp_class], _z[_frame_ind, _tmp_class]])\n",
        "        return _output_dict\n",
        "\n",
        "    # ------------------------------- Misc public functions -------------------------------\n",
        "    def get_classes(self):\n",
        "        return self._unique_classes\n",
        "\n",
        "    def get_normalized_feat_dir(self):\n",
        "        return os.path.join(\n",
        "            self._feat_label_dir,\n",
        "            '{}_norm'.format(self._dataset_combination)\n",
        "        )\n",
        "\n",
        "\n",
        "    def get_label_dir(self):\n",
        "        if self._is_eval:\n",
        "            return None\n",
        "        else:\n",
        "            return os.path.join(\n",
        "                self._feat_label_dir, '{}_label'.format(self._dataset_combination)\n",
        "            )\n",
        "\n",
        "    def get_nb_classes(self):\n",
        "        return len(self._unique_classes)\n",
        "\n",
        "    def nb_frames_1s(self):\n",
        "        return self._nb_label_frames_1s\n",
        "\n",
        "    def get_hop_len_sec(self):\n",
        "        return self._hop_len_s\n",
        "\n",
        "    def get_nb_frames(self):\n",
        "        return self._max_label_frames\n",
        "\n",
        "    def get_nb_mel_bins(self):\n",
        "        return self._nb_mel_bins\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4kXXlRsPiAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#utils functions\n",
        "def collect_test_labels(_data_gen_test, _data_out, _nb_classes, quick_test):\n",
        "    \n",
        "    # Collecting ground truth for test data\n",
        "    nb_batch = 2 if quick_test else _data_gen_test.get_total_batches_in_data()\n",
        "\n",
        "    batch_size = _data_out[0][0]\n",
        "    gt_sed = np.zeros((nb_batch * batch_size, _data_out[0][1], _data_out[0][2]))\n",
        "    gt_doa = np.zeros((nb_batch * batch_size, _data_out[0][1], _data_out[1][2]))\n",
        "\n",
        "    print(\"nb_batch in test: {}\".format(nb_batch))\n",
        "    cnt = 0\n",
        "    for tmp_feat, tmp_label in _data_gen_test.generate():\n",
        "        gt_sed[cnt * batch_size:(cnt + 1) * batch_size, :, :] = tmp_label[0]\n",
        "        if _data_gen_test.get_data_gen_mode():\n",
        "            doa_label = tmp_label[1]\n",
        "        else:\n",
        "          if doa_objective == 'masked_mse':\n",
        "            doa_label = tmp_label[1][:, :, _nb_classes:]\n",
        "          elif doa_objective == 'mse':\n",
        "            doa_label = tmp_label[1][:, :, :]\n",
        "        \n",
        "        gt_doa[cnt * batch_size:(cnt + 1) * batch_size, :, :] = doa_label\n",
        "        cnt = cnt + 1\n",
        "        if cnt == nb_batch:\n",
        "            break\n",
        "    return gt_sed.astype(int), gt_doa\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU4c8OjrguOU",
        "colab_type": "text"
      },
      "source": [
        "## Data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGo2iqpxej5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Data generator for training the SELDnet\n",
        "#\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "class DataGenerator(object):\n",
        "    def __init__(\n",
        "            self, split=1, shuffle=True, per_file=False, is_eval=False\n",
        "    ):\n",
        "        self._per_file = per_file\n",
        "        self._is_eval = is_eval\n",
        "        self._splits = np.array(split)\n",
        "        self._batch_size = batch_size\n",
        "        self._feature_seq_len = feature_sequence_length\n",
        "        self._label_seq_len = label_sequence_length\n",
        "        self._shuffle = shuffle\n",
        "        self._feat_cls = FeatureClass(is_eval=self._is_eval)\n",
        "        self._label_dir = self._feat_cls.get_label_dir()\n",
        "        self._feat_dir = self._feat_cls.get_normalized_feat_dir()\n",
        "        self._data_aug = dataset_aug\n",
        "\n",
        "        self._filenames_list = list()\n",
        "        self._nb_frames_file = 0     # Using a fixed number of frames in feat files. Updated in _get_label_filenames_sizes()\n",
        "        self._nb_mel_bins = self._feat_cls.get_nb_mel_bins()\n",
        "        self._nb_ch = None\n",
        "        self._label_len = None  # total length of label - DOA + SED\n",
        "        self._doa_len = None    # DOA label length\n",
        "        self._class_dict = self._feat_cls.get_classes()\n",
        "        self._nb_classes = self._feat_cls.get_nb_classes()\n",
        "        self._get_filenames_list_and_feat_label_sizes()\n",
        "\n",
        "        self._feature_batch_seq_len = self._batch_size*self._feature_seq_len\n",
        "        self._label_batch_seq_len = self._batch_size*self._label_seq_len\n",
        "        self._circ_buf_feat = None\n",
        "        self._circ_buf_label = None\n",
        "\n",
        "        if self._per_file:\n",
        "            self._nb_total_batches = len(self._filenames_list)\n",
        "        else:\n",
        "            self._nb_total_batches = int(np.floor((len(self._filenames_list) * self._nb_frames_file /\n",
        "                                               float(self._feature_batch_seq_len))))\n",
        "\n",
        "\n",
        "        print(\n",
        "            '\\tDatagen_mode: {}, nb_files: {}, nb_classes:{}\\n'\n",
        "            '\\tnb_frames_file: {}, feat_len: {}, nb_ch: {}, label_len:{}\\n'.format(\n",
        "                'eval' if self._is_eval else 'dev', len(self._filenames_list),  self._nb_classes,\n",
        "                self._nb_frames_file, self._nb_mel_bins, self._nb_ch, self._label_len\n",
        "                )\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            '\\tDataset: {}, split: {}\\n'\n",
        "            '\\tbatch_size: {}, feat_seq_len: {}, label_seq_len: {}, shuffle: {}\\n'\n",
        "            '\\tTotal batches in dataset: {}\\n'\n",
        "            '\\tlabel_dir: {}\\n '\n",
        "            '\\tfeat_dir: {}\\n'.format(\n",
        "                dataset, split,\n",
        "                self._batch_size, self._feature_seq_len, self._label_seq_len, self._shuffle,\n",
        "                self._nb_total_batches,\n",
        "                self._label_dir, self._feat_dir\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def get_data_sizes(self):\n",
        "        feat_shape = (self._batch_size, self._nb_ch, self._feature_seq_len, self._nb_mel_bins)\n",
        "        if self._is_eval:\n",
        "            label_shape = None\n",
        "        else:\n",
        "            label_shape = [\n",
        "                (self._batch_size, self._label_seq_len, self._nb_classes),\n",
        "                (self._batch_size, self._label_seq_len, self._nb_classes*3)\n",
        "            ]\n",
        "        return feat_shape, label_shape\n",
        "\n",
        "    def get_total_batches_in_data(self):\n",
        "        return self._nb_total_batches\n",
        "\n",
        "    def _get_filenames_list_and_feat_label_sizes(self):\n",
        "        for filename in os.listdir(self._feat_dir):\n",
        "            if self._is_eval:\n",
        "              self._filenames_list.append(filename)\n",
        "            else:\n",
        "              if int(filename[4]) in self._splits:\n",
        "                self._filenames_list.append(filename)\n",
        "\n",
        "        temp_feat = np.load(os.path.join(self._feat_dir, self._filenames_list[0]))\n",
        "        self._nb_frames_file = temp_feat.shape[0]\n",
        "        self._nb_ch = temp_feat.shape[1] // self._nb_mel_bins\n",
        "\n",
        "        if not self._is_eval:\n",
        "            filename = self._filenames_list[0].split('-')[0] + '.npy'\n",
        "            temp_label = np.load(os.path.join(self._label_dir, filename))\n",
        "            self._label_len = temp_label.shape[-1]\n",
        "            self._doa_len = (self._label_len - self._nb_classes)//self._nb_classes\n",
        "\n",
        "        if self._per_file:\n",
        "            self._batch_size = int(np.ceil(temp_feat.shape[0]/float(self._feature_seq_len)))\n",
        "\n",
        "        return\n",
        "\n",
        "    def generate(self):\n",
        "        \"\"\"\n",
        "        Generates batches of samples\n",
        "        :return: \n",
        "        \"\"\"\n",
        "\n",
        "        while 1:\n",
        "            if self._shuffle:\n",
        "                random.shuffle(self._filenames_list)\n",
        "\n",
        "            # Ideally this should have been outside the while loop. But while generating the test data we want the data\n",
        "            # to be the same exactly for all epoch's hence we keep it here.\n",
        "            self._circ_buf_feat = deque()\n",
        "            self._circ_buf_label = deque()\n",
        "\n",
        "            file_cnt = 0\n",
        "            if self._is_eval:\n",
        "                for i in range(self._nb_total_batches):\n",
        "                    # load feat and label to circular buffer. Always maintain atleast one batch worth feat and label in the\n",
        "                    # circular buffer. If not keep refilling it.\n",
        "                    while len(self._circ_buf_feat) < self._feature_batch_seq_len:\n",
        "                        temp_feat = np.load(os.path.join(self._feat_dir, self._filenames_list[file_cnt]))\n",
        "\n",
        "                        for row_cnt, row in enumerate(temp_feat):\n",
        "                            self._circ_buf_feat.append(row)\n",
        "\n",
        "                        # If self._per_file is True, this returns the sequences belonging to a single audio recording\n",
        "                        if self._per_file:\n",
        "                            extra_frames = self._feature_batch_seq_len - temp_feat.shape[0]\n",
        "                            extra_feat = np.ones((extra_frames, temp_feat.shape[1])) * 1e-6\n",
        "\n",
        "                            for row_cnt, row in enumerate(extra_feat):\n",
        "                                self._circ_buf_feat.append(row)\n",
        "\n",
        "                        file_cnt = file_cnt + 1\n",
        "\n",
        "                    # Read one batch size from the circular buffer\n",
        "                    feat = np.zeros((self._feature_batch_seq_len, self._nb_mel_bins * self._nb_ch))\n",
        "                    for j in range(self._feature_batch_seq_len):\n",
        "                        feat[j, :] = self._circ_buf_feat.popleft()\n",
        "                    feat = np.reshape(feat, (self._feature_batch_seq_len, self._nb_mel_bins, self._nb_ch))\n",
        "\n",
        "                    # Split to sequences\n",
        "                    feat = self._split_in_seqs(feat, self._feature_seq_len)\n",
        "\n",
        "                    yield feat\n",
        "\n",
        "            else:\n",
        "                for i in range(self._nb_total_batches):\n",
        "\n",
        "                    # load feat and label to circular buffer. Always maintain atleast one batch worth feat and label in the\n",
        "                    # circular buffer. If not keep refilling it.\n",
        "                    while len(self._circ_buf_feat) < self._feature_batch_seq_len:\n",
        "                        temp_feat = np.load(os.path.join(self._feat_dir, self._filenames_list[file_cnt]))\n",
        "                        if int(self._filenames_list[file_cnt].split('-')[1].split('.')[0]) == 0:\n",
        "                          #print('caso 0: ', self._filenames_list[file_cnt])\n",
        "                          label_name = (self._filenames_list[file_cnt].split('-')[0]) + '.npy'\n",
        "                          temp_label = np.load(os.path.join(self._label_dir, label_name))\n",
        "                        else:\n",
        "                          #print(self._filenames_list[file_cnt])\n",
        "                          temp_label = np.load(os.path.join(self._label_dir, self._filenames_list[file_cnt]))\n",
        "\n",
        "                        for f_row in temp_feat:\n",
        "                            self._circ_buf_feat.append(f_row)\n",
        "                        for l_row in temp_label:\n",
        "                            self._circ_buf_label.append(l_row)\n",
        "\n",
        "                        # If self._per_file is True, this returns the sequences belonging to a single audio recording\n",
        "                        if self._per_file:\n",
        "                            feat_extra_frames = self._feature_batch_seq_len - temp_feat.shape[0]\n",
        "                            extra_feat = np.ones((feat_extra_frames, temp_feat.shape[1])) * 1e-6\n",
        "\n",
        "                            label_extra_frames = self._label_batch_seq_len - temp_label.shape[0]\n",
        "                            extra_labels = np.zeros((label_extra_frames, temp_label.shape[1]))\n",
        "\n",
        "                            for f_row in extra_feat:\n",
        "                                self._circ_buf_feat.append(f_row)\n",
        "                            for l_row in extra_labels:\n",
        "                                self._circ_buf_label.append(l_row)\n",
        "\n",
        "                        file_cnt = file_cnt + 1\n",
        "\n",
        "                    # Read one batch size from the circular buffer\n",
        "                    feat = np.zeros((self._feature_batch_seq_len, self._nb_mel_bins * self._nb_ch))\n",
        "                    label = np.zeros((self._label_batch_seq_len, self._label_len))\n",
        "                    for j in range(self._feature_batch_seq_len):\n",
        "                        feat[j, :] = self._circ_buf_feat.popleft()\n",
        "                    for j in range(self._label_batch_seq_len):\n",
        "                        label[j, :] = self._circ_buf_label.popleft()\n",
        "                    feat = np.reshape(feat, (self._feature_batch_seq_len, self._nb_mel_bins, self._nb_ch))\n",
        "\n",
        "                    # Split to sequences\n",
        "                    feat = self._split_in_seqs(feat, self._feature_seq_len)\n",
        "                    label = self._split_in_seqs(label, self._label_seq_len)\n",
        "\n",
        "                    if doa_objective == 'masked_mse':\n",
        "                      label = [\n",
        "                          label[:, :, :self._nb_classes],  # SED labels\n",
        "                          label #SED + DOA LABEL\n",
        "                          ]\n",
        "                    elif doa_objective == 'mse':\n",
        "                      \n",
        "                      label = [\n",
        "                          label[:, :, :self._nb_classes],  # SED labels\n",
        "                          label[:, :, self._nb_classes:]\n",
        "                          ]\n",
        "                    \n",
        "                    yield feat, label\n",
        "\n",
        "    def _split_in_seqs(self, data, _seq_len):\n",
        "        if len(data.shape) == 1:\n",
        "            if data.shape[0] % _seq_len:\n",
        "                data = data[:-(data.shape[0] % _seq_len), :]\n",
        "            data = data.reshape((data.shape[0] // _seq_len, _seq_len, 1))\n",
        "        elif len(data.shape) == 2:\n",
        "            if data.shape[0] % _seq_len:\n",
        "                data = data[:-(data.shape[0] % _seq_len), :]\n",
        "            data = data.reshape((data.shape[0] // _seq_len, _seq_len, data.shape[1]))\n",
        "        elif len(data.shape) == 3:\n",
        "            if data.shape[0] % _seq_len:\n",
        "                data = data[:-(data.shape[0] % _seq_len), :, :]\n",
        "            data = data.reshape((data.shape[0] // _seq_len, _seq_len, data.shape[1], data.shape[2]))\n",
        "        else:\n",
        "            print('ERROR: Unknown data dimensions: {}'.format(data.shape))\n",
        "            exit()\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def split_multi_channels(data, num_channels):\n",
        "        tmp = None\n",
        "        in_shape = data.shape\n",
        "        if len(in_shape) == 3:\n",
        "            hop = in_shape[2] / num_channels\n",
        "            tmp = np.zeros((in_shape[0], num_channels, in_shape[1], hop))\n",
        "            for i in range(num_channels):\n",
        "                tmp[:, i, :, :] = data[:, :, i * hop:(i + 1) * hop]\n",
        "        elif len(in_shape) == 4 and num_channels == 1:\n",
        "            tmp = np.zeros((in_shape[0], 1, in_shape[1], in_shape[2], in_shape[3]))\n",
        "            tmp[:, 0, :, :, :] = data\n",
        "        else:\n",
        "            print('ERROR: The input should be a 3D matrix but it seems to have dimensions: {}'.format(in_shape))\n",
        "            exit()\n",
        "        return tmp\n",
        "\n",
        "    def get_default_elevation(self):\n",
        "        return self._default_ele\n",
        "\n",
        "    def get_azi_ele_list(self):\n",
        "        return self._feat_cls.get_azi_ele_list()\n",
        "\n",
        "    def get_nb_classes(self):\n",
        "        return self._nb_classes\n",
        "\n",
        "    def nb_frames_1s(self):\n",
        "        return self._feat_cls.nb_frames_1s()\n",
        "\n",
        "    def get_hop_len_sec(self):\n",
        "        return self._feat_cls.get_hop_len_sec()\n",
        "\n",
        "    def get_classes(self):\n",
        "        return self._feat_cls.get_classes()\n",
        "    \n",
        "    def get_filelist(self):\n",
        "        return self._filenames_list\n",
        "\n",
        "    def get_frame_per_file(self):\n",
        "        return self._label_batch_seq_len\n",
        "\n",
        "    def get_nb_frames(self):\n",
        "        return self._feat_cls.get_nb_frames()\n",
        "    \n",
        "    def get_data_gen_mode(self):\n",
        "        return self._is_eval\n",
        "\n",
        "    def write_output_format_file(self, _out_file, _out_dict):\n",
        "        return self._feat_cls.write_output_format_file(_out_file, _out_dict)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPO0CMI6hAOh",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "Keras and first channel system "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfIq6h9HhB0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# The SELDnet architecture\n",
        "#\n",
        "\n",
        "from tensorflow.keras.layers import Bidirectional, Conv2D, MaxPooling2D, Input, Concatenate\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Reshape, Permute\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow.keras\n",
        "\n",
        "tensorflow.keras.backend.set_image_data_format('channels_last')\n",
        "\n",
        "\n",
        "def get_model(data_in, data_out, dropout_rate, nb_cnn2d_filt, f_pool_size, t_pool_size,\n",
        "              rnn_size, fnn_size, weights, doa_objective, sed_threshold):\n",
        "    # model definition\n",
        "    spec_start = Input(shape=(data_in[-2], data_in[-1], data_in[-3]))\n",
        "\n",
        "    # CNN\n",
        "    spec_cnn = spec_start\n",
        "    for i, convCnt in enumerate(f_pool_size):\n",
        "        spec_cnn = Conv2D(filters=nb_cnn2d_filt, kernel_size=(1, 48), padding='same')(spec_cnn)\n",
        "        spec_cnn = BatchNormalization(trainable=True)(spec_cnn, training=True)\n",
        "        spec_cnn = Activation('relu')(spec_cnn)\n",
        "        spec_cnn = MaxPooling2D(pool_size=(t_pool_size[i], f_pool_size[i]))(spec_cnn)\n",
        "        spec_cnn = Dropout(dropout_rate_cnn)(spec_cnn)\n",
        "\n",
        "    # RNN\n",
        "    #todo: none output\n",
        "    spec_rnn = Reshape((data_out[0][-2], spec_cnn.shape[-2]*spec_cnn.shape[-1]))(spec_cnn)\n",
        "    for nb_rnn_filt in rnn_size:\n",
        "        spec_rnn = Bidirectional(\n",
        "            GRU(nb_rnn_filt, activation='tanh', dropout=dropout_rate_rnn, recurrent_dropout=dropout_rate_rnn,\n",
        "                return_sequences=True, reset_after=False),\n",
        "            merge_mode='mul'\n",
        "        )(spec_rnn, training=True)\n",
        "\n",
        "    # FC - DOA\n",
        "    doa = spec_rnn\n",
        "    for nb_fnn_filt in fnn_size:\n",
        "        doa = TimeDistributed(Dense(nb_fnn_filt))(doa)\n",
        "        doa = Dropout(dropout_rate)(doa)\n",
        "\n",
        "    doa = TimeDistributed(Dense(data_out[1][-1]))(doa)\n",
        "    doa = Activation('tanh', name='doa_out')(doa)\n",
        "\n",
        "    # FC - SED\n",
        "    sed = spec_rnn\n",
        "    for nb_fnn_filt in fnn_size:\n",
        "        sed = TimeDistributed(Dense(nb_fnn_filt))(sed)\n",
        "        sed = Dropout(dropout_rate)(sed)\n",
        "    sed = TimeDistributed(Dense(data_out[0][-1]))(sed)\n",
        "    sed = Activation('sigmoid', name='sed_out')(sed)\n",
        "\n",
        "    model = None\n",
        "    if doa_objective is 'mse':\n",
        "        model = Model(inputs=spec_start, outputs=[sed, doa])\n",
        "        model.compile(optimizer=Adam(), loss=['binary_crossentropy', 'mse'], loss_weights=weights)\n",
        "    elif doa_objective is 'masked_mse':\n",
        "        doa_concat = Concatenate(axis=-1, name='doa_concat')([sed, doa])\n",
        "        model = Model(inputs=spec_start, outputs=[sed, doa_concat])\n",
        "        model.compile(optimizer=Adam(), loss=['binary_crossentropy', masked_mse], loss_weights=weights)\n",
        "    else:\n",
        "        print('ERROR: Unknown doa_objective: {}'.format(doa_objective))\n",
        "        exit()\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def masked_mse(y_gt, model_out):\n",
        "    # SED mask: Use only the predicted DOAs when gt SED > 0.5\n",
        "    sed_out = y_gt[:, :, :nb_classes] >= sed_threshold #TODO fix this hardcoded value of number of classes\n",
        "    sed_out = tensorflow.keras.backend.repeat_elements(sed_out, 3, -1)\n",
        "    sed_out = tensorflow.keras.backend.cast(sed_out, 'float32')\n",
        "\n",
        "    # Use the mask to computed mse now. Normalize with the mask weights #TODO fix this hardcoded value of number of classes\n",
        "    return tensorflow.keras.backend.sqrt(tensorflow.keras.backend.sum(tensorflow.keras.backend.square(y_gt[:, :, nb_classes:] - model_out[:, :, nb_classes:]) * sed_out))/tensorflow.keras.backend.sum(sed_out)\n",
        "\n",
        "\n",
        "def load_seld_model(model_file, doa_objective):\n",
        "    if doa_objective is 'mse':\n",
        "        return load_model(model_file)\n",
        "    elif doa_objective is 'masked_mse':\n",
        "        return load_model(model_file, custom_objects={'masked_mse': masked_mse})\n",
        "    else:\n",
        "        print('ERROR: Unknown doa objective: {}'.format(doa_objective))\n",
        "        exit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzmM7AGmNv4F",
        "colab_type": "text"
      },
      "source": [
        "## Parameters definition for the network and starting point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgT4oLrFHJUD",
        "colab_type": "code",
        "outputId": "84186e5f-c2af-4185-c415-f9cd93f2ac4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#variables and parameters definition\n",
        "feat_cls = FeatureClass()\n",
        "train_splits, val_splits, test_splits = None, None, None\n",
        "\n",
        "if mode == 'dev':\n",
        "      test_splits = [1]\n",
        "      val_splits = [2]\n",
        "      train_splits = [[3, 4, 5, 6]]\n",
        "\n",
        "elif mode == 'eval':\n",
        "      test_splits = [[7, 8]]\n",
        "      val_splits = [[1]]\n",
        "      train_splits = [[2, 3, 4, 5, 6]]\n",
        "\n",
        "avg_scores_val = []\n",
        "avg_scores_test = []\n",
        "\n",
        "print(\"Test split {}\".format(test_splits))\n",
        "print(\"Val split {}\".format(val_splits))\n",
        "print(\"Train split {}\".format(train_splits))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test split [[7, 8]]\n",
            "Val split [[1]]\n",
            "Train split [[2, 3, 4, 5, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4z0CjGOd5o0",
        "colab_type": "text"
      },
      "source": [
        "## Training of the network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZqY34-DomTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reshape_3Dto2D(A):\n",
        "    return A.reshape(A.shape[0] * A.shape[1], A.shape[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC4ByFWqQTbT",
        "colab_type": "code",
        "outputId": "bf114e5b-671a-4fab-87ed-28bf80e62f20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "import json\n",
        "\n",
        "for split_cnt, split in enumerate(test_splits):\n",
        "    \n",
        "    print('\\n\\n')\n",
        "    print('-' * 85)\n",
        "    print('-' * 39 + ' SPLIT {}'.format(str(split)) + '-' * 38)\n",
        "    print('-' * 85)\n",
        "    print(\"Quick test: \", quick_test)\n",
        "\n",
        "    #creation split folder\n",
        "    split_folder = os.path.join(output_dir, 'split_{}'.format(split_cnt))\n",
        "    create_folder(split_folder)\n",
        "\n",
        "    #creation prediction folder\n",
        "    prediction_folder = os.path.join(split_folder, \"\".join(['prediction_', dataset, '/']))\n",
        "    create_folder(prediction_folder)\n",
        "\n",
        "    #creation history folder\n",
        "    history_folder = os.path.join(split_folder, 'history/')\n",
        "    create_folder(history_folder)\n",
        "\n",
        "    #creation checkpoint folder\n",
        "    checkpoint_folder = os.path.join(split_folder, 'checkpoints/')\n",
        "    create_folder(checkpoint_folder)\n",
        "    checkpoint_path = checkpoint_folder + 'cp.ckpt'\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "\n",
        "    #creation model folder\n",
        "    model_dir = os.path.join(split_folder, 'models/')\n",
        "    create_folder(model_dir)\n",
        "\n",
        "    #creation of datafile path were to save all the data\n",
        "    datafilepath = split_folder + '/data.json'\n",
        "    \n",
        "    #############################################################################################\n",
        "\n",
        "    # Load train and validation data\n",
        "    print('Loading training dataset:')\n",
        "    data_gen_train = DataGenerator(\n",
        "        split=train_splits[split_cnt]\n",
        "    )\n",
        "\n",
        "    print('Loading validation dataset:')\n",
        "    data_gen_val = DataGenerator(\n",
        "        split=val_splits[split_cnt], shuffle=False\n",
        "    )\n",
        "\n",
        "    # Collect the reference labels for validation data\n",
        "    data_in, data_out = data_gen_train.get_data_sizes()\n",
        "    print('FEATURES:\\n\\tdata_in: {}\\n\\tdata_out: {}\\n'.format(data_in, data_out))\n",
        "\n",
        "    nb_classes = data_gen_train.get_nb_classes()\n",
        "    gt = collect_test_labels(data_gen_val, data_out, nb_classes, quick_test)\n",
        "    sed_gt = reshape_3Dto2D(gt[0])\n",
        "    doa_gt = reshape_3Dto2D(gt[1])\n",
        "\n",
        "    print('MODEL:\\n\\tdropout_rate: {}\\n\\tCNN: nb_cnn_filt: {}, f_pool_size{}, t_pool_size{}\\n\\trnn_size: {}, fnn_size: {}\\n\\tdoa_objective: {}\\n'.format(\n",
        "        dropout_rate, nb_cnn2d_filt, f_pool_size, t_pool_size, rnn_size, \n",
        "        fnn_size, doa_objective))\n",
        "\n",
        "    print('Using loss weights : {}'.format(loss_weights))\n",
        "    model = get_model(data_in=data_in, data_out=data_out, dropout_rate=dropout_rate,\n",
        "                                      nb_cnn2d_filt=nb_cnn2d_filt, f_pool_size=f_pool_size, t_pool_size=t_pool_size, \n",
        "                                      rnn_size=rnn_size, fnn_size=fnn_size, \n",
        "                                      weights=loss_weights, doa_objective=doa_objective, sed_threshold=sed_threshold)\n",
        "\n",
        " \n",
        "    \n",
        "    #checkpoint_callback\n",
        "    cp_callback = tensorflow.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 save_freq = data_gen_train.get_total_batches_in_data() * epochs_per_fit,\n",
        "                                                 verbose=1)\n",
        " \n",
        "    nb_epoch = 2 if quick_test else nb_epochs\n",
        "    \n",
        "    data = {}\n",
        "    epoch_cnt = -1 \n",
        "    patience_cnt = 0\n",
        "\n",
        "    # start training\n",
        "    for _ in range(nb_epoch):  \n",
        "\n",
        "      start = time.time() \n",
        "\n",
        "      #so we can start from zero\n",
        "      epoch_cnt += 1\n",
        "      data['epoch'] = epoch_cnt\n",
        "      print(\"epoch: {}\".format(epoch_cnt))\n",
        "\n",
        "\n",
        "      if len(os.listdir(checkpoint_folder)) != 0 and epoch_cnt == 0:\n",
        "\n",
        "        #we started the loop again, probabily for timeout or disconnection reason\n",
        "        print(\"Checkpoints exist: Loading weight\")\n",
        "        model.load_weights(tensorflow.train.latest_checkpoint(checkpoint_folder))\n",
        "         \n",
        "        \n",
        "        with open(datafilepath, \"r\") as fp:\n",
        "          data = json.load(fp)\n",
        "          \n",
        "        epoch_cnt = data['epoch']\n",
        "        patience_cnt = data['patience_cnt']\n",
        "        val_loss_prec = data['val_loss_prec']\n",
        "        print(\"Restoring from epoch: {}\".format(data['epoch']))\n",
        "        #print(data)\n",
        "\n",
        "      \n",
        "      with open(datafilepath, \"w+\") as fp:\n",
        "        json.dump(data, fp)\n",
        "\n",
        "      if epoch_cnt >= nb_epoch or patience_cnt >= patience:\n",
        "        print(\"Training finished\")\n",
        "        break   \n",
        "\n",
        "\n",
        "      hist = model.fit(\n",
        "                    x=data_gen_train.generate(),\n",
        "                    steps_per_epoch=2 if quick_test else data_gen_train.get_total_batches_in_data(),\n",
        "                    epochs=epochs_per_fit,\n",
        "                    callbacks=[cp_callback], \n",
        "                    validation_data = data_gen_val.generate(),\n",
        "                    validation_steps = 2 if quick_test else data_gen_val.get_total_batches_in_data(),\n",
        "      )\n",
        "      \n",
        "\n",
        "      # predict once per epoch\n",
        "      pred = model.predict(\n",
        "              x=data_gen_val.generate(),\n",
        "              steps=2 if quick_test else data_gen_val.get_total_batches_in_data(),\n",
        "              verbose=2,\n",
        "        )\n",
        "\n",
        "      ################################################################################################\n",
        "\n",
        "      #saving the model\n",
        "      model_name = '{}_model.h5'.format(epoch_cnt)\n",
        "      #print(\"model_name: {}\\n\".format(model_name))\n",
        "      model_path = model_dir + model_name\n",
        "      model.save(model_path)\n",
        "\n",
        "      pred_sed_filename = 'pred_%s_sed_%s' % (dataset, str(epoch_cnt))\n",
        "      pred_doa_filename = 'pred_%s_doa_%s' % (dataset, str(epoch_cnt))\n",
        "      \n",
        "      #training_loss_saved\n",
        "      hist_loss_filename = os.path.join(\"\".join([history_folder, 'pred_%s_%s_hist_loss.json' % (dataset, str(epoch_cnt))]))\n",
        "      with open(hist_loss_filename, \"w+\") as fp:\n",
        "        json.dump(str(hist.history), fp)\n",
        "      #print(\"File prediction loss saved {}\".format(hist_loss_filename))\n",
        "        \n",
        "      #prediction sed\n",
        "      np.save(os.path.join(prediction_folder + pred_sed_filename), pred[0])\n",
        "      #print(\"File prediction sed saved {}\".format(os.path.join(prediction_folder + pred_sed_filename)))\n",
        "\n",
        "      #prediction doa\n",
        "      np.save(os.path.join(prediction_folder + pred_doa_filename), pred[1])\n",
        "      #print(\"File prediction doa saved {}\".format(os.path.join(prediction_folder + pred_doa_filename)))\n",
        "\n",
        "      if epoch_cnt > 0 and hist.history['val_loss'][-1] >= val_loss_prec:\n",
        "        patience_cnt += 1\n",
        "      else:\n",
        "        patience_cnt = 0\n",
        "        \n",
        "      val_loss_prec = hist.history['val_loss'][-1]\n",
        "      #print(val_loss_prec)\n",
        "      data['patience_cnt'] = patience_cnt\n",
        "      data['val_loss_prec'] = val_loss_prec\n",
        "\n",
        "      #print(patience_cnt)\n",
        "      if patience_cnt >= patience:\n",
        "        print(\"Ealy stop breaking\")\n",
        "        data['nb_epochs'] = epoch_cnt\n",
        "        break\n",
        "\n",
        "      with open(datafilepath, \"w+\") as fp:\n",
        "        json.dump(data, fp)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------------------\n",
            "--------------------------------------- SPLIT [7, 8]--------------------------------------\n",
            "-------------------------------------------------------------------------------------\n",
            "Quick test:  False\n",
            "Loading training dataset:\n",
            "\tDatagen_mode: dev, nb_files: 1500, nb_classes:14\n",
            "\tnb_frames_file: 3000, feat_len: 64, nb_ch: 7, label_len:56\n",
            "\n",
            "\tDataset: foa, split: [2, 3, 4, 5, 6]\n",
            "\tbatch_size: 128, feat_seq_len: 300, label_seq_len: 60, shuffle: True\n",
            "\tTotal batches in dataset: 117\n",
            "\tlabel_dir: /content/drive/My Drive/Dataset-FP/dataset-eval/feat_label-AR/foa_dev_label\n",
            " \tfeat_dir: /content/drive/My Drive/Dataset-FP/dataset-eval/feat_label-AR/foa_dev_norm\n",
            "\n",
            "Loading validation dataset:\n",
            "\tDatagen_mode: dev, nb_files: 100, nb_classes:14\n",
            "\tnb_frames_file: 3000, feat_len: 64, nb_ch: 7, label_len:56\n",
            "\n",
            "\tDataset: foa, split: [1]\n",
            "\tbatch_size: 128, feat_seq_len: 300, label_seq_len: 60, shuffle: False\n",
            "\tTotal batches in dataset: 7\n",
            "\tlabel_dir: /content/drive/My Drive/Dataset-FP/dataset-eval/feat_label-AR/foa_dev_label\n",
            " \tfeat_dir: /content/drive/My Drive/Dataset-FP/dataset-eval/feat_label-AR/foa_dev_norm\n",
            "\n",
            "FEATURES:\n",
            "\tdata_in: (128, 7, 300, 64)\n",
            "\tdata_out: [(128, 60, 14), (128, 60, 42)]\n",
            "\n",
            "nb_batch in test: 7\n",
            "MODEL:\n",
            "\tdropout_rate: 0\n",
            "\tCNN: nb_cnn_filt: 64, f_pool_size[2, 2, 2, 2, 2], t_pool_size[5, 1, 1, 1, 1]\n",
            "\trnn_size: [128, 128], fnn_size: [128]\n",
            "\tdoa_objective: mse\n",
            "\n",
            "Using loss weights : [1.0, 1000.0]\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 300, 64, 7)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 300, 64, 64)  21568       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 300, 64, 64)  256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 300, 64, 64)  0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 60, 32, 64)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 60, 32, 64)   0           max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 60, 32, 64)   196672      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 60, 32, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 60, 32, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 60, 16, 64)   0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 60, 16, 64)   0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 60, 16, 64)   196672      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 60, 16, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 60, 16, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 60, 8, 64)    0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 60, 8, 64)    0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 60, 8, 64)    196672      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 60, 8, 64)    256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 60, 8, 64)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 60, 4, 64)    0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 60, 4, 64)    0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 60, 4, 64)    196672      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 60, 4, 64)    256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 60, 4, 64)    0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 60, 2, 64)    0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 60, 2, 64)    0           max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 60, 128)      0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 60, 128)      197376      reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 60, 128)      197376      bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 60, 128)      16512       bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 60, 128)      16512       bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 60, 128)      0           time_distributed_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 60, 128)      0           time_distributed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 60, 14)       1806        dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 60, 42)       5418        dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "sed_out (Activation)            (None, 60, 14)       0           time_distributed_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "doa_out (Activation)            (None, 60, 42)       0           time_distributed_1[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 1,244,536\n",
            "Trainable params: 1,243,896\n",
            "Non-trainable params: 640\n",
            "__________________________________________________________________________________________________\n",
            "epoch: 0\n",
            "Checkpoints exist: Loading weight\n",
            "Restoring from epoch: 39\n",
            "Epoch 1/5\n",
            "117/117 [==============================] - 1031s 9s/step - loss: 2.2857 - sed_out_loss: 0.0266 - doa_out_loss: 0.0023 - val_loss: 13.6072 - val_sed_out_loss: 0.2138 - val_doa_out_loss: 0.0134\n",
            "Epoch 2/5\n",
            "117/117 [==============================] - 634s 5s/step - loss: 2.2513 - sed_out_loss: 0.0263 - doa_out_loss: 0.0022 - val_loss: 13.4847 - val_sed_out_loss: 0.2306 - val_doa_out_loss: 0.0133\n",
            "Epoch 3/5\n",
            "117/117 [==============================] - 648s 6s/step - loss: 2.4042 - sed_out_loss: 0.0280 - doa_out_loss: 0.0024 - val_loss: 13.8470 - val_sed_out_loss: 0.2248 - val_doa_out_loss: 0.0136\n",
            "Epoch 4/5\n",
            "117/117 [==============================] - 626s 5s/step - loss: 2.2506 - sed_out_loss: 0.0264 - doa_out_loss: 0.0022 - val_loss: 14.1462 - val_sed_out_loss: 0.2328 - val_doa_out_loss: 0.0139\n",
            "Epoch 5/5\n",
            "116/117 [============================>.] - ETA: 5s - loss: 2.2270 - sed_out_loss: 0.0262 - doa_out_loss: 0.0022 \n",
            "Epoch 00005: saving model to /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/checkpoints/cp.ckpt\n",
            "117/117 [==============================] - 695s 6s/step - loss: 2.2291 - sed_out_loss: 0.0262 - doa_out_loss: 0.0022 - val_loss: 13.6136 - val_sed_out_loss: 0.2206 - val_doa_out_loss: 0.0134\n",
            "7/7 - 4s\n",
            "epoch: 40\n",
            "Epoch 1/5\n",
            "117/117 [==============================] - 689s 6s/step - loss: 2.3114 - sed_out_loss: 0.0276 - doa_out_loss: 0.0023 - val_loss: 14.2241 - val_sed_out_loss: 0.2304 - val_doa_out_loss: 0.0140\n",
            "Epoch 2/5\n",
            "117/117 [==============================] - 678s 6s/step - loss: 2.4367 - sed_out_loss: 0.0283 - doa_out_loss: 0.0024 - val_loss: 13.6689 - val_sed_out_loss: 0.2111 - val_doa_out_loss: 0.0135\n",
            "Epoch 3/5\n",
            "117/117 [==============================] - 672s 6s/step - loss: 2.2388 - sed_out_loss: 0.0264 - doa_out_loss: 0.0022 - val_loss: 13.7635 - val_sed_out_loss: 0.2182 - val_doa_out_loss: 0.0135\n",
            "Epoch 4/5\n",
            "117/117 [==============================] - 698s 6s/step - loss: 2.1762 - sed_out_loss: 0.0259 - doa_out_loss: 0.0022 - val_loss: 13.4856 - val_sed_out_loss: 0.2149 - val_doa_out_loss: 0.0133\n",
            "Epoch 5/5\n",
            "116/117 [============================>.] - ETA: 5s - loss: 2.1981 - sed_out_loss: 0.0259 - doa_out_loss: 0.0022 \n",
            "Epoch 00005: saving model to /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/checkpoints/cp.ckpt\n",
            "117/117 [==============================] - 656s 6s/step - loss: 2.1982 - sed_out_loss: 0.0259 - doa_out_loss: 0.0022 - val_loss: 13.8335 - val_sed_out_loss: 0.2236 - val_doa_out_loss: 0.0136\n",
            "7/7 - 4s\n",
            "epoch: 41\n",
            "Epoch 1/5\n",
            "117/117 [==============================] - 667s 6s/step - loss: 2.2551 - sed_out_loss: 0.0264 - doa_out_loss: 0.0022 - val_loss: 13.9898 - val_sed_out_loss: 0.2271 - val_doa_out_loss: 0.0138\n",
            "Epoch 2/5\n",
            "117/117 [==============================] - 652s 6s/step - loss: 2.2259 - sed_out_loss: 0.0259 - doa_out_loss: 0.0022 - val_loss: 13.5777 - val_sed_out_loss: 0.2186 - val_doa_out_loss: 0.0134\n",
            "Epoch 3/5\n",
            "117/117 [==============================] - 619s 5s/step - loss: 2.1617 - sed_out_loss: 0.0254 - doa_out_loss: 0.0021 - val_loss: 13.7595 - val_sed_out_loss: 0.2270 - val_doa_out_loss: 0.0135\n",
            "Epoch 4/5\n",
            "117/117 [==============================] - 611s 5s/step - loss: 2.1765 - sed_out_loss: 0.0255 - doa_out_loss: 0.0022 - val_loss: 13.8526 - val_sed_out_loss: 0.2256 - val_doa_out_loss: 0.0136\n",
            "Epoch 5/5\n",
            "116/117 [============================>.] - ETA: 4s - loss: 2.1400 - sed_out_loss: 0.0252 - doa_out_loss: 0.0021\n",
            "Epoch 00005: saving model to /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/checkpoints/cp.ckpt\n",
            "117/117 [==============================] - 626s 5s/step - loss: 2.1473 - sed_out_loss: 0.0253 - doa_out_loss: 0.0021 - val_loss: 13.8817 - val_sed_out_loss: 0.2291 - val_doa_out_loss: 0.0137\n",
            "7/7 - 5s\n",
            "epoch: 42\n",
            "Epoch 1/5\n",
            "117/117 [==============================] - 655s 6s/step - loss: 2.1404 - sed_out_loss: 0.0252 - doa_out_loss: 0.0021 - val_loss: 13.7852 - val_sed_out_loss: 0.2216 - val_doa_out_loss: 0.0136\n",
            "Epoch 2/5\n",
            "117/117 [==============================] - 618s 5s/step - loss: 2.1788 - sed_out_loss: 0.0255 - doa_out_loss: 0.0022 - val_loss: 13.6116 - val_sed_out_loss: 0.2213 - val_doa_out_loss: 0.0134\n",
            "Epoch 3/5\n",
            "117/117 [==============================] - 613s 5s/step - loss: 2.1321 - sed_out_loss: 0.0250 - doa_out_loss: 0.0021 - val_loss: 14.1084 - val_sed_out_loss: 0.2336 - val_doa_out_loss: 0.0139\n",
            "Epoch 4/5\n",
            "117/117 [==============================] - 623s 5s/step - loss: 2.1622 - sed_out_loss: 0.0254 - doa_out_loss: 0.0021 - val_loss: 13.7854 - val_sed_out_loss: 0.2255 - val_doa_out_loss: 0.0136\n",
            "Epoch 5/5\n",
            "116/117 [============================>.] - ETA: 4s - loss: 2.2141 - sed_out_loss: 0.0259 - doa_out_loss: 0.0022\n",
            "Epoch 00005: saving model to /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/checkpoints/cp.ckpt\n",
            "117/117 [==============================] - 616s 5s/step - loss: 2.2215 - sed_out_loss: 0.0260 - doa_out_loss: 0.0022 - val_loss: 14.3690 - val_sed_out_loss: 0.2407 - val_doa_out_loss: 0.0141\n",
            "7/7 - 5s\n",
            "epoch: 43\n",
            "Epoch 1/5\n",
            "117/117 [==============================] - 600s 5s/step - loss: 2.2165 - sed_out_loss: 0.0259 - doa_out_loss: 0.0022 - val_loss: 13.5780 - val_sed_out_loss: 0.2255 - val_doa_out_loss: 0.0134\n",
            "Epoch 2/5\n",
            "117/117 [==============================] - 609s 5s/step - loss: 2.1852 - sed_out_loss: 0.0255 - doa_out_loss: 0.0022 - val_loss: 13.8515 - val_sed_out_loss: 0.2275 - val_doa_out_loss: 0.0136\n",
            "Epoch 3/5\n",
            "117/117 [==============================] - 655s 6s/step - loss: 2.1465 - sed_out_loss: 0.0252 - doa_out_loss: 0.0021 - val_loss: 13.6702 - val_sed_out_loss: 0.2257 - val_doa_out_loss: 0.0134\n",
            "Epoch 4/5\n",
            "117/117 [==============================] - 683s 6s/step - loss: 2.2437 - sed_out_loss: 0.0263 - doa_out_loss: 0.0022 - val_loss: 14.0287 - val_sed_out_loss: 0.2261 - val_doa_out_loss: 0.0138\n",
            "Epoch 5/5\n",
            "116/117 [============================>.] - ETA: 5s - loss: 2.2139 - sed_out_loss: 0.0258 - doa_out_loss: 0.0022 \n",
            "Epoch 00005: saving model to /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/checkpoints/cp.ckpt\n",
            "117/117 [==============================] - 669s 6s/step - loss: 2.2176 - sed_out_loss: 0.0257 - doa_out_loss: 0.0022 - val_loss: 14.0753 - val_sed_out_loss: 0.2269 - val_doa_out_loss: 0.0138\n",
            "7/7 - 5s\n",
            "epoch: 44\n",
            "Epoch 1/5\n",
            "117/117 [==============================] - 663s 6s/step - loss: 2.2528 - sed_out_loss: 0.0261 - doa_out_loss: 0.0022 - val_loss: 14.2882 - val_sed_out_loss: 0.2323 - val_doa_out_loss: 0.0141\n",
            "Epoch 2/5\n",
            "117/117 [==============================] - 666s 6s/step - loss: 2.2321 - sed_out_loss: 0.0261 - doa_out_loss: 0.0022 - val_loss: 14.3879 - val_sed_out_loss: 0.2314 - val_doa_out_loss: 0.0142\n",
            "Epoch 3/5\n",
            "117/117 [==============================] - 733s 6s/step - loss: 2.1625 - sed_out_loss: 0.0256 - doa_out_loss: 0.0021 - val_loss: 14.1783 - val_sed_out_loss: 0.2326 - val_doa_out_loss: 0.0139\n",
            "Epoch 4/5\n",
            "117/117 [==============================] - 726s 6s/step - loss: 2.3398 - sed_out_loss: 0.0276 - doa_out_loss: 0.0023 - val_loss: 13.6963 - val_sed_out_loss: 0.2148 - val_doa_out_loss: 0.0135\n",
            "Epoch 5/5\n",
            "116/117 [============================>.] - ETA: 5s - loss: 2.1681 - sed_out_loss: 0.0255 - doa_out_loss: 0.0021 \n",
            "Epoch 00005: saving model to /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/checkpoints/cp.ckpt\n",
            "117/117 [==============================] - 712s 6s/step - loss: 2.1670 - sed_out_loss: 0.0255 - doa_out_loss: 0.0021 - val_loss: 14.0708 - val_sed_out_loss: 0.2207 - val_doa_out_loss: 0.0139\n",
            "7/7 - 4s\n",
            "epoch: 45\n",
            "Epoch 1/5\n",
            "117/117 [==============================] - 737s 6s/step - loss: 2.3185 - sed_out_loss: 0.0276 - doa_out_loss: 0.0023 - val_loss: 13.9036 - val_sed_out_loss: 0.2252 - val_doa_out_loss: 0.0137\n",
            "Epoch 2/5\n",
            "117/117 [==============================] - 737s 6s/step - loss: 2.1907 - sed_out_loss: 0.0262 - doa_out_loss: 0.0022 - val_loss: 13.8483 - val_sed_out_loss: 0.2254 - val_doa_out_loss: 0.0136\n",
            "Epoch 3/5\n",
            "117/117 [==============================] - 708s 6s/step - loss: 2.1116 - sed_out_loss: 0.0254 - doa_out_loss: 0.0021 - val_loss: 14.0689 - val_sed_out_loss: 0.2266 - val_doa_out_loss: 0.0138\n",
            "Epoch 4/5\n",
            "117/117 [==============================] - 743s 6s/step - loss: 2.1067 - sed_out_loss: 0.0250 - doa_out_loss: 0.0021 - val_loss: 13.3365 - val_sed_out_loss: 0.2144 - val_doa_out_loss: 0.0131\n",
            "Epoch 5/5\n",
            "116/117 [============================>.] - ETA: 5s - loss: 2.0935 - sed_out_loss: 0.0250 - doa_out_loss: 0.0021 \n",
            "Epoch 00005: saving model to /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/checkpoints/cp.ckpt\n",
            "117/117 [==============================] - 693s 6s/step - loss: 2.0934 - sed_out_loss: 0.0250 - doa_out_loss: 0.0021 - val_loss: 13.4014 - val_sed_out_loss: 0.2123 - val_doa_out_loss: 0.0132\n",
            "7/7 - 4s\n",
            "epoch: 46\n",
            "Epoch 1/5\n",
            "117/117 [==============================] - 693s 6s/step - loss: 2.0735 - sed_out_loss: 0.0246 - doa_out_loss: 0.0020 - val_loss: 13.6504 - val_sed_out_loss: 0.2270 - val_doa_out_loss: 0.0134\n",
            "Epoch 2/5\n",
            "117/117 [==============================] - 655s 6s/step - loss: 2.0644 - sed_out_loss: 0.0246 - doa_out_loss: 0.0020 - val_loss: 13.6304 - val_sed_out_loss: 0.2193 - val_doa_out_loss: 0.0134\n",
            "Epoch 3/5\n",
            "117/117 [==============================] - 658s 6s/step - loss: 2.0816 - sed_out_loss: 0.0247 - doa_out_loss: 0.0021 - val_loss: 13.6344 - val_sed_out_loss: 0.2199 - val_doa_out_loss: 0.0134\n",
            "Epoch 4/5\n",
            "117/117 [==============================] - 637s 5s/step - loss: 2.1116 - sed_out_loss: 0.0251 - doa_out_loss: 0.0021 - val_loss: 13.4424 - val_sed_out_loss: 0.2183 - val_doa_out_loss: 0.0132\n",
            "Epoch 5/5\n",
            "116/117 [============================>.] - ETA: 5s - loss: 2.0661 - sed_out_loss: 0.0243 - doa_out_loss: 0.0020 \n",
            "Epoch 00005: saving model to /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/checkpoints/cp.ckpt\n",
            "117/117 [==============================] - 668s 6s/step - loss: 2.0650 - sed_out_loss: 0.0243 - doa_out_loss: 0.0020 - val_loss: 13.4638 - val_sed_out_loss: 0.2208 - val_doa_out_loss: 0.0132\n",
            "7/7 - 4s\n",
            "epoch: 47\n",
            "Epoch 1/5\n",
            "117/117 [==============================] - 649s 6s/step - loss: 2.1339 - sed_out_loss: 0.0254 - doa_out_loss: 0.0021 - val_loss: 13.2613 - val_sed_out_loss: 0.2142 - val_doa_out_loss: 0.0130\n",
            "Epoch 2/5\n",
            "117/117 [==============================] - 639s 5s/step - loss: 2.0777 - sed_out_loss: 0.0244 - doa_out_loss: 0.0021 - val_loss: 13.5786 - val_sed_out_loss: 0.2245 - val_doa_out_loss: 0.0134\n",
            "Epoch 3/5\n",
            "117/117 [==============================] - 649s 6s/step - loss: 2.0655 - sed_out_loss: 0.0246 - doa_out_loss: 0.0020 - val_loss: 14.0438 - val_sed_out_loss: 0.2304 - val_doa_out_loss: 0.0138\n",
            "Epoch 4/5\n",
            "117/117 [==============================] - 631s 5s/step - loss: 2.0488 - sed_out_loss: 0.0241 - doa_out_loss: 0.0020 - val_loss: 13.7200 - val_sed_out_loss: 0.2231 - val_doa_out_loss: 0.0135\n",
            "Epoch 5/5\n",
            "116/117 [============================>.] - ETA: 5s - loss: 2.2086 - sed_out_loss: 0.0264 - doa_out_loss: 0.0022 \n",
            "Epoch 00005: saving model to /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/checkpoints/cp.ckpt\n",
            "117/117 [==============================] - 660s 6s/step - loss: 2.2048 - sed_out_loss: 0.0264 - doa_out_loss: 0.0022 - val_loss: 13.9318 - val_sed_out_loss: 0.2275 - val_doa_out_loss: 0.0137\n",
            "7/7 - 5s\n",
            "epoch: 48\n",
            "Epoch 1/5\n",
            "117/117 [==============================] - 665s 6s/step - loss: 2.3724 - sed_out_loss: 0.0284 - doa_out_loss: 0.0023 - val_loss: 13.9082 - val_sed_out_loss: 0.2253 - val_doa_out_loss: 0.0137\n",
            "Epoch 2/5\n",
            "117/117 [==============================] - 657s 6s/step - loss: 2.1279 - sed_out_loss: 0.0250 - doa_out_loss: 0.0021 - val_loss: 13.7674 - val_sed_out_loss: 0.2236 - val_doa_out_loss: 0.0135\n",
            "Epoch 3/5\n",
            "117/117 [==============================] - 668s 6s/step - loss: 2.0655 - sed_out_loss: 0.0245 - doa_out_loss: 0.0020 - val_loss: 14.0884 - val_sed_out_loss: 0.2346 - val_doa_out_loss: 0.0139\n",
            "Epoch 4/5\n",
            "117/117 [==============================] - 676s 6s/step - loss: 2.0274 - sed_out_loss: 0.0241 - doa_out_loss: 0.0020 - val_loss: 13.9777 - val_sed_out_loss: 0.2351 - val_doa_out_loss: 0.0137\n",
            "Epoch 5/5\n",
            "116/117 [============================>.] - ETA: 5s - loss: 2.0160 - sed_out_loss: 0.0238 - doa_out_loss: 0.0020 \n",
            "Epoch 00005: saving model to /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/checkpoints/cp.ckpt\n",
            "117/117 [==============================] - 669s 6s/step - loss: 2.0111 - sed_out_loss: 0.0238 - doa_out_loss: 0.0020 - val_loss: 13.7362 - val_sed_out_loss: 0.2316 - val_doa_out_loss: 0.0135\n",
            "7/7 - 5s\n",
            "epoch: 49\n",
            "Epoch 1/5\n",
            "117/117 [==============================] - 712s 6s/step - loss: 2.1948 - sed_out_loss: 0.0261 - doa_out_loss: 0.0022 - val_loss: 14.4873 - val_sed_out_loss: 0.2340 - val_doa_out_loss: 0.0143\n",
            "Epoch 2/5\n",
            "117/117 [==============================] - 697s 6s/step - loss: 2.0410 - sed_out_loss: 0.0243 - doa_out_loss: 0.0020 - val_loss: 13.4821 - val_sed_out_loss: 0.2242 - val_doa_out_loss: 0.0133\n",
            "Epoch 3/5\n",
            "117/117 [==============================] - 693s 6s/step - loss: 2.0112 - sed_out_loss: 0.0238 - doa_out_loss: 0.0020 - val_loss: 13.6197 - val_sed_out_loss: 0.2206 - val_doa_out_loss: 0.0134\n",
            "Epoch 4/5\n",
            "117/117 [==============================] - 656s 6s/step - loss: 1.9926 - sed_out_loss: 0.0236 - doa_out_loss: 0.0020 - val_loss: 14.0831 - val_sed_out_loss: 0.2402 - val_doa_out_loss: 0.0138\n",
            "Epoch 5/5\n",
            "116/117 [============================>.] - ETA: 5s - loss: 1.9878 - sed_out_loss: 0.0237 - doa_out_loss: 0.0020 \n",
            "Epoch 00005: saving model to /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/checkpoints/cp.ckpt\n",
            "117/117 [==============================] - 676s 6s/step - loss: 1.9856 - sed_out_loss: 0.0236 - doa_out_loss: 0.0020 - val_loss: 13.5907 - val_sed_out_loss: 0.2281 - val_doa_out_loss: 0.0134\n",
            "7/7 - 5s\n",
            "epoch: 50\n",
            "Training finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFOuQXWjkJsv",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation part\n",
        "Here there will be the part for the evaluation of the network, saving the results and evaluating the netwrok. Before than that, we will define the functions which we will need for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0pY2EupnJ-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Implements the core metrics from sound event detection evaluation module http://tut-arg.github.io/sed_eval/ and\n",
        "# The DOA metrics are explained in the SELDnet paper\n",
        "#\n",
        "# This script has MIT license\n",
        "#\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "eps = np.finfo(np.float).eps\n",
        "\n",
        "\n",
        "##########################################################################################\n",
        "# SELD scoring functions - class implementation\n",
        "#\n",
        "# NOTE: Supports only one-hot labels for both SED and DOA. Doesnt work for baseline method\n",
        "# directly, since it estimated DOA in regression approach. Check below the class for\n",
        "# one shot (function) implementations of all metrics. The function implementation has\n",
        "# support for both one-hot labels and regression values of DOA estimation.\n",
        "##########################################################################################\n",
        "\n",
        "###############################################################\n",
        "# SED scoring functions\n",
        "###############################################################\n",
        "class evaluationmetric_SELDMetrics(object):\n",
        "  def __init__(self, nb_frames_1s=None, data_gen=None):\n",
        "        # SED params\n",
        "        self._S = 0\n",
        "        self._D = 0\n",
        "        self._I = 0\n",
        "        self._TP = 0\n",
        "        self._Nref = 0\n",
        "        self._Nsys = 0\n",
        "        self._block_size = nb_frames_1s\n",
        "\n",
        "        # DOA params\n",
        "        self._doa_loss_pred_cnt = 0\n",
        "        self._nb_frames = 0\n",
        "\n",
        "        self._doa_loss_pred = 0\n",
        "        self._nb_good_pks = 0\n",
        "\n",
        "        self._data_gen = data_gen\n",
        "\n",
        "        self._less_est_cnt, self._less_est_frame_cnt = 0, 0\n",
        "        self._more_est_cnt, self._more_est_frame_cnt = 0, 0\n",
        "\n",
        "  def f1_overall_framewise(O, T):\n",
        "    TP = ((2 * T - O) == 1).sum()\n",
        "    Nref, Nsys = T.sum(), O.sum()\n",
        "    self._TP += TP\n",
        "    self._Nref += Nref\n",
        "    self._Nsys += Nsys\n",
        "\n",
        "\n",
        "  def er_overall_framewise(O, T):\n",
        "      FP = np.logical_and(T == 0, O == 1).sum(1)\n",
        "      FN = np.logical_and(T == 1, O == 0).sum(1)\n",
        "      S = np.minimum(FP, FN).sum()\n",
        "      D = np.maximum(0, FN - FP).sum()\n",
        "      I = np.maximum(0, FP - FN).sum()\n",
        "      self._S += S\n",
        "      self._D += D\n",
        "      self._I += I\n",
        "\n",
        "\n",
        "  def f1_overall_1sec(O, T, block_size):\n",
        "    new_size = int(np.ceil(float(O.shape[0]) / self._block_size))\n",
        "    O_block = np.zeros((new_size, O.shape[1]))\n",
        "    T_block = np.zeros((new_size, O.shape[1]))\n",
        "    for i in range(0, new_size):\n",
        "      O_block[i, :] = np.max(O[int(i * self._block_size):int(i * self._block_size + self._block_size - 1), :], axis=0)\n",
        "      T_block[i, :] = np.max(T[int(i * self._block_size):int(i * self._block_size + self._block_size - 1), :], axis=0)\n",
        "    return self.f1_overall_framewise(O_block, T_block)\n",
        "\n",
        "  def er_overall_1sec(self, O, T):\n",
        "    new_size = int(np.ceil(float(O.shape[0]) / self._block_size))\n",
        "    O_block = np.zeros((new_size, O.shape[1]))\n",
        "    T_block = np.zeros((new_size, O.shape[1]))\n",
        "    for i in range(0, new_size):\n",
        "      O_block[i, :] = np.max(O[int(i * self._block_size):int(i * self._block_size + self._block_size - 1), :], axis=0)\n",
        "      T_block[i, :] = np.max(T[int(i * self._block_size):int(i * self._block_size + self._block_size - 1), :], axis=0)\n",
        "    return self.er_overall_framewise(O_block, T_block)\n",
        "\n",
        "  def update_sed_scores(self, pred, gt):\n",
        "    self.f1_overall_1sec(pred, gt)\n",
        "    self.er_overall_1sec(pred, gt)\n",
        "\n",
        "\n",
        "  def compute_sed_scores(self):\n",
        "\n",
        "      ER = (self._S + self._D + self._I) / (self._Nref + 0.0)\n",
        "\n",
        "      prec = float(self._TP) / float(self._Nsys + eps)\n",
        "      recall = float(self._TP) / float(self._Nref + eps)\n",
        "      F = 2 * prec * recall / (prec + recall + eps)\n",
        "\n",
        "      return ER, F\n",
        "\n",
        "  def update_doa_scores(self, pred_doa_thresholded, gt_doa):\n",
        "      self._doa_loss_pred_cnt += np.sum(pred_doa_thresholded)\n",
        "      self._nb_frames += pred_doa_thresholded.shape[0]\n",
        "\n",
        "      for frame in range(pred_doa_thresholded.shape[0]):\n",
        "        nb_gt_peaks = int(np.sum(gt_doa[frame, :]))\n",
        "        nb_pred_peaks = int(np.sum(pred_doa_thresholded[frame, :]))\n",
        "\n",
        "            # good_frame_cnt includes frames where the nb active sources were zero in both groundtruth and prediction\n",
        "        if nb_gt_peaks == nb_pred_peaks:\n",
        "          self._nb_good_pks += 1\n",
        "        elif nb_gt_peaks > nb_pred_peaks:\n",
        "          self._less_est_frame_cnt += 1\n",
        "          self._less_est_cnt += (nb_gt_peaks - nb_pred_peaks)\n",
        "        elif nb_pred_peaks > nb_gt_peaks:\n",
        "          self._more_est_frame_cnt += 1\n",
        "          self._more_est_cnt += (nb_pred_peaks - nb_gt_peaks)\n",
        "\n",
        "            # when nb_ref_doa > nb_estimated_doa, ignores the extra ref doas and scores only the nearest matching doas\n",
        "            # similarly, when nb_estimated_doa > nb_ref_doa, ignores the extra estimated doa and scores the remaining matching doas\n",
        "        if nb_gt_peaks and nb_pred_peaks:\n",
        "          pred_ind = np.where(pred_doa_thresholded[frame] == 1)[1]\n",
        "          pred_list_rad = np.array(self._data_gen .get_matrix_index(pred_ind)) * np.pi / 180\n",
        "\n",
        "          gt_ind = np.where(gt_doa[frame] == 1)[1]\n",
        "          gt_list_rad = np.array(self._data_gen .get_matrix_index(gt_ind)) * np.pi / 180\n",
        "\n",
        "          frame_dist = distance_between_gt_pred(gt_list_rad.T, pred_list_rad.T)\n",
        "          self._doa_loss_pred += frame_dist \n",
        "\n",
        "  def compute_doa_scores(self):\n",
        "        doa_error = self._doa_loss_pred / self._doa_loss_pred_cnt\n",
        "        frame_recall = self._nb_good_pks / float(self._nb_frames)\n",
        "        return doa_error, frame_recall\n",
        "\n",
        "  def reset(self):\n",
        "        # SED params\n",
        "        self._S = 0\n",
        "        self._D = 0\n",
        "        self._I = 0\n",
        "        self._TP = 0\n",
        "        self._Nref = 0\n",
        "        self._Nsys = 0\n",
        "\n",
        "        # DOA params\n",
        "        self._doa_loss_pred_cnt = 0\n",
        "        self._nb_frames = 0\n",
        "\n",
        "        self._doa_loss_pred = 0\n",
        "        self._nb_good_pks = 0\n",
        "\n",
        "        self._less_est_cnt, self._less_est_frame_cnt = 0, 0\n",
        "        self._more_est_cnt, self._more_est_frame_cnt = 0, 0\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# DOA scoring functions\n",
        "###############################################################\n",
        "\n",
        "\n",
        "def f1_overall_framewise(O, T):\n",
        "  if len(O.shape) == 3:\n",
        "      O, T = reshape_3Dto2D(O), reshape_3Dto2D(T)\n",
        "  TP = ((2 * T - O) == 1).sum()\n",
        "  Nref, Nsys = T.sum(), O.sum()\n",
        "\n",
        "  prec = float(TP) / float(Nsys + eps)\n",
        "  recall = float(TP) / float(Nref + eps)\n",
        "  f1_score = 2 * prec * recall / (prec + recall + eps)\n",
        "  return f1_score\n",
        "\n",
        "\n",
        "def er_overall_framewise(O, T):\n",
        "  if len(O.shape) == 3:\n",
        "    O, T = reshape_3Dto2D(O), reshape_3Dto2D(T)\n",
        "\n",
        "  FP = np.logical_and(T == 0, O == 1).sum(1)\n",
        "  FN = np.logical_and(T == 1, O == 0).sum(1)\n",
        "\n",
        "  S = np.minimum(FP, FN).sum()\n",
        "  D = np.maximum(0, FN-FP).sum()\n",
        "  I = np.maximum(0, FP-FN).sum()\n",
        "\n",
        "  Nref = T.sum()\n",
        "  ER = (S+D+I) / (Nref + 0.0)\n",
        "  return ER\n",
        "\n",
        "\n",
        "def f1_overall_1sec(O, T, block_size):\n",
        "  if len(O.shape) == 3:\n",
        "      O, T = reshape_3Dto2D(O), reshape_3Dto2D(T)\n",
        "  new_size = int(np.ceil(float(O.shape[0]) / block_size))\n",
        "  O_block = np.zeros((new_size, O.shape[1]))\n",
        "  T_block = np.zeros((new_size, O.shape[1]))\n",
        "  for i in range(0, new_size):\n",
        "    O_block[i, :] = np.max(O[int(i * block_size):int(i * block_size + block_size - 1), :], axis=0)\n",
        "    T_block[i, :] = np.max(T[int(i * block_size):int(i * block_size + block_size - 1), :], axis=0)\n",
        "  return f1_overall_framewise(O_block, T_block)\n",
        "\n",
        "\n",
        "def er_overall_1sec(O, T, block_size):\n",
        "  if len(O.shape) == 3:\n",
        "      O, T = reshape_3Dto2D(O), reshape_3Dto2D(T)\n",
        "  new_size = int(np.ceil(float(O.shape[0]) / block_size))\n",
        "  O_block = np.zeros((new_size, O.shape[1]))\n",
        "  T_block = np.zeros((new_size, O.shape[1]))\n",
        "  for i in range(0, new_size):\n",
        "    O_block[i, :] = np.max(O[int(i * block_size):int(i * block_size + block_size - 1), :], axis=0)\n",
        "    T_block[i, :] = np.max(T[int(i * block_size):int(i * block_size + block_size - 1), :], axis=0)\n",
        "  return er_overall_framewise(O_block, T_block)\n",
        "\n",
        "\n",
        "def compute_sed_scores(pred, gt, nb_frames_1s):\n",
        "  f1o = f1_overall_1sec(pred, gt, nb_frames_1s)\n",
        "  ero = er_overall_1sec(pred, gt, nb_frames_1s)\n",
        "  scores = [ero, f1o]\n",
        "  return scores\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# DOA scoring functions\n",
        "###############################################################\n",
        "\n",
        "\n",
        "def compute_doa_scores_regr_xyz(pred_doa, gt_doa, pred_sed, gt_sed):\n",
        "    \"\"\"\n",
        "        Compute DOA metrics when DOA is estimated using regression approach\n",
        "\n",
        "    :param pred_doa: predicted doa_labels is of dimension [nb_frames, 3*nb_classes],\n",
        "                        nb_classes each for x, y, and z axes,\n",
        "                        if active, the DOA values will be in real numbers [-1 1] range, else, it will contain default doa values of (0, 0, 0)\n",
        "    :param gt_doa: reference doa_labels is of dimension [nb_frames, 3*nb_classes],\n",
        "    :param pred_sed: predicted sed label of dimension [nb_frames, nb_classes] which is 1 for active sound event else zero\n",
        "    :param gt_sed: reference sed label of dimension [nb_frames, nb_classes] which is 1 for active sound event else zero\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    nb_src_gt_list = np.zeros(gt_doa.shape[0]).astype(int)\n",
        "    nb_src_pred_list = np.zeros(gt_doa.shape[0]).astype(int)\n",
        "    good_frame_cnt = 0\n",
        "    doa_loss_pred = 0.0\n",
        "    nb_sed = gt_sed.shape[-1]\n",
        "\n",
        "    less_est_cnt, less_est_frame_cnt = 0, 0\n",
        "    more_est_cnt, more_est_frame_cnt = 0, 0\n",
        "\n",
        "    for frame_cnt, sed_frame in enumerate(gt_sed):\n",
        "        nb_src_gt_list[frame_cnt] = int(np.sum(sed_frame))\n",
        "        nb_src_pred_list[frame_cnt] = int(np.sum(pred_sed[frame_cnt]))\n",
        "\n",
        "        # good_frame_cnt includes frames where the nb active sources were zero in both groundtruth and prediction\n",
        "        if nb_src_gt_list[frame_cnt] == nb_src_pred_list[frame_cnt]:\n",
        "            good_frame_cnt = good_frame_cnt + 1\n",
        "        elif nb_src_gt_list[frame_cnt] > nb_src_pred_list[frame_cnt]:\n",
        "            less_est_cnt = less_est_cnt + nb_src_gt_list[frame_cnt] - nb_src_pred_list[frame_cnt]\n",
        "            less_est_frame_cnt = less_est_frame_cnt + 1\n",
        "        elif nb_src_gt_list[frame_cnt] < nb_src_pred_list[frame_cnt]:\n",
        "            more_est_cnt = more_est_cnt + nb_src_pred_list[frame_cnt] - nb_src_gt_list[frame_cnt]\n",
        "            more_est_frame_cnt = more_est_frame_cnt + 1\n",
        "\n",
        "        # when nb_ref_doa > nb_estimated_doa, ignores the extra ref doas and scores only the nearest matching doas\n",
        "        # similarly, when nb_estimated_doa > nb_ref_doa, ignores the extra estimated doa and scores the remaining matching doas\n",
        "        if nb_src_gt_list[frame_cnt] and nb_src_pred_list[frame_cnt]:\n",
        "            # DOA Loss with respect to predicted confidence\n",
        "            sed_frame_gt = gt_sed[frame_cnt]\n",
        "            doa_frame_gt_x = gt_doa[frame_cnt][:nb_sed][sed_frame_gt == 1]\n",
        "            doa_frame_gt_y = gt_doa[frame_cnt][nb_sed:2*nb_sed][sed_frame_gt == 1]\n",
        "            doa_frame_gt_z = gt_doa[frame_cnt][2*nb_sed:][sed_frame_gt == 1]\n",
        "\n",
        "            sed_frame_pred = pred_sed[frame_cnt]\n",
        "            doa_frame_pred_x = pred_doa[frame_cnt][:nb_sed][sed_frame_pred == 1]\n",
        "            doa_frame_pred_y = pred_doa[frame_cnt][nb_sed:2*nb_sed][sed_frame_pred == 1]\n",
        "            doa_frame_pred_z = pred_doa[frame_cnt][2*nb_sed:][sed_frame_pred == 1]\n",
        "\n",
        "            doa_loss_pred += distance_between_gt_pred_xyz(np.vstack((doa_frame_gt_x, doa_frame_gt_y, doa_frame_gt_z)).T,\n",
        "                                                      np.vstack((doa_frame_pred_x, doa_frame_pred_y, doa_frame_pred_z)).T)\n",
        "\n",
        "    doa_loss_pred_cnt = np.sum(nb_src_pred_list)\n",
        "    if doa_loss_pred_cnt:\n",
        "        doa_loss_pred /= doa_loss_pred_cnt\n",
        "\n",
        "    frame_recall = good_frame_cnt / float(gt_sed.shape[0])\n",
        "    er_metric = [doa_loss_pred, frame_recall, doa_loss_pred_cnt, good_frame_cnt, more_est_cnt, less_est_cnt]\n",
        "    return er_metric\n",
        "\n",
        "\n",
        "def compute_doa_scores_regr(pred_doa_rad, gt_doa_rad, pred_sed, gt_sed):\n",
        "    \"\"\"\n",
        "        Compute DOA metrics when DOA is estimated using regression approach\n",
        "\n",
        "    :param pred_doa_rad: predicted doa_labels is of dimension [nb_frames, 2*nb_classes],\n",
        "                        nb_classes each for azimuth and elevation angles,\n",
        "                        if active, the DOA values will be in RADIANS, else, it will contain default doa values\n",
        "    :param gt_doa_rad: reference doa_labels is of dimension [nb_frames, 2*nb_classes],\n",
        "                    nb_classes each for azimuth and elevation angles,\n",
        "                    if active, the DOA values will be in RADIANS, else, it will contain default doa values\n",
        "    :param pred_sed: predicted sed label of dimension [nb_frames, nb_classes] which is 1 for active sound event else zero\n",
        "    :param gt_sed: reference sed label of dimension [nb_frames, nb_classes] which is 1 for active sound event else zero\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    nb_src_gt_list = np.zeros(gt_doa_rad.shape[0]).astype(int)\n",
        "    nb_src_pred_list = np.zeros(gt_doa_rad.shape[0]).astype(int)\n",
        "    good_frame_cnt = 0\n",
        "    doa_loss_pred = 0.0\n",
        "    nb_sed = gt_sed.shape[-1]\n",
        "\n",
        "    less_est_cnt, less_est_frame_cnt = 0, 0\n",
        "    more_est_cnt, more_est_frame_cnt = 0, 0\n",
        "\n",
        "    for frame_cnt, sed_frame in enumerate(gt_sed):\n",
        "        nb_src_gt_list[frame_cnt] = int(np.sum(sed_frame))\n",
        "        nb_src_pred_list[frame_cnt] = int(np.sum(pred_sed[frame_cnt]))\n",
        "\n",
        "        # good_frame_cnt includes frames where the nb active sources were zero in both groundtruth and prediction\n",
        "        if nb_src_gt_list[frame_cnt] == nb_src_pred_list[frame_cnt]:\n",
        "            good_frame_cnt = good_frame_cnt + 1\n",
        "        elif nb_src_gt_list[frame_cnt] > nb_src_pred_list[frame_cnt]:\n",
        "            less_est_cnt = less_est_cnt + nb_src_gt_list[frame_cnt] - nb_src_pred_list[frame_cnt]\n",
        "            less_est_frame_cnt = less_est_frame_cnt + 1\n",
        "        elif nb_src_gt_list[frame_cnt] < nb_src_pred_list[frame_cnt]:\n",
        "            more_est_cnt = more_est_cnt + nb_src_pred_list[frame_cnt] - nb_src_gt_list[frame_cnt]\n",
        "            more_est_frame_cnt = more_est_frame_cnt + 1\n",
        "\n",
        "        # when nb_ref_doa > nb_estimated_doa, ignores the extra ref doas and scores only the nearest matching doas\n",
        "        # similarly, when nb_estimated_doa > nb_ref_doa, ignores the extra estimated doa and scores the remaining matching doas\n",
        "        if nb_src_gt_list[frame_cnt] and nb_src_pred_list[frame_cnt]:\n",
        "            # DOA Loss with respect to predicted confidence\n",
        "            sed_frame_gt = gt_sed[frame_cnt]\n",
        "            doa_frame_gt_azi = gt_doa_rad[frame_cnt][:nb_sed][sed_frame_gt == 1]\n",
        "            doa_frame_gt_ele = gt_doa_rad[frame_cnt][nb_sed:][sed_frame_gt == 1]\n",
        "\n",
        "            sed_frame_pred = pred_sed[frame_cnt]\n",
        "            doa_frame_pred_azi = pred_doa_rad[frame_cnt][:nb_sed][sed_frame_pred == 1]\n",
        "            doa_frame_pred_ele = pred_doa_rad[frame_cnt][nb_sed:][sed_frame_pred == 1]\n",
        "\n",
        "            doa_loss_pred += distance_between_gt_pred(np.vstack((doa_frame_gt_azi, doa_frame_gt_ele)).T,\n",
        "                                                      np.vstack((doa_frame_pred_azi, doa_frame_pred_ele)).T)\n",
        "\n",
        "    doa_loss_pred_cnt = np.sum(nb_src_pred_list)\n",
        "    if doa_loss_pred_cnt:\n",
        "        doa_loss_pred /= doa_loss_pred_cnt\n",
        "\n",
        "    frame_recall = good_frame_cnt / float(gt_sed.shape[0])\n",
        "    er_metric = [doa_loss_pred, frame_recall, doa_loss_pred_cnt, good_frame_cnt, more_est_cnt, less_est_cnt]\n",
        "    return er_metric\n",
        "\n",
        "\n",
        "def compute_doa_scores_clas(pred_doa_thresholded, gt_doa, data_gen_test):\n",
        "    '''\n",
        "    Compute DOA metrics when DOA is estimated using classification approach\n",
        "\n",
        "    :param pred_doa_thresholded: predicted results of dimension [nb_frames, nb_classes, nb_azi*nb_ele],\n",
        "                                with value 1 when sound event active, else 0\n",
        "    :param gt_doa: reference results of dimension [nb_frames, nb_classes, nb_azi*nb_ele],\n",
        "                    with value 1 when sound event active, else 0\n",
        "    :param data_gen_test: feature or data generator class\n",
        "\n",
        "    :return: DOA metrics\n",
        "\n",
        "    '''\n",
        "    doa_loss_pred_cnt = np.sum(pred_doa_thresholded)\n",
        "\n",
        "    doa_loss_pred = 0\n",
        "    nb_good_pks = 0\n",
        "\n",
        "    less_est_cnt, less_est_frame_cnt = 0, 0\n",
        "    more_est_cnt, more_est_frame_cnt = 0, 0\n",
        "\n",
        "    for frame in range(pred_doa_thresholded.shape[0]):\n",
        "        nb_gt_peaks = int(np.sum(gt_doa[frame, :]))\n",
        "        nb_pred_peaks = int(np.sum(pred_doa_thresholded[frame, :]))\n",
        "\n",
        "        # good_frame_cnt includes frames where the nb active sources were zero in both groundtruth and prediction\n",
        "        if nb_gt_peaks == nb_pred_peaks:\n",
        "            nb_good_pks += 1\n",
        "        elif nb_gt_peaks > nb_pred_peaks:\n",
        "            less_est_frame_cnt += 1\n",
        "            less_est_cnt += (nb_gt_peaks - nb_pred_peaks)\n",
        "        elif nb_pred_peaks > nb_gt_peaks:\n",
        "            more_est_frame_cnt += 1\n",
        "            more_est_cnt += (nb_pred_peaks - nb_gt_peaks)\n",
        "\n",
        "        # when nb_ref_doa > nb_estimated_doa, ignores the extra ref doas and scores only the nearest matching doas\n",
        "        # similarly, when nb_estimated_doa > nb_ref_doa, ignores the extra estimated doa and scores the remaining matching doas\n",
        "        if nb_gt_peaks and nb_pred_peaks:\n",
        "            pred_ind = np.where(pred_doa_thresholded[frame] == 1)[1]\n",
        "            pred_list_rad = np.array(data_gen_test.get_matrix_index(pred_ind)) * np.pi / 180\n",
        "\n",
        "            gt_ind = np.where(gt_doa[frame] == 1)[1]\n",
        "            gt_list_rad = np.array(data_gen_test.get_matrix_index(gt_ind)) * np.pi / 180\n",
        "\n",
        "            frame_dist = distance_between_gt_pred(gt_list_rad.T, pred_list_rad.T)\n",
        "            doa_loss_pred += frame_dist\n",
        "\n",
        "    if doa_loss_pred_cnt:\n",
        "        doa_loss_pred /= doa_loss_pred_cnt\n",
        "\n",
        "    frame_recall = nb_good_pks / float(pred_doa_thresholded.shape[0])\n",
        "    er_metric = [doa_loss_pred, frame_recall, doa_loss_pred_cnt, nb_good_pks, more_est_cnt, less_est_cnt]\n",
        "    return er_metric\n",
        "\n",
        "\n",
        "def distance_between_gt_pred(gt_list_rad, pred_list_rad):\n",
        "    \"\"\"\n",
        "    Shortest distance between two sets of spherical coordinates. Given a set of groundtruth spherical coordinates,\n",
        "     and its respective predicted coordinates, we calculate the spherical distance between each of the spherical\n",
        "     coordinate pairs resulting in a matrix of distances, where one axis represents the number of groundtruth\n",
        "     coordinates and the other the predicted coordinates. The number of estimated peaks need not be the same as in\n",
        "     groundtruth, thus the distance matrix is not always a square matrix. We use the hungarian algorithm to find the\n",
        "     least cost in this distance matrix.\n",
        "\n",
        "    :param gt_list_rad: list of ground-truth spherical coordinates\n",
        "    :param pred_list_rad: list of predicted spherical coordinates\n",
        "    :return: cost -  distance\n",
        "    :return: less - number of DOA's missed\n",
        "    :return: extra - number of DOA's over-estimated\n",
        "    \"\"\"\n",
        "\n",
        "    gt_len, pred_len = gt_list_rad.shape[0], pred_list_rad.shape[0]\n",
        "    ind_pairs = np.array([[x, y] for y in range(pred_len) for x in range(gt_len)])\n",
        "    cost_mat = np.zeros((gt_len, pred_len))\n",
        "\n",
        "    # Slow implementation\n",
        "    # cost_mat = np.zeros((gt_len, pred_len))\n",
        "    # for gt_cnt, gt in enumerate(gt_list_rad):\n",
        "    #     for pred_cnt, pred in enumerate(pred_list_rad):\n",
        "    #         cost_mat[gt_cnt, pred_cnt] = distance_between_spherical_coordinates_rad(gt, pred)\n",
        "\n",
        "    # Fast implementation\n",
        "    if gt_len and pred_len:\n",
        "        az1, ele1, az2, ele2 = gt_list_rad[ind_pairs[:, 0], 0], gt_list_rad[ind_pairs[:, 0], 1], \\\n",
        "                               pred_list_rad[ind_pairs[:, 1], 0], pred_list_rad[ind_pairs[:, 1], 1]\n",
        "        cost_mat[ind_pairs[:, 0], ind_pairs[:, 1]] = distance_between_spherical_coordinates_rad(az1, ele1, az2, ele2)\n",
        "\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_mat)\n",
        "    cost = cost_mat[row_ind, col_ind].sum()\n",
        "    return cost\n",
        "\n",
        "\n",
        "def distance_between_gt_pred_xyz(gt_list, pred_list):\n",
        "    \"\"\"\n",
        "    Shortest distance between two sets of Cartesian coordinates. Given a set of groundtruth coordinates,\n",
        "     and its respective predicted coordinates, we calculate the spherical distance between each of the spherical\n",
        "     coordinate pairs resulting in a matrix of distances, where one axis represents the number of groundtruth\n",
        "     coordinates and the other the predicted coordinates. The number of estimated peaks need not be the same as in\n",
        "     groundtruth, thus the distance matrix is not always a square matrix. We use the hungarian algorithm to find the\n",
        "     least cost in this distance matrix.\n",
        "\n",
        "    :param gt_list: list of ground-truth Cartesian coordinates\n",
        "    :param pred_list: list of predicted Cartesian coordinates\n",
        "    :return: cost -  distance\n",
        "    :return: less - number of DOA's missed\n",
        "    :return: extra - number of DOA's over-estimated\n",
        "    \"\"\"\n",
        "\n",
        "    gt_len, pred_len = gt_list.shape[0], pred_list.shape[0]\n",
        "    ind_pairs = np.array([[x, y] for y in range(pred_len) for x in range(gt_len)])\n",
        "    cost_mat = np.zeros((gt_len, pred_len))\n",
        "\n",
        "    # Slow implementation\n",
        "    # cost_mat = np.zeros((gt_len, pred_len))\n",
        "    # for gt_cnt, gt in enumerate(gt_list_rad):\n",
        "    #     for pred_cnt, pred in enumerate(pred_list_rad):\n",
        "    #         cost_mat[gt_cnt, pred_cnt] = distance_between_spherical_coordinates_rad(gt, pred)\n",
        "\n",
        "    # Fast implementation\n",
        "    if gt_len and pred_len:\n",
        "        x1, y1, z1, x2, y2, z2 = gt_list[ind_pairs[:, 0], 0], gt_list[ind_pairs[:, 0], 1], gt_list[ind_pairs[:, 0], 2], \\\n",
        "                               pred_list[ind_pairs[:, 1], 0], pred_list[ind_pairs[:, 1], 1], pred_list[ind_pairs[:, 1], 2]\n",
        "        cost_mat[ind_pairs[:, 0], ind_pairs[:, 1]] = distance_between_cartesian_coordinates(x1, y1, z1, x2, y2, z2)\n",
        "\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_mat)\n",
        "    cost = cost_mat[row_ind, col_ind].sum()\n",
        "    return cost\n",
        "\n",
        "\n",
        "def distance_between_spherical_coordinates_rad(az1, ele1, az2, ele2):\n",
        "    \"\"\"\n",
        "    Angular distance between two spherical coordinates\n",
        "    MORE: https://en.wikipedia.org/wiki/Great-circle_distance\n",
        "\n",
        "    :return: angular distance in degrees\n",
        "    \"\"\"\n",
        "    dist = np.sin(ele1) * np.sin(ele2) + np.cos(ele1) * np.cos(ele2) * np.cos(np.abs(az1 - az2))\n",
        "    # Making sure the dist values are in -1 to 1 range, else np.arccos kills the job\n",
        "    dist = np.clip(dist, -1, 1)\n",
        "    dist = np.arccos(dist) * 180 / np.pi\n",
        "    return dist\n",
        "\n",
        "\n",
        "def distance_between_cartesian_coordinates(x1, y1, z1, x2, y2, z2):\n",
        "    \"\"\"\n",
        "    Angular distance between two cartesian coordinates\n",
        "    MORE: https://en.wikipedia.org/wiki/Great-circle_distance\n",
        "    Check 'From chord length' section\n",
        "\n",
        "    :return: angular distance in degrees\n",
        "    \"\"\"\n",
        "    # Normalize the Cartesian vectors\n",
        "    N1 = np.sqrt(x1**2 + y1**2 + z1**2 + 1e-10)\n",
        "    N2 = np.sqrt(x2**2 + y2**2 + z2**2 + 1e-10)\n",
        "    x1, y1, z1, x2, y2, z2 = x1/N1, y1/N1, z1/N1, x2/N2, y2/N2, z2/N2\n",
        "\n",
        "    #Compute the distance\n",
        "    dist = x1*x2 + y1*y2 + z1*z2\n",
        "    dist = np.clip(dist, -1, 1)\n",
        "    dist = np.arccos(dist) * 180 / np.pi\n",
        "    return dist\n",
        "\n",
        "\n",
        "def sph2cart(azimuth, elevation, r):\n",
        "    '''\n",
        "    Convert spherical to cartesian coordinates\n",
        "\n",
        "    :param azimuth: in radians\n",
        "    :param elevation: in radians\n",
        "    :param r: in meters\n",
        "    :return: cartesian coordinates\n",
        "    '''\n",
        "\n",
        "    x = r * np.cos(elevation) * np.cos(azimuth)\n",
        "    y = r * np.cos(elevation) * np.sin(azimuth)\n",
        "    z = r * np.sin(elevation)\n",
        "    return x, y, z\n",
        "\n",
        "\n",
        "def cart2sph(x, y, z):\n",
        "    '''\n",
        "    Convert cartesian to spherical coordinates\n",
        "\n",
        "    :param x:\n",
        "    :param y:\n",
        "    :param z:\n",
        "    :return: azi, ele in radians and r in meters\n",
        "    '''\n",
        "\n",
        "    azimuth = np.arctan2(y,x)\n",
        "    elevation = np.arctan2(z,np.sqrt(x**2 + y**2))\n",
        "    r = np.sqrt(x**2 + y**2 + z**2)\n",
        "    return azimuth, elevation, r\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# SELD scoring functions\n",
        "###############################################################\n",
        "\n",
        "\n",
        "def early_stopping_metric(sed_error, doa_error):\n",
        "    \"\"\"\n",
        "    Compute early stopping metric from sed and doa errors.\n",
        "\n",
        "    :param sed_error: [error rate (0 to 1 range), f score (0 to 1 range)]\n",
        "    :param doa_error: [doa error (in degrees), frame recall (0 to 1 range)]\n",
        "    :return: seld metric result\n",
        "    \"\"\"\n",
        "    seld_metric = np.mean([\n",
        "        sed_error[0],\n",
        "        1 - sed_error[1],\n",
        "        doa_error[0]/180,\n",
        "        1 - doa_error[1]]\n",
        "        )\n",
        "    return seld_metric\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAceyAFhoDEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Implements the localization and detection metrics proposed in the paper\n",
        "#\n",
        "# Joint Measurement of Localization and Detection of Sound Events\n",
        "# Annamaria Mesaros, Sharath Adavanne, Archontis Politis, Toni Heittola, Tuomas Virtanen\n",
        "# WASPAA 2019\n",
        "#\n",
        "#\n",
        "# This script has MIT license\n",
        "#\n",
        "\n",
        "\n",
        "eps = np.finfo(np.float).eps\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "\n",
        "class SeldEvaluationMetrics_SELDMetrics(object):\n",
        "    def __init__(self, doa_threshold=20, nb_classes=11):\n",
        "        '''\n",
        "            This class implements both the class-sensitive localization and location-sensitive detection metrics.\n",
        "            Additionally, based on the user input, the corresponding averaging is performed within the segment.\n",
        "\n",
        "        :param nb_classes: Number of sound classes. In the paper, nb_classes = 11\n",
        "        :param doa_thresh: DOA threshold for location sensitive detection.\n",
        "        '''\n",
        "\n",
        "        self._TP = 0\n",
        "        self._FP = 0\n",
        "        self._TN = 0\n",
        "        self._FN = 0\n",
        "\n",
        "        self._S = 0\n",
        "        self._D = 0\n",
        "        self._I = 0\n",
        "\n",
        "        self._Nref = 0\n",
        "        self._Nsys = 0\n",
        "\n",
        "        self._total_DE = 0\n",
        "        self._DE_TP = 0\n",
        "\n",
        "        self._spatial_T = doa_threshold\n",
        "        self._nb_classes = nb_classes\n",
        "\n",
        "    def compute_seld_scores(self):\n",
        "        '''\n",
        "        Collect the final SELD scores\n",
        "\n",
        "        :return: returns both location-sensitive detection scores and class-sensitive localization scores\n",
        "        '''\n",
        "\n",
        "        # Location-senstive detection performance\n",
        "        ER = (self._S + self._D + self._I) / float(self._Nref + eps)\n",
        "\n",
        "        prec = float(self._TP) / float(self._Nsys + eps)\n",
        "        recall = float(self._TP) / float(self._Nref + eps)\n",
        "        F = 2 * prec * recall / (prec + recall + eps)\n",
        "\n",
        "        # Class-sensitive localization performance\n",
        "        if self._DE_TP:\n",
        "            DE = self._total_DE / float(self._DE_TP + eps)\n",
        "        else:\n",
        "            # When the total number of prediction is zero\n",
        "            DE = 180\n",
        "\n",
        "        DE_prec = float(self._DE_TP) / float(self._Nsys + eps)\n",
        "        DE_recall = float(self._DE_TP) / float(self._Nref + eps)\n",
        "        DE_F = 2 * DE_prec * DE_recall / (DE_prec + DE_recall + eps)\n",
        "\n",
        "        return ER, F, DE, DE_F\n",
        "\n",
        "    def update_seld_scores_xyz(self, pred, gt):\n",
        "        '''\n",
        "        Implements the spatial error averaging according to equation [5] in the paper, using Cartesian distance\n",
        "\n",
        "        :param pred: dictionary containing class-wise prediction results for each N-seconds segment block\n",
        "        :param gt: dictionary containing class-wise groundtruth for each N-seconds segment block\n",
        "        '''\n",
        "        for block_cnt in range(len(gt.keys())):\n",
        "            # print('\\nblock_cnt', block_cnt, end='')\n",
        "            loc_FN, loc_FP = 0, 0\n",
        "            for class_cnt in range(self._nb_classes):\n",
        "                # print('\\tclass:', class_cnt, end='')\n",
        "                # Counting the number of ref and sys outputs should include the number of tracks for each class in the segment\n",
        "                if class_cnt in gt[block_cnt]:\n",
        "                    self._Nref += 1\n",
        "                if class_cnt in pred[block_cnt]:\n",
        "                    self._Nsys += 1\n",
        "\n",
        "                if class_cnt in gt[block_cnt] and class_cnt in pred[block_cnt]:\n",
        "                    # True positives or False negative case\n",
        "\n",
        "                    # NOTE: For multiple tracks per class, identify multiple tracks using hungarian algorithm and then\n",
        "                    # calculate the spatial distance using the following code. In the current code, if there are multiple \n",
        "                    # tracks of the same class in a frame we are calculating the least cost between the groundtruth and predicted and using it.\n",
        "\n",
        "                    total_spatial_dist = 0\n",
        "                    total_framewise_matching_doa = 0\n",
        "                    gt_ind_list = gt[block_cnt][class_cnt][0][0]\n",
        "                    pred_ind_list = pred[block_cnt][class_cnt][0][0]\n",
        "                    for gt_ind, gt_val in enumerate(gt_ind_list):\n",
        "                        if gt_val in pred_ind_list:\n",
        "                            total_framewise_matching_doa += 1\n",
        "                            pred_ind = pred_ind_list.index(gt_val)\n",
        "\n",
        "                            gt_arr = np.array(gt[block_cnt][class_cnt][0][1][gt_ind])\n",
        "                            pred_arr = np.array(pred[block_cnt][class_cnt][0][1][pred_ind])\n",
        "\n",
        "                            if gt_arr.shape[0]==1 and pred_arr.shape[0]==1:\n",
        "                                total_spatial_dist += distance_between_cartesian_coordinates(gt_arr[0][0], gt_arr[0][1], gt_arr[0][2], pred_arr[0][0], pred_arr[0][1], pred_arr[0][2])\n",
        "                            else:\n",
        "                                total_spatial_dist += least_distance_between_gt_pred(gt_arr, pred_arr)\n",
        "\n",
        "                    if total_spatial_dist == 0 and total_framewise_matching_doa == 0:\n",
        "                        loc_FN += 1\n",
        "                        self._FN += 1\n",
        "                    else:\n",
        "                        avg_spatial_dist = (total_spatial_dist / total_framewise_matching_doa)\n",
        "\n",
        "                        self._total_DE += avg_spatial_dist\n",
        "                        self._DE_TP += 1\n",
        "\n",
        "                        if avg_spatial_dist <= self._spatial_T:\n",
        "                            self._TP += 1\n",
        "                        else:\n",
        "                            loc_FN += 1\n",
        "                            self._FN += 1\n",
        "                elif class_cnt in gt[block_cnt] and class_cnt not in pred[block_cnt]:\n",
        "                    # False negative\n",
        "                    loc_FN += 1\n",
        "                    self._FN += 1\n",
        "                elif class_cnt not in gt[block_cnt] and class_cnt in pred[block_cnt]:\n",
        "                    # False positive\n",
        "                    loc_FP += 1\n",
        "                    self._FP += 1\n",
        "                elif class_cnt not in gt[block_cnt] and class_cnt not in pred[block_cnt]:\n",
        "                    # True negative\n",
        "                    self._TN += 1\n",
        "\n",
        "            self._S += np.minimum(loc_FP, loc_FN)\n",
        "            self._D += np.maximum(0, loc_FN - loc_FP)\n",
        "            self._I += np.maximum(0, loc_FP - loc_FN)\n",
        "        return\n",
        "\n",
        "    def update_seld_scores(self, pred_deg, gt_deg):\n",
        "        '''\n",
        "        Implements the spatial error averaging according to equation [5] in the paper, using Polar distance\n",
        "        Expects the angles in degrees\n",
        "\n",
        "        :param pred_deg: dictionary containing class-wise prediction results for each N-seconds segment block\n",
        "        :param gt_deg: dictionary containing class-wise groundtruth for each N-seconds segment block\n",
        "        '''\n",
        "        for block_cnt in range(len(gt_deg.keys())):\n",
        "            # print('\\nblock_cnt', block_cnt, end='')\n",
        "            loc_FN, loc_FP = 0, 0\n",
        "            for class_cnt in range(self._nb_classes):\n",
        "                # print('\\tclass:', class_cnt, end='')\n",
        "                # Counting the number of ref and sys outputs should include the number of tracks for each class in the segment\n",
        "                if class_cnt in gt_deg[block_cnt]:\n",
        "                    self._Nref += 1\n",
        "                if class_cnt in pred_deg[block_cnt]:\n",
        "                    self._Nsys += 1\n",
        "\n",
        "                if class_cnt in gt_deg[block_cnt] and class_cnt in pred_deg[block_cnt]:\n",
        "                    # True positives or False negative case\n",
        "\n",
        "                    # NOTE: For multiple tracks per class, identify multiple tracks using hungarian algorithm and then\n",
        "                    # calculate the spatial distance using the following code. In the current code, if there are multiple \n",
        "                    # tracks of the same class in a frame we are calculating the least cost between the groundtruth and predicted and using it.\n",
        "                    total_spatial_dist = 0\n",
        "                    total_framewise_matching_doa = 0\n",
        "                    gt_ind_list = gt_deg[block_cnt][class_cnt][0][0]\n",
        "                    pred_ind_list = pred_deg[block_cnt][class_cnt][0][0]\n",
        "                    for gt_ind, gt_val in enumerate(gt_ind_list):\n",
        "                        if gt_val in pred_ind_list:\n",
        "                            total_framewise_matching_doa += 1\n",
        "                            pred_ind = pred_ind_list.index(gt_val)\n",
        "\n",
        "                            gt_arr = np.array(gt_deg[block_cnt][class_cnt][0][1][gt_ind]) * np.pi / 180\n",
        "                            pred_arr = np.array(pred_deg[block_cnt][class_cnt][0][1][pred_ind]) * np.pi / 180\n",
        "                            if gt_arr.shape[0]==1 and pred_arr.shape[0]==1:\n",
        "                                total_spatial_dist += distance_between_spherical_coordinates_rad(gt_arr[0][0], gt_arr[0][1], pred_arr[0][0], pred_arr[0][1])\n",
        "                            else:\n",
        "                                total_spatial_dist += least_distance_between_gt_pred(gt_arr, pred_arr)\n",
        "\n",
        "                    if total_spatial_dist == 0 and total_framewise_matching_doa == 0:\n",
        "                        loc_FN += 1\n",
        "                        self._FN += 1\n",
        "                    else:\n",
        "                        avg_spatial_dist = (total_spatial_dist / total_framewise_matching_doa)\n",
        "\n",
        "                        self._total_DE += avg_spatial_dist\n",
        "                        self._DE_TP += 1\n",
        "\n",
        "                        if avg_spatial_dist <= self._spatial_T:\n",
        "                            self._TP += 1\n",
        "                        else:\n",
        "                            loc_FN += 1\n",
        "                            self._FN += 1\n",
        "                elif class_cnt in gt_deg[block_cnt] and class_cnt not in pred_deg[block_cnt]:\n",
        "                    # False negative\n",
        "                    loc_FN += 1\n",
        "                    self._FN += 1\n",
        "                elif class_cnt not in gt_deg[block_cnt] and class_cnt in pred_deg[block_cnt]:\n",
        "                    # False positive\n",
        "                    loc_FP += 1\n",
        "                    self._FP += 1\n",
        "                elif class_cnt not in gt_deg[block_cnt] and class_cnt not in pred_deg[block_cnt]:\n",
        "                    # True negative\n",
        "                    self._TN += 1\n",
        "\n",
        "            self._S += np.minimum(loc_FP, loc_FN)\n",
        "            self._D += np.maximum(0, loc_FN - loc_FP)\n",
        "            self._I += np.maximum(0, loc_FP - loc_FN)\n",
        "        return\n",
        "\n",
        "\n",
        "def least_distance_between_gt_pred(gt_list, pred_list):\n",
        "    \"\"\"\n",
        "        Shortest distance between two sets of DOA coordinates. Given a set of groundtruth coordinates,\n",
        "        and its respective predicted coordinates, we calculate the distance between each of the \n",
        "        coordinate pairs resulting in a matrix of distances, where one axis represents the number of groundtruth\n",
        "        coordinates and the other the predicted coordinates. The number of estimated peaks need not be the same as in\n",
        "        groundtruth, thus the distance matrix is not always a square matrix. We use the hungarian algorithm to find the\n",
        "        least cost in this distance matrix.\n",
        "        :param gt_list_xyz: list of ground-truth Cartesian or Polar coordinates in Radians\n",
        "        :param pred_list_xyz: list of predicted Carteisan or Polar coordinates in Radians\n",
        "        :return: cost -  distance\n",
        "        :return: less - number of DOA's missed\n",
        "        :return: extra - number of DOA's over-estimated\n",
        "    \"\"\"\n",
        "    gt_len, pred_len = gt_list.shape[0], pred_list.shape[0]\n",
        "    ind_pairs = np.array([[x, y] for y in range(pred_len) for x in range(gt_len)])\n",
        "    cost_mat = np.zeros((gt_len, pred_len))\n",
        "\n",
        "    if gt_len and pred_len:\n",
        "        if len(gt_list[0]) == 3: #Cartesian\n",
        "            x1, y1, z1, x2, y2, z2 = gt_list[ind_pairs[:, 0], 0], gt_list[ind_pairs[:, 0], 1], gt_list[ind_pairs[:, 0], 2], pred_list[ind_pairs[:, 1], 0], pred_list[ind_pairs[:, 1], 1], pred_list[ind_pairs[:, 1], 2]\n",
        "            cost_mat[ind_pairs[:, 0], ind_pairs[:, 1]] = distance_between_cartesian_coordinates(x1, y1, z1, x2, y2, z2)\n",
        "        else:\n",
        "            az1, ele1, az2, ele2 = gt_list[ind_pairs[:, 0], 0], gt_list[ind_pairs[:, 0], 1], pred_list[ind_pairs[:, 1], 0], pred_list[ind_pairs[:, 1], 1]\n",
        "            cost_mat[ind_pairs[:, 0], ind_pairs[:, 1]] = distance_between_spherical_coordinates_rad(az1, ele1, az2, ele2)\n",
        "\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_mat)\n",
        "    cost = cost_mat[row_ind, col_ind].sum()\n",
        "    return cost\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndRpJCUMkVXs",
        "colab_type": "code",
        "outputId": "79c8f667-1e71-4786-edd8-e8b57515bc5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for split_cnt, split in enumerate(test_splits):#this need to be repeated for each test_split\n",
        "  # split folder\n",
        "  split_folder = os.path.join(output_dir, 'split_{}/'.format(split_cnt))\n",
        "  # prediction folder\n",
        "  prediction_folder = os.path.join(split_folder, 'prediction_' + dataset + '/')\n",
        "  # model and resluts folder \n",
        "  model_dir = os.path.join(split_folder, 'models/')\n",
        "  dcase_dir = os.path.join(split_folder, 'results/')\n",
        "\n",
        "  #json file to save all info in\n",
        "  datafilepath = split_folder + 'data.json'\n",
        "\n",
        "  with open(datafilepath, \"r\") as fp:\n",
        "          data = json.load(fp)\n",
        "\n",
        "  best_seld_metric = 99999\n",
        "  best_epoch = -1\n",
        "  nb_epoch = 2 if quick_test else data['epoch'] ##depending on how many epochs it trained \n",
        "  #nb_epoch = 2 if quick_test else nb_epochs\n",
        "  print(\"Number of epoch: %s\" % (nb_epoch))\n",
        "  seld_metric = np.zeros(nb_epoch)\n",
        "  new_seld_metric = np.zeros(nb_epoch)\n",
        "  doa_metric = np.zeros((nb_epoch, 6))\n",
        "  sed_metric = np.zeros((nb_epoch, 2))\n",
        "  new_metric = np.zeros((nb_epoch, 4))\n",
        "\n",
        "  for epoch_cnt in range(nb_epoch):\n",
        "\n",
        "    print(\"Epoch {}\".format(epoch_cnt))\n",
        "    \n",
        "    #retriving the prediction to consider the best model\n",
        "    pred_sed_filename = \"\".join(['pred_', dataset, '_sed_', str(epoch_cnt), '.npy'])\n",
        "    pred_doa_filename = \"\".join(['pred_', dataset, '_doa_', str(epoch_cnt), '.npy'])\n",
        "    sed_pred = np.load(os.path.join(prediction_folder, pred_sed_filename))\n",
        "    doa_pred = np.load(os.path.join(prediction_folder, pred_doa_filename))\n",
        "    sed_pred = reshape_3Dto2D(sed_pred) > sed_threshold\n",
        "    doa_pred = reshape_3Dto2D(doa_pred if doa_objective is 'mse' else doa_pred[:, :, nb_classes:])\n",
        "\n",
        "    # Calculate the DCASE 2019 metrics - Detection-only and Localization-only scores\n",
        "    sed_metric[epoch_cnt, :] = compute_sed_scores(sed_pred, sed_gt, data_gen_val.nb_frames_1s())\n",
        "    doa_metric[epoch_cnt, :] = compute_doa_scores_regr_xyz(doa_pred, doa_gt, sed_pred, sed_gt)\n",
        "    seld_metric[epoch_cnt] = early_stopping_metric(sed_metric[epoch_cnt, :], doa_metric[epoch_cnt, :])\n",
        "\n",
        "    # Calculate the DCASE 2020 metrics - Location-aware detection and Class-aware localization scores\n",
        "    cls_new_metric = SeldEvaluationMetrics_SELDMetrics(nb_classes=data_gen_val.get_nb_classes(), doa_threshold=lad_doa_thresh)\n",
        "\n",
        "    pred_dict = feat_cls.regression_label_format_to_output_format(sed_pred, doa_pred)\n",
        "    gt_dict = feat_cls.regression_label_format_to_output_format(sed_gt, doa_gt)\n",
        "\n",
        "    pred_blocks_dict = feat_cls.segment_labels(pred_dict, sed_pred.shape[0])\n",
        "    gt_blocks_dict = feat_cls.segment_labels(gt_dict, sed_gt.shape[0])\n",
        "\n",
        "    cls_new_metric.update_seld_scores_xyz(pred_blocks_dict, gt_blocks_dict)\n",
        "    new_metric[epoch_cnt, :] = cls_new_metric.compute_seld_scores()\n",
        "    new_seld_metric[epoch_cnt] = early_stopping_metric(new_metric[epoch_cnt, :2], new_metric[epoch_cnt, 2:])\n",
        "\n",
        "    # Visualize the metrics with respect to epochs\n",
        "    #plot_functions(unique_name, tr_loss, sed_metric, doa_metric, seld_metric, new_metric, new_seld_metric)\n",
        "\n",
        "\n",
        "    if new_seld_metric[epoch_cnt] < best_seld_metric:\n",
        "      best_seld_metric = new_seld_metric[epoch_cnt]\n",
        "      best_epoch = epoch_cnt\n",
        "      model_name = '%s_model.h5' % (best_epoch)\n",
        "      best_model_path = \"\".join([model_dir, model_name])\n",
        "      \n",
        "\n",
        "    print(\n",
        "          'epoch_cnt: {}, time: {:0.2f}s,' \n",
        "          '\\n\\t\\t DCASE2019 SCORES: ER: {:0.2f}, F: {:0.1f}, DE: {:0.1f}, FR:{:0.1f}, seld_score: {:0.2f}, ' \n",
        "          '\\n\\t\\t DCASE2020 SCORES: ER: {:0.2f}, F: {:0.1f}, DE: {:0.1f}, DE_F:{:0.1f}, seld_score (early stopping score): {:0.2f}, '\n",
        "          'best_seld_score: {:0.2f}, best_epoch : {}\\n'.format(\n",
        "          epoch_cnt, time.time() - start, \n",
        "          sed_metric[epoch_cnt, 0], sed_metric[epoch_cnt, 1]*100,\n",
        "          doa_metric[epoch_cnt, 0], doa_metric[epoch_cnt, 1]*100, seld_metric[epoch_cnt],\n",
        "          new_metric[epoch_cnt, 0], new_metric[epoch_cnt, 1]*100,\n",
        "          new_metric[epoch_cnt, 2], new_metric[epoch_cnt, 3]*100,\n",
        "          new_seld_metric[epoch_cnt], best_seld_metric, best_epoch\n",
        "                  )\n",
        "              )\n",
        "\n",
        "    \n",
        "    #outer loop\n",
        "  avg_scores_val.append([new_metric[best_epoch, 0], new_metric[best_epoch, 1], new_metric[best_epoch, 2],\n",
        "                                  new_metric[best_epoch, 3], best_seld_metric])\n",
        "\n",
        "  print('\\nResults on validation split:')\n",
        "  print('\\tUnique_name: {} '.format(best_model_path))\n",
        "  print('\\tSaved model for the best_epoch: {}'.format(best_epoch))\n",
        "  print('\\tSELD_score (early stopping score) : {}'.format(best_seld_metric))\n",
        "\n",
        "  print('\\n\\tDCASE2020 scores')\n",
        "  print('\\tClass-aware localization scores: DOA_error: {:0.1f}, F-score: {:0.1f}'.format(new_metric[best_epoch, 2], new_metric[best_epoch, 3]*100))\n",
        "  print('\\tLocation-aware detection scores: Error rate: {:0.2f}, F-score: {:0.1f}'.format(new_metric[best_epoch, 0], new_metric[best_epoch, 1]*100))\n",
        "\n",
        "  print('\\n\\tDCASE2019 scores')\n",
        "  print('\\tLocalization-only scores: DOA_error: {:0.1f}, Frame recall: {:0.1f}'.format(doa_metric[best_epoch, 0], doa_metric[best_epoch, 1]*100))\n",
        "  print('\\tDetection-only scores: Error rate: {:0.2f}, F-score: {:0.1f}\\n'.format(sed_metric[best_epoch, 0], sed_metric[best_epoch, 1]*100))\n",
        "\n",
        "    #### last part to add\n",
        "     # ------------------  Calculate metric scores for unseen test split ---------------------------------\n",
        "  print('\\nLoading the best model and predicting results on the testing split')\n",
        "  print('\\tLoading testing dataset:')\n",
        "  data_gen_test = DataGenerator(\n",
        "            split=split, shuffle=False, per_file=dcase_output, is_eval=True if mode is 'eval' else False\n",
        "        )\n",
        "\n",
        "  model = load_seld_model(best_model_path, doa_objective)\n",
        "  print(\"Model loaded: {}\".format(best_model_path))\n",
        "\n",
        "  pred_test = model.predict(\n",
        "            x=data_gen_test.generate(),\n",
        "            steps=2 if quick_test else data_gen_test.get_total_batches_in_data(),\n",
        "            verbose=2\n",
        "        )\n",
        "\n",
        "  test_sed_pred = reshape_3Dto2D(pred_test[0]) > sed_threshold\n",
        "  test_doa_pred = reshape_3Dto2D(pred_test[1] if doa_objective is 'mse' else pred_test[1][:, :, nb_classes:])\n",
        "\n",
        "  if dcase_output:\n",
        "            # Dump results in DCASE output format for calculating final scores\n",
        "      dcase_dump_folder = os.path.join(dcase_dir, '{}_{}'.format(dataset, mode))\n",
        "      create_folder(dcase_dump_folder)\n",
        "      print('Dumping recording-wise results in: {}'.format(dcase_dump_folder))\n",
        "\n",
        "      test_filelist = data_gen_test.get_filelist()\n",
        "            # Number of frames for a 60 second audio with 20ms hop length = 3000 frames\n",
        "      max_frames_with_content = data_gen_test.get_nb_frames()\n",
        "\n",
        "            # Number of frames in one batch (batch_size* sequence_length) consists of all the 3000 frames above with\n",
        "            # zero padding in the remaining frames\n",
        "      frames_per_file = data_gen_test.get_frame_per_file()\n",
        "\n",
        "      for file_cnt in range(test_sed_pred.shape[0]//frames_per_file):\n",
        "          filename = test_filelist[file_cnt].split('-')[0] + '.npy'\n",
        "          output_file = os.path.join(dcase_dump_folder, filename.replace('.npy', '.csv'))\n",
        "          dc = file_cnt * frames_per_file\n",
        "          output_dict = feat_cls.regression_label_format_to_output_format(\n",
        "                    test_sed_pred[dc:dc + max_frames_with_content, :],\n",
        "                    test_doa_pred[dc:dc + max_frames_with_content, :]\n",
        "                )\n",
        "          data_gen_test.write_output_format_file(output_file, output_dict)\n",
        "\n",
        "  if mode is 'dev':\n",
        "\n",
        "    test_data_in, test_data_out = data_gen_test.get_data_sizes()\n",
        "    test_gt = collect_test_labels(data_gen_test, test_data_out, nb_classes, quick_test)\n",
        "    test_sed_gt = reshape_3Dto2D(test_gt[0])\n",
        "    test_doa_gt = reshape_3Dto2D(test_gt[1])\n",
        "         \n",
        "            # Calculate DCASE2019 scores\n",
        "    test_sed_loss = compute_sed_scores(test_sed_pred, test_sed_gt, data_gen_test.nb_frames_1s())\n",
        "    test_doa_loss = compute_doa_scores_regr_xyz(test_doa_pred, test_doa_gt, test_sed_pred, test_sed_gt)\n",
        "    test_metric_loss = early_stopping_metric(test_sed_loss, test_doa_loss)\n",
        "\n",
        "            # Calculate DCASE2020 scores\n",
        "    cls_new_metric = SeldEvaluationMetrics_SELDMetrics(nb_classes=data_gen_test.get_nb_classes(), doa_threshold=lad_doa_thresh)\n",
        "    test_pred_dict = feat_cls.regression_label_format_to_output_format(\n",
        "            test_sed_pred, test_doa_pred\n",
        "        )\n",
        "    test_gt_dict = feat_cls.regression_label_format_to_output_format(\n",
        "            test_sed_gt, test_doa_gt\n",
        "      )\n",
        "\n",
        "    test_pred_blocks_dict = feat_cls.segment_labels(test_pred_dict, test_sed_pred.shape[0])\n",
        "    test_gt_blocks_dict = feat_cls.segment_labels(test_gt_dict, test_sed_gt.shape[0])\n",
        "\n",
        "    cls_new_metric.update_seld_scores_xyz(test_pred_blocks_dict, test_gt_blocks_dict)\n",
        "    test_new_metric = cls_new_metric.compute_seld_scores()\n",
        "    test_new_seld_metric = early_stopping_metric(test_new_metric[:2], test_new_metric[2:])\n",
        "\n",
        "    avg_scores_test.append([test_new_metric[0], test_new_metric[1], test_new_metric[2], test_new_metric[3], test_new_seld_metric])\n",
        "    print('Results on test split:')\n",
        "\n",
        "    print('\\tDCASE2020 Scores')\n",
        "    print('\\tClass-aware localization scores: DOA Error: {:0.1f}, F-score: {:0.1f}'.format(test_new_metric[2], test_new_metric[3]*100))\n",
        "    print('\\tLocation-aware detection scores: Error rate: {:0.2f}, F-score: {:0.1f}'.format(test_new_metric[0], test_new_metric[1]*100))\n",
        "    print('\\tSELD (early stopping metric): {:0.2f}'.format(test_new_seld_metric))\n",
        "\n",
        "    print('\\n\\tDCASE2019 Scores')\n",
        "    print('\\tLocalization-only scores: DOA Error: {:0.1f}, Frame recall: {:0.1f}'.format(test_doa_loss[0], test_doa_loss[1]*100))\n",
        "    print('\\tDetection-only scores:Error rate: {:0.2f}, F-score: {:0.1f}'.format(test_sed_loss[0], test_sed_loss[1]*100))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of epoch: 50\n",
            "Epoch 0\n",
            "epoch_cnt: 0, time: 9.18s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.68, F: 46.9, DE: 20.7, FR:53.0, seld_score: 0.45, \n",
            "\t\t DCASE2020 SCORES: ER: 0.84, F: 23.9, DE: 23.5, DE_F:46.9, seld_score (early stopping score): 0.57, best_seld_score: 0.57, best_epoch : 0\n",
            "\n",
            "Epoch 1\n",
            "epoch_cnt: 1, time: 16.89s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.54, F: 61.2, DE: 20.2, FR:61.6, seld_score: 0.36, \n",
            "\t\t DCASE2020 SCORES: ER: 0.74, F: 35.8, DE: 23.9, DE_F:61.1, seld_score (early stopping score): 0.48, best_seld_score: 0.48, best_epoch : 1\n",
            "\n",
            "Epoch 2\n",
            "epoch_cnt: 2, time: 25.23s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.52, F: 61.6, DE: 19.8, FR:64.9, seld_score: 0.34, \n",
            "\t\t DCASE2020 SCORES: ER: 0.72, F: 37.0, DE: 21.7, DE_F:61.5, seld_score (early stopping score): 0.46, best_seld_score: 0.46, best_epoch : 2\n",
            "\n",
            "Epoch 3\n",
            "epoch_cnt: 3, time: 34.22s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.50, F: 63.8, DE: 18.8, FR:67.5, seld_score: 0.32, \n",
            "\t\t DCASE2020 SCORES: ER: 0.70, F: 38.4, DE: 21.7, DE_F:63.9, seld_score (early stopping score): 0.45, best_seld_score: 0.45, best_epoch : 3\n",
            "\n",
            "Epoch 4\n",
            "epoch_cnt: 4, time: 42.94s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.50, F: 64.7, DE: 18.9, FR:67.6, seld_score: 0.32, \n",
            "\t\t DCASE2020 SCORES: ER: 0.72, F: 38.6, DE: 22.1, DE_F:64.6, seld_score (early stopping score): 0.45, best_seld_score: 0.45, best_epoch : 3\n",
            "\n",
            "Epoch 5\n",
            "epoch_cnt: 5, time: 52.06s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.52, F: 63.2, DE: 19.3, FR:67.9, seld_score: 0.33, \n",
            "\t\t DCASE2020 SCORES: ER: 0.71, F: 38.6, DE: 21.2, DE_F:63.2, seld_score (early stopping score): 0.45, best_seld_score: 0.45, best_epoch : 3\n",
            "\n",
            "Epoch 6\n",
            "epoch_cnt: 6, time: 60.89s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.49, F: 66.9, DE: 18.3, FR:67.3, seld_score: 0.31, \n",
            "\t\t DCASE2020 SCORES: ER: 0.69, F: 41.5, DE: 21.0, DE_F:67.1, seld_score (early stopping score): 0.43, best_seld_score: 0.43, best_epoch : 6\n",
            "\n",
            "Epoch 7\n",
            "epoch_cnt: 7, time: 70.22s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.49, F: 66.1, DE: 16.9, FR:68.7, seld_score: 0.31, \n",
            "\t\t DCASE2020 SCORES: ER: 0.66, F: 44.7, DE: 19.7, DE_F:66.0, seld_score (early stopping score): 0.41, best_seld_score: 0.41, best_epoch : 7\n",
            "\n",
            "Epoch 8\n",
            "epoch_cnt: 8, time: 92.23s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.7, DE: 16.7, FR:71.3, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.62, F: 47.8, DE: 19.1, DE_F:67.7, seld_score (early stopping score): 0.39, best_seld_score: 0.39, best_epoch : 8\n",
            "\n",
            "Epoch 9\n",
            "epoch_cnt: 9, time: 101.46s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.47, F: 66.6, DE: 17.1, FR:70.8, seld_score: 0.30, \n",
            "\t\t DCASE2020 SCORES: ER: 0.61, F: 47.6, DE: 19.4, DE_F:66.5, seld_score (early stopping score): 0.40, best_seld_score: 0.39, best_epoch : 8\n",
            "\n",
            "Epoch 10\n",
            "epoch_cnt: 10, time: 110.87s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.4, DE: 16.6, FR:71.2, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.61, F: 48.3, DE: 19.7, DE_F:67.2, seld_score (early stopping score): 0.39, best_seld_score: 0.39, best_epoch : 10\n",
            "\n",
            "Epoch 11\n",
            "epoch_cnt: 11, time: 120.35s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.8, DE: 15.8, FR:70.7, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.59, F: 50.2, DE: 18.3, DE_F:67.7, seld_score (early stopping score): 0.38, best_seld_score: 0.38, best_epoch : 11\n",
            "\n",
            "Epoch 12\n",
            "epoch_cnt: 12, time: 129.55s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.7, DE: 16.0, FR:70.9, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.59, F: 50.1, DE: 18.3, DE_F:67.6, seld_score (early stopping score): 0.38, best_seld_score: 0.38, best_epoch : 12\n",
            "\n",
            "Epoch 13\n",
            "epoch_cnt: 13, time: 139.55s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 68.2, DE: 15.5, FR:70.5, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.58, F: 51.8, DE: 18.4, DE_F:68.2, seld_score (early stopping score): 0.37, best_seld_score: 0.37, best_epoch : 13\n",
            "\n",
            "Epoch 14\n",
            "epoch_cnt: 14, time: 149.28s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 68.3, DE: 15.4, FR:69.8, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.58, F: 51.5, DE: 18.6, DE_F:68.4, seld_score (early stopping score): 0.37, best_seld_score: 0.37, best_epoch : 13\n",
            "\n",
            "Epoch 15\n",
            "epoch_cnt: 15, time: 158.60s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.48, F: 65.9, DE: 16.0, FR:71.4, seld_score: 0.30, \n",
            "\t\t DCASE2020 SCORES: ER: 0.61, F: 48.4, DE: 19.2, DE_F:66.0, seld_score (early stopping score): 0.39, best_seld_score: 0.37, best_epoch : 13\n",
            "\n",
            "Epoch 16\n",
            "epoch_cnt: 16, time: 168.10s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.2, DE: 15.2, FR:71.0, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.58, F: 51.6, DE: 17.6, DE_F:67.2, seld_score (early stopping score): 0.37, best_seld_score: 0.37, best_epoch : 13\n",
            "\n",
            "Epoch 17\n",
            "epoch_cnt: 17, time: 177.35s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 67.5, DE: 15.7, FR:71.9, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.58, F: 51.1, DE: 17.7, DE_F:67.3, seld_score (early stopping score): 0.37, best_seld_score: 0.37, best_epoch : 13\n",
            "\n",
            "Epoch 18\n",
            "epoch_cnt: 18, time: 187.16s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 69.0, DE: 14.9, FR:70.4, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 53.1, DE: 18.0, DE_F:69.1, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 18\n",
            "\n",
            "Epoch 19\n",
            "epoch_cnt: 19, time: 196.79s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.8, DE: 15.6, FR:71.6, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.58, F: 52.3, DE: 18.0, DE_F:67.8, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 18\n",
            "\n",
            "Epoch 20\n",
            "epoch_cnt: 20, time: 205.96s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.1, DE: 15.4, FR:71.4, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.58, F: 51.9, DE: 17.9, DE_F:67.2, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 18\n",
            "\n",
            "Epoch 21\n",
            "epoch_cnt: 21, time: 215.56s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.7, DE: 15.4, FR:71.2, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.58, F: 52.4, DE: 17.8, DE_F:67.7, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 18\n",
            "\n",
            "Epoch 22\n",
            "epoch_cnt: 22, time: 224.82s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 68.5, DE: 15.2, FR:71.5, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.56, F: 53.3, DE: 17.5, DE_F:68.5, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 22\n",
            "\n",
            "Epoch 23\n",
            "epoch_cnt: 23, time: 234.29s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.47, F: 67.0, DE: 15.7, FR:71.3, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.58, F: 52.1, DE: 17.0, DE_F:66.9, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 22\n",
            "\n",
            "Epoch 24\n",
            "epoch_cnt: 24, time: 243.55s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 68.0, DE: 14.8, FR:71.1, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 53.2, DE: 17.4, DE_F:67.8, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 22\n",
            "\n",
            "Epoch 25\n",
            "epoch_cnt: 25, time: 253.06s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.47, F: 67.1, DE: 15.3, FR:71.6, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 53.2, DE: 16.9, DE_F:66.9, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 22\n",
            "\n",
            "Epoch 26\n",
            "epoch_cnt: 26, time: 262.65s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 67.9, DE: 15.6, FR:72.1, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 52.8, DE: 17.7, DE_F:67.9, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 22\n",
            "\n",
            "Epoch 27\n",
            "epoch_cnt: 27, time: 271.81s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.47, F: 66.7, DE: 15.7, FR:71.1, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.59, F: 51.1, DE: 18.6, DE_F:66.7, seld_score (early stopping score): 0.38, best_seld_score: 0.36, best_epoch : 22\n",
            "\n",
            "Epoch 28\n",
            "epoch_cnt: 28, time: 281.68s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 68.9, DE: 15.5, FR:71.8, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.56, F: 53.3, DE: 18.1, DE_F:69.0, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 28\n",
            "\n",
            "Epoch 29\n",
            "epoch_cnt: 29, time: 290.87s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.44, F: 68.2, DE: 15.2, FR:73.1, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.55, F: 53.7, DE: 17.5, DE_F:68.2, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 29\n",
            "\n",
            "Epoch 30\n",
            "epoch_cnt: 30, time: 300.57s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.8, DE: 15.3, FR:72.2, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 52.9, DE: 17.4, DE_F:67.8, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 29\n",
            "\n",
            "Epoch 31\n",
            "epoch_cnt: 31, time: 310.20s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 67.4, DE: 15.1, FR:72.5, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.58, F: 52.0, DE: 17.5, DE_F:67.3, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 29\n",
            "\n",
            "Epoch 32\n",
            "epoch_cnt: 32, time: 319.47s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 67.9, DE: 15.5, FR:72.6, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.56, F: 54.0, DE: 17.2, DE_F:67.9, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 29\n",
            "\n",
            "Epoch 33\n",
            "epoch_cnt: 33, time: 329.06s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.5, DE: 15.2, FR:71.6, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 52.3, DE: 17.7, DE_F:67.4, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 29\n",
            "\n",
            "Epoch 34\n",
            "epoch_cnt: 34, time: 338.36s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.4, DE: 15.3, FR:71.7, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.58, F: 52.5, DE: 17.7, DE_F:67.4, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 29\n",
            "\n",
            "Epoch 35\n",
            "epoch_cnt: 35, time: 347.88s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.1, DE: 15.2, FR:72.1, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 52.2, DE: 17.6, DE_F:67.1, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 29\n",
            "\n",
            "Epoch 36\n",
            "epoch_cnt: 36, time: 357.44s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 66.6, DE: 15.4, FR:72.8, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 52.9, DE: 17.2, DE_F:66.5, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 29\n",
            "\n",
            "Epoch 37\n",
            "epoch_cnt: 37, time: 366.54s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.2, DE: 14.7, FR:71.7, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.56, F: 53.3, DE: 16.7, DE_F:67.1, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 29\n",
            "\n",
            "Epoch 38\n",
            "epoch_cnt: 38, time: 376.17s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.2, DE: 15.1, FR:72.5, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 53.1, DE: 17.1, DE_F:67.1, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 29\n",
            "\n",
            "Epoch 39\n",
            "epoch_cnt: 39, time: 385.89s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 68.7, DE: 14.8, FR:72.7, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.56, F: 54.4, DE: 17.3, DE_F:68.7, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 39\n",
            "\n",
            "Epoch 40\n",
            "epoch_cnt: 40, time: 395.21s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.44, F: 68.4, DE: 15.4, FR:72.6, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.56, F: 54.3, DE: 17.0, DE_F:68.4, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 39\n",
            "\n",
            "Epoch 41\n",
            "epoch_cnt: 41, time: 404.87s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 68.2, DE: 14.9, FR:72.8, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 53.0, DE: 17.0, DE_F:68.1, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 39\n",
            "\n",
            "Epoch 42\n",
            "epoch_cnt: 42, time: 414.14s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 67.4, DE: 15.5, FR:73.3, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 52.6, DE: 17.3, DE_F:67.4, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 39\n",
            "\n",
            "Epoch 43\n",
            "epoch_cnt: 43, time: 423.88s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 68.2, DE: 15.3, FR:72.2, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 53.0, DE: 17.5, DE_F:68.2, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 39\n",
            "\n",
            "Epoch 44\n",
            "epoch_cnt: 44, time: 433.09s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 66.6, DE: 15.4, FR:72.6, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 52.6, DE: 17.3, DE_F:66.6, seld_score (early stopping score): 0.37, best_seld_score: 0.36, best_epoch : 39\n",
            "\n",
            "Epoch 45\n",
            "epoch_cnt: 45, time: 442.79s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 68.7, DE: 15.5, FR:72.8, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 53.8, DE: 17.3, DE_F:68.6, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 39\n",
            "\n",
            "Epoch 46\n",
            "epoch_cnt: 46, time: 452.38s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.44, F: 67.8, DE: 15.7, FR:74.1, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.55, F: 54.2, DE: 16.9, DE_F:67.7, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 39\n",
            "\n",
            "Epoch 47\n",
            "epoch_cnt: 47, time: 461.56s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 67.8, DE: 15.0, FR:71.7, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 53.6, DE: 17.2, DE_F:67.8, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 39\n",
            "\n",
            "Epoch 48\n",
            "epoch_cnt: 48, time: 471.11s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.46, F: 67.4, DE: 15.0, FR:72.6, seld_score: 0.29, \n",
            "\t\t DCASE2020 SCORES: ER: 0.56, F: 53.6, DE: 16.9, DE_F:67.4, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 39\n",
            "\n",
            "Epoch 49\n",
            "epoch_cnt: 49, time: 479.87s,\n",
            "\t\t DCASE2019 SCORES: ER: 0.45, F: 68.5, DE: 14.7, FR:72.7, seld_score: 0.28, \n",
            "\t\t DCASE2020 SCORES: ER: 0.57, F: 53.8, DE: 17.1, DE_F:68.4, seld_score (early stopping score): 0.36, best_seld_score: 0.36, best_epoch : 39\n",
            "\n",
            "\n",
            "Results on validation split:\n",
            "\tUnique_name: /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/models/39_model.h5 \n",
            "\tSaved model for the best_epoch: 39\n",
            "\tSELD_score (early stopping score) : 0.35565264042226796\n",
            "\n",
            "\tDCASE2020 scores\n",
            "\tClass-aware localization scores: DOA_error: 17.3, F-score: 68.7\n",
            "\tLocation-aware detection scores: Error rate: 0.56, F-score: 54.4\n",
            "\n",
            "\tDCASE2019 scores\n",
            "\tLocalization-only scores: DOA_error: 14.8, Frame recall: 72.7\n",
            "\tDetection-only scores: Error rate: 0.45, F-score: 68.7\n",
            "\n",
            "\n",
            "Loading the best model and predicting results on the testing split\n",
            "\tLoading testing dataset:\n",
            "\tDatagen_mode: eval, nb_files: 200, nb_classes:14\n",
            "\tnb_frames_file: 3000, feat_len: 64, nb_ch: 7, label_len:None\n",
            "\n",
            "\tDataset: foa, split: [7, 8]\n",
            "\tbatch_size: 10, feat_seq_len: 300, label_seq_len: 60, shuffle: False\n",
            "\tTotal batches in dataset: 200\n",
            "\tlabel_dir: None\n",
            " \tfeat_dir: /content/drive/My Drive/Dataset-FP/dataset-eval/feat_label-AR/foa_eval_norm\n",
            "\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "Model loaded: /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/models/39_model.h5\n",
            "200/200 - 172s\n",
            "/content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/results/foa_eval folder does not exist, creating it.\n",
            "Dumping recording-wise results in: /content/drive/My Drive/Dataset-FP/dataset-eval-output/TFr-5CNN-1x48-mse-dropCNNRNN-AR2-foa/split_0/results/foa_eval\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bINjWWu0uAZX",
        "colab_type": "text"
      },
      "source": [
        "## Validation and Testing loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQjz4Vvfqbje",
        "colab_type": "code",
        "outputId": "db7f65b8-f60b-4a08-a9d6-aa8dfc83e03d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import ast \n",
        "#split_folder = os.path.join(output_dir, 'split_{}'.format(split_cnt))\n",
        "split_folder = os.path.join(output_dir, 'split_{}'.format(0))\n",
        "datafilepath = split_folder + '/data.json'\n",
        "history_folder = os.path.join(split_folder, 'history/')\n",
        "\n",
        "#print(datafilepath)\n",
        "loss = []\n",
        "sed_out_loss = []\n",
        "doa_out_loss = []\n",
        "val_loss = []\n",
        "sed_out_val_loss = []\n",
        "doa_out_val_loss = []\n",
        "\n",
        "with open(datafilepath) as fp:\n",
        "  data = json.load(fp)\n",
        "\n",
        "nb_epoch = data['epoch']\n",
        "#nb_epoch = 70\n",
        "\n",
        "#print(data)\n",
        "for epoch_cnt in range(nb_epoch):\n",
        "  hist_loss_filename = os.path.join(\"\".join([history_folder, 'pred_%s_%s_hist_loss.json' % (dataset, str(epoch_cnt))]))\n",
        "  with open(hist_loss_filename) as fp:\n",
        "    hist_loss = json.load(fp)\n",
        "    hist_loss = ast.literal_eval(hist_loss) \n",
        "    if len(loss) == 0:\n",
        "      loss = hist_loss['loss']\n",
        "      sed_out_loss = hist_loss['sed_out_loss']\n",
        "      doa_out_loss = hist_loss['doa_out_loss']\n",
        "      val_loss = hist_loss['val_loss']\n",
        "      sed_out_val_loss = hist_loss['val_sed_out_loss']\n",
        "      doa_out_val_loss = hist_loss['val_doa_out_loss']\n",
        "    else:\n",
        "      loss.append(hist_loss['loss'][0])\n",
        "      sed_out_loss.append(hist_loss['sed_out_loss'][0])\n",
        "      doa_out_loss.append(hist_loss['doa_out_loss'][0])\n",
        "      val_loss.append(hist_loss['val_loss'][0])\n",
        "      sed_out_val_loss.append(hist_loss['val_sed_out_loss'][0])\n",
        "      doa_out_val_loss.append(hist_loss['val_doa_out_loss'][0])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(22, 5))\n",
        "plt.plot(loss)\n",
        "plt.plot(sed_out_loss)\n",
        "plt.plot(doa_out_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.plot(sed_out_val_loss)\n",
        "plt.plot(doa_out_val_loss)\n",
        "plt.title('Model loss: {}, nb epoch: {}'.format(network, nb_epoch))\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss', 'sed_loss', 'doa_loss', 'val_loss', 'val_sed_out_loss', 'val_doa_out_loss'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1584x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}